import json
import re
import os
import time
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass
from typing import Dict, List, Generator, Tuple, Optional
from crazy_functions.crazy_utils import request_gpt_model_in_new_thread_with_ui_alive
from toolbox import update_ui, promote_file_to_downloadzone, write_history_to_file, CatchException, report_exception
from shared_utils.fastapi_server import validate_path_safety
from crazy_functions.paper_fns.paper_download import extract_paper_id, get_arxiv_paper, format_arxiv_id
import difflib


def _estimate_tokens(text: str, llm_model: str) -> int:
    """ä½¿ç”¨å·²é…ç½®æ¨¡å‹çš„tokenizerä¼°ç®—æ–‡æœ¬tokenæ•°ã€‚"""
    try:
        from request_llms.bridge_all import model_info
        cnt_fn = model_info.get(llm_model, {}).get("token_cnt", None)
        if cnt_fn is None:
            # å…œåº•ï¼šè‹¥æ¨¡å‹æœªé…ç½®ï¼Œä½¿ç”¨gpt-3.5çš„tokenizerè¿‘ä¼¼
            cnt_fn = model_info["gpt-3.5-turbo"]["token_cnt"]
        return int(cnt_fn(text or ""))
    except Exception:
        # æ— æ³•ä¼°è®¡æ—¶ä»¥å­—ç¬¦æ•°è¿‘ä¼¼
        return len(text or "")


def estimate_token_usage(inputs: List[str], outputs: List[str], llm_model: str) -> Dict:
    """
    ç‹¬ç«‹çš„æ£€æµ‹å‡½æ•°ï¼šä¼°ç®—ä¸€ç»„äº¤äº’çš„è¾“å…¥/è¾“å‡ºtokenæ¶ˆè€—ã€‚
    """
    n = max(len(inputs or []), len(outputs or []))
    items = []
    sum_in = 0
    sum_out = 0
    for i in range(n):
        inp = inputs[i] if i < len(inputs) else ""
        out = outputs[i] if i < len(outputs) else ""
        ti = _estimate_tokens(inp, llm_model)
        to = _estimate_tokens(out, llm_model)
        items.append({
            'input_tokens': ti,
            'output_tokens': to,
            'total_tokens': ti + to,
        })
        sum_in += ti
        sum_out += to
    return {
        'model': llm_model,
        'items': items,
        'sum_input_tokens': sum_in,
        'sum_output_tokens': sum_out,
        'sum_total_tokens': sum_in + sum_out,
    }


@dataclass
class PaperQuestion:
    """è®ºæ–‡åˆ†æé—®é¢˜ç±»"""
    id: str  # é—®é¢˜ID
    question: str  # é—®é¢˜å†…å®¹
    importance: int  # é‡è¦æ€§ (1-5ï¼Œ5æœ€é«˜)
    description: str  # é—®é¢˜æè¿°
    domain: str  # é€‚ç”¨é¢†åŸŸ ("general", "rf_ic", "both")


class UnifiedBatchPaperAnalyzer:
    """ç»Ÿä¸€çš„æ‰¹é‡è®ºæ–‡åˆ†æå™¨ - æ”¯æŒä¸»é¢˜åˆ†ç±»å’ŒåŠ¨æ€prompt"""

    def __init__(self, llm_kwargs: Dict, plugin_kwargs: Dict, chatbot: List, history: List, system_prompt: str):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.llm_kwargs = llm_kwargs
        self.plugin_kwargs = plugin_kwargs
        self.chatbot = chatbot
        self.history = history
        self.system_prompt = system_prompt
        self.paper_content = ""
        self.results = {}
        self.paper_file_path = None
        self.secondary_category = None
        self.paper_domain = "general"  # è®ºæ–‡é¢†åŸŸåˆ†ç±»
        self.context_history = []  # ä¸LLMå…±äº«çš„ä¸Šä¸‹æ–‡ï¼ˆæ¯ç¯‡è®ºæ–‡æ³¨å…¥ä¸€æ¬¡å…¨æ–‡ï¼‰
        # ç»Ÿè®¡ç”¨ï¼šè®°å½•æ¯æ¬¡LLMäº¤äº’çš„è¾“å…¥ä¸è¾“å‡º
        self._token_inputs: List[str] = []
        self._token_outputs: List[str] = []
        
        # ---------- è¯»å–åˆ†ç±»æ ‘ ----------
        json_path = os.path.join(os.path.dirname(__file__), 'paper.json')
        with open(json_path, 'r', encoding='utf-8') as f:
            self.category_tree = json.load(f)          # Dict[str, List[str]]

        # ç”Ÿæˆç»™ LLM çš„å½“å‰åˆ†ç±»æ¸…å•
        category_lines = [f"{main} -> {', '.join(subs)}"
                        for main, subs in self.category_tree.items()]
        self.category_prompt_str = '\n'.join(category_lines)

        # å®šä¹‰é€Ÿè¯»é—®é¢˜åº“ï¼ˆç²¾ç®€ç‰ˆï¼Œä¸“æ³¨äºå¿«é€Ÿç­›é€‰ï¼‰
        self.questions = [
            # é€šç”¨é€Ÿè¯»é—®é¢˜ï¼ˆé€‚ç”¨äºæ‰€æœ‰è®ºæ–‡ï¼‰
            PaperQuestion(
                id="research_methods_and_data",
                question="è¯·ç®€è¦æ¦‚æ‹¬è®ºæ–‡çš„æ ¸å¿ƒå†…å®¹ï¼š1) ç ”ç©¶é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ2) ä¸»è¦æ–¹æ³•/æŠ€æœ¯è·¯çº¿æ˜¯ä»€ä¹ˆï¼Ÿ3) å®éªŒæ•°æ®æ¥æºå¦‚ä½•ï¼Ÿ",
                importance=5,
                description="ç ”ç©¶é—®é¢˜ä¸æ–¹æ³•æ¦‚è¿°",
                domain="both"
            ),
            PaperQuestion(
                id="findings_innovations_and_impact",
                question="è¯·æ€»ç»“è®ºæ–‡çš„ä¸»è¦å‘ç°ä¸åˆ›æ–°ï¼š1) æ ¸å¿ƒç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ2) ä¸»è¦åˆ›æ–°ç‚¹æœ‰å“ªäº›ï¼Ÿ3) å¯¹é¢†åŸŸçš„å½±å“å¦‚ä½•ï¼Ÿ",
                importance=4,
                description="ä¸»è¦å‘ç°ä¸åˆ›æ–°ç‚¹",
                domain="both"
            ),
            PaperQuestion(
                id="ppt_md_summary",
                question=(
                    "è¯·è¾“å‡ºç”¨äºPPTçš„Markdownæç®€æ‘˜è¦ï¼ˆä»…æŒ‰å¦‚ä¸‹ç»“æ„ï¼Œå‹¿åµŒå…¥ä»£ç å—ï¼‰ï¼š\n\n"
                    "# æ€»è¿°ï¼ˆ1 è¡Œï¼‰\n"
                    "- ç”¨ä¸€å¥è¯æ¦‚æ‹¬è®ºæ–‡åšäº†ä»€ä¹ˆã€ä¸ºä½•æœ‰æ•ˆ\n\n"
                    "# æ ¸å¿ƒè¦ç‚¹ï¼ˆ3-5æ¡ï¼‰\n"
                    "- å…³é”®è¾“å…¥/æ–¹æ³•/è¾“å‡º/åˆ›æ–°ï¼ˆæ¯æ¡ â‰¤ 16 å­—ï¼‰\n\n"
                    "# åº”ç”¨ä¸æ•ˆæœï¼ˆâ‰¤ 3 æ¡ï¼Œå¯çœç•¥ï¼‰\n"
                    "- åœºæ™¯/æŒ‡æ ‡/æ”¶ç›Š"
                ),
                importance=3,
                description="PPT ç”¨æç®€Markdownæ‘˜è¦",
                domain="both"
            ),
            PaperQuestion(
                id="worth_reading_judgment",
                question="è¯·ç»¼åˆè¯„ä¼°è¿™ç¯‡è®ºæ–‡æ˜¯å¦å€¼å¾—ç²¾è¯»ï¼Œå¹¶ç»™å‡ºæ˜ç¡®çš„æ¨èç­‰çº§ï¼š\n1) **åˆ›æ–°æ€§**ï¼šæ˜¯å¦å…·æœ‰å¼€åˆ›æ€§è´¡çŒ®ï¼Ÿ\n2) **å¯é æ€§**ï¼šç ”ç©¶æ–¹æ³•æ˜¯å¦ä¸¥è°¨ï¼Ÿ\n3) **å½±å“åŠ›**ï¼šæ˜¯å¦å¯èƒ½äº§ç”Ÿé‡è¦å½±å“ï¼Ÿ\n4) **ç»¼åˆå»ºè®®**ï¼šç»™å‡º\"å¼ºçƒˆæ¨è\"ã€\"æ¨è\"ã€\"ä¸€èˆ¬\"æˆ–\"ä¸æ¨è\"çš„è¯„çº§ï¼Œå¹¶ç®€è¦è¯´æ˜ç†ç”±ã€‚",
                importance=5,
                description="æ˜¯å¦å€¼å¾—ç²¾è¯»",
                domain="both"
            ),
            PaperQuestion(
                id="category_assignment",
                question=(
                    "è¯·æ ¹æ®è®ºæ–‡å†…å®¹ï¼Œåˆ¤æ–­å…¶æœ€å‡†ç¡®çš„äºŒçº§åˆ†ç±»å½’å±ã€‚\n\n"
                    "å½“å‰åˆ†ç±»æ ‘å¦‚ä¸‹ï¼ˆä¸€çº§ -> äºŒçº§ï¼‰ï¼š\n"
                    f"{self.category_prompt_str}\n\n"
                    "è¦æ±‚ï¼š\n"
                    "1) è‹¥å®Œå…¨åŒ¹é…ç°æœ‰äºŒçº§åˆ†ç±»ï¼Œç›´æ¥å›ç­”ï¼š\n"
                    "   å½’å±ï¼š<ä¸€çº§ç±»åˆ«> -> <äºŒçº§å­åˆ†ç±»>\n"
                    "2) è‹¥éœ€æ–°å»ºäºŒçº§åˆ†ç±»ï¼Œå›ç­”ï¼š\n"
                    "   æ–°å¢äºŒçº§ï¼š<ä¸€çº§ç±»åˆ«> -> <æ–°å­åˆ†ç±»å>\n"
                    "3) è‹¥éœ€æ–°å»ºä¸€çº§ç±»åˆ«ï¼Œå›ç­”ï¼š\n"
                    "   æ–°å¢ä¸€çº§ï¼š<æ–°ä¸€çº§ç±»åˆ«> -> [<å­åˆ†ç±»1>, <å­åˆ†ç±»2>, ...]\n"
                    "4) ç”¨ä¸€å¥è¯è¯´æ˜åˆ¤æ–­ç†ç”±ã€‚"
                ),
                importance=1,
                description="è®ºæ–‡äºŒçº§åˆ†ç±»å½’å±",
                domain="both"
            ),
            
            # RF ICä¸“ç”¨é€Ÿè¯»é—®é¢˜ï¼ˆç®€åŒ–ç‰ˆï¼‰
            PaperQuestion(
                id="rf_ic_design_and_metrics",
                question="è¯·ç®€è¦åˆ†æRF ICè®ºæ–‡çš„æŠ€æœ¯è¦ç‚¹ï¼š1) ç”µè·¯æ¶æ„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ2) ä¸»è¦æ€§èƒ½æŒ‡æ ‡å¦‚ä½•ï¼Ÿ3) è®¾è®¡åˆ›æ–°ç‚¹åœ¨å“ªé‡Œï¼Ÿ",
                importance=4,
                description="RF ICæŠ€æœ¯è¦ç‚¹æ¦‚è¿°",
                domain="rf_ic"
            ),
            PaperQuestion(
                id="rf_ic_applications_challenges_future",
                question="è¯·è¯„ä¼°RF ICè®ºæ–‡çš„åº”ç”¨ä»·å€¼ï¼š1) ç›®æ ‡åº”ç”¨åœºæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ2) æŠ€æœ¯éš¾ç‚¹åœ¨å“ªé‡Œï¼Ÿ3) äº§ä¸šåŒ–å‰æ™¯å¦‚ä½•ï¼Ÿ",
                importance=3,
                description="RF ICåº”ç”¨ä¸å‰æ™¯è¯„ä¼°",
                domain="rf_ic"
            ),
            PaperQuestion(
                id="rf_ic_ppt_md_summary",
                question=(
                    "è¯·è¾“å‡ºç”¨äºPPTçš„RF ICæ–¹å‘Markdownæç®€æ‘˜è¦ï¼ˆä»…æŒ‰å¦‚ä¸‹ç»“æ„ï¼Œå‹¿åµŒå…¥ä»£ç å—ï¼‰ï¼š\n\n"
                    "# æ€»è¿°ï¼ˆ1 è¡Œï¼‰\n"
                    "- ç”¨ä¸€å¥è¯æ¦‚æ‹¬è¯¥ç”µè·¯/ç³»ç»Ÿåšäº†ä»€ä¹ˆã€ä¸ºä½•æœ‰æ•ˆ\n\n"
                    "# ç”µè·¯/è®¾è®¡è¦ç‚¹ï¼ˆ3-5æ¡ï¼‰\n"
                    "- æ ¸å¿ƒæ¨¡å—/ä¿¡å·æµ/å…³é”®è®¾è®¡\n\n"
                    "# æ€§èƒ½ä¸åº”ç”¨\n"
                    "- æŒ‡æ ‡/åœºæ™¯/æ”¶ç›Š"
                ),
                importance=3,
                description="RF IC PPT ç”¨æç®€Markdownæ‘˜è¦",
                domain="rf_ic"
            ),
        ]

        # æŒ‰é‡è¦æ€§æ’åº
        self.questions.sort(key=lambda q: q.importance, reverse=True)

    def _classify_paper_domain(self) -> Generator:
        """ä½¿ç”¨LLMå¯¹è®ºæ–‡è¿›è¡Œä¸»é¢˜åˆ†ç±»ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºRF ICç›¸å…³è®ºæ–‡"""
        try:
            classification_prompt = f"""è¯·åˆ†æä»¥ä¸‹è®ºæ–‡å†…å®¹ï¼Œåˆ¤æ–­å…¶æ˜¯å¦å±äºå°„é¢‘é›†æˆç”µè·¯(RF IC)é¢†åŸŸï¼š

è®ºæ–‡å†…å®¹ç‰‡æ®µï¼š
{self.paper_content[:2000]}...

è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¿›è¡Œåˆ¤æ–­ï¼š
1. å¦‚æœè®ºæ–‡æ¶‰åŠå°„é¢‘å‰ç«¯ç”µè·¯ï¼ˆLNAã€PAã€æ··é¢‘å™¨ã€VCOã€PLLç­‰ï¼‰
2. å¦‚æœè®ºæ–‡æ¶‰åŠæ— çº¿é€šä¿¡ç³»ç»Ÿé›†æˆã€æ¯«ç±³æ³¢æŠ€æœ¯ã€å¤ªèµ«å…¹æŠ€æœ¯
3. å¦‚æœè®ºæ–‡æ¶‰åŠå°„é¢‘ç”µè·¯è®¾è®¡ã€åŠå¯¼ä½“å·¥è‰ºåœ¨å°„é¢‘åº”ç”¨
4. å¦‚æœè®ºæ–‡æ¶‰åŠå°„é¢‘æ€§èƒ½æŒ‡æ ‡ï¼ˆå™ªå£°ç³»æ•°ã€çº¿æ€§åº¦ã€æ•ˆç‡ç­‰ï¼‰
5. å¦‚æœè®ºæ–‡æ¶‰åŠåˆ°ä½¿ç”¨MLæˆ–è€…ä¸€ç³»åˆ—EDAå·¥å…·ï¼Œæ¶‰åŠäººå·¥æ™ºèƒ½ï¼Œé‚£ä¹ˆå°±æ˜¯GENERALï¼Œå³æ‰€æœ‰AI+RFICä¹Ÿæ˜¯GENERAL

è¯·åªå›ç­”ï¼š"RF_IC" æˆ– "GENERAL"ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

            response = yield from request_gpt_model_in_new_thread_with_ui_alive(
                inputs=classification_prompt,
                inputs_show_user="æ­£åœ¨åˆ†æè®ºæ–‡ä¸»é¢˜åˆ†ç±»...",
                llm_kwargs=self.llm_kwargs,
                chatbot=self.chatbot,
                history=[],
                sys_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®ºæ–‡åˆ†ç±»åŠ©æ‰‹ï¼Œè¯·æ ¹æ®è®ºæ–‡å†…å®¹å‡†ç¡®åˆ¤æ–­å…¶æ‰€å±é¢†åŸŸã€‚"
            )

            if response and isinstance(response, str):
                response = response.strip().upper()
                if "RF_IC" in response:
                    self.paper_domain = "rf_ic"
                    self.chatbot.append(["ä¸»é¢˜åˆ†ç±»", "æ£€æµ‹åˆ°RF ICç›¸å…³è®ºæ–‡ï¼Œå°†ä½¿ç”¨ä¸“ä¸šRF ICåˆ†æç­–ç•¥"])
                else:
                    self.paper_domain = "general"
                    self.chatbot.append(["ä¸»é¢˜åˆ†ç±»", "æ£€æµ‹åˆ°é€šç”¨è®ºæ–‡ï¼Œå°†ä½¿ç”¨é€šç”¨åˆ†æç­–ç•¥"])
            else:
                self.paper_domain = "general"
                self.chatbot.append(["ä¸»é¢˜åˆ†ç±»", "æ— æ³•ç¡®å®šä¸»é¢˜ï¼Œä½¿ç”¨é€šç”¨åˆ†æç­–ç•¥"])

            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return True

        except Exception as e:
            self.paper_domain = "general"
            self.chatbot.append(["åˆ†ç±»é”™è¯¯", f"ä¸»é¢˜åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨ç­–ç•¥: {str(e)}"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return False

    def _get_domain_specific_questions(self) -> List[PaperQuestion]:
        """æ ¹æ®è®ºæ–‡é¢†åŸŸè·å–ç›¸åº”çš„é—®é¢˜åˆ—è¡¨"""
        if self.paper_domain == "rf_ic":
            # RF ICè®ºæ–‡ï¼šåŒ…å«RF ICä¸“ç”¨é—®é¢˜å’Œæ ¸å¿ƒé€šç”¨é—®é¢˜
            return [q for q in self.questions if q.domain in ["both", "rf_ic"]]
        else:
            # é€šç”¨è®ºæ–‡ï¼šåªåŒ…å«é€šç”¨é—®é¢˜
            return [q for q in self.questions if q.domain in ["both", "general"]]

    def _get_domain_specific_system_prompt(self) -> str:
        """æ ¹æ®è®ºæ–‡é¢†åŸŸè·å–ç›¸åº”çš„ç³»ç»Ÿæç¤º"""
        if self.paper_domain == "rf_ic":
            return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°„é¢‘é›†æˆç”µè·¯(RF IC)åˆ†æä¸“å®¶ï¼Œå…·æœ‰æ·±åšçš„ç”µè·¯è®¾è®¡ã€åŠå¯¼ä½“å·¥è‰ºå’Œæ— çº¿é€šä¿¡ç³»ç»ŸçŸ¥è¯†ã€‚è¯·ä»RF ICä¸“ä¸šè§’åº¦æ·±å…¥åˆ†æè®ºæ–‡ï¼Œä½¿ç”¨å‡†ç¡®çš„æœ¯è¯­ï¼Œæä¾›æœ‰è§åœ°çš„æŠ€æœ¯è¯„ä¼°ã€‚"""
        else:
            return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘ç ”è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œéœ€è¦ä»”ç»†é˜…è¯»è®ºæ–‡å†…å®¹å¹¶å›ç­”é—®é¢˜ã€‚è¯·ä¿æŒå®¢è§‚ã€å‡†ç¡®ï¼Œå¹¶åŸºäºè®ºæ–‡å†…å®¹æä¾›æ·±å…¥åˆ†æã€‚"""

    def _get_domain_specific_analysis_prompt(self, question: PaperQuestion) -> str:
        """æ ¹æ®è®ºæ–‡é¢†åŸŸå’Œé—®é¢˜ç”Ÿæˆç›¸åº”çš„åˆ†ææç¤º"""
        if self.paper_domain == "rf_ic":
            return f"""è¯·åŸºäºå·²è®°ä½çš„å°„é¢‘é›†æˆç”µè·¯è®ºæ–‡å…¨æ–‡ï¼Œä»RF ICä¸“ä¸šè§’åº¦ç®€è¦å›ç­”ï¼š

é—®é¢˜ï¼š{question.question}

è¯·ä¿æŒç®€æ´æ˜äº†ï¼Œé‡ç‚¹å…³æ³¨æŠ€æœ¯åˆ›æ–°ç‚¹å’Œåº”ç”¨ä»·å€¼ã€‚"""
        else:
            return f"è¯·åŸºäºå·²è®°ä½çš„è®ºæ–‡å…¨æ–‡ç®€è¦å›ç­”ï¼š{question.question}"

    # ---------- å…³é”®è¯åº“å·¥å…·ï¼ˆä¸ Batch_Paper_Reading ä¿æŒä¸€è‡´ï¼‰ ----------
    def _get_keywords_db_path(self) -> str:
        return os.path.join(os.path.dirname(__file__), 'keywords.txt')

    def _load_keywords_db(self) -> List[str]:
        path = self._get_keywords_db_path()
        if os.path.exists(path):
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    return [line.strip() for line in f if line.strip()]
            except Exception:
                return []
        return []

    def _save_keywords_db(self, keywords: List[str]):
        path = self._get_keywords_db_path()
        with open(path, 'w', encoding='utf-8') as f:
            for kw in sorted(set(keywords), key=lambda x: x.lower()):
                f.write(kw + '\n')

    def _normalize_keyword(self, kw: str) -> str:
        kw = kw.strip()
        # è‹±æ–‡å…³é”®è¯ï¼šç»Ÿä¸€å°å†™ï¼Œå»é™¤å¤šä½™ç©ºç™½ä¸å°¾éƒ¨æ ‡ç‚¹
        kw = re.sub(r'[\s\u3000]+', ' ', kw)
        kw = kw.strip().strip('.,;:')
        return kw.lower()

    def _find_similar_in_db(self, db: List[str], new_kw: str, threshold: float = 0.88) -> str:
        if not new_kw:
            return None
        candidates = difflib.get_close_matches(new_kw, [self._normalize_keyword(k) for k in db], n=1, cutoff=threshold)
        if candidates:
            # æ˜ å°„å›åŸå§‹å¤§å°å†™å½¢å¼ï¼ˆä¼˜å…ˆç¬¬ä¸€ä¸ªåŒ¹é…é¡¹ï¼‰
            norm = candidates[0]
            for k in db:
                if self._normalize_keyword(k) == norm:
                    return k
        return None

    def _merge_keywords_with_db(self, extracted_keywords: List[str]) -> Tuple[List[str], List[str]]:
        """
        å°†æå–çš„å…³é”®è¯ä¸å…³é”®è¯åº“è¿›è¡Œåˆå¹¶å»é‡ï¼Œè¿”å›ï¼š
        - canonical_keywords: æ›¿æ¢/åˆå¹¶åçš„å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨äºå†™å› YAMLï¼‰
        - updated_db: æ›´æ–°åçš„å…³é”®è¯åº“ï¼ˆè‹¥æœ‰æ–°å¢ï¼‰
        """
        db = self._load_keywords_db()
        canonical_list: List[str] = []

        for kw in extracted_keywords:
            clean = self._normalize_keyword(kw)
            if not clean:
                continue
            similar = self._find_similar_in_db(db, clean)
            if similar:
                # ä½¿ç”¨åº“ä¸­çš„æ ‡å‡†è¯å½¢
                if similar not in canonical_list:
                    canonical_list.append(similar)
            else:
                # æ–°å…³é”®è¯ï¼šåŠ å…¥åº“ä¸ç»“æœ
                db.append(kw)
                if kw not in canonical_list:
                    canonical_list.append(kw)

        # ä¿å­˜æ›´æ–°çš„å…³é”®è¯åº“
        self._save_keywords_db(db)
        return canonical_list, db

    def _update_category_json(self, llm_answer: str):
        """
        è§£æ LLM è¿”å›çš„å½’å±/æ–°å¢æŒ‡ä»¤ï¼Œå¹¶æ›´æ–° paper.json
        """
        json_path = os.path.join(os.path.dirname(__file__), 'paper.json')

        # 1) æ–°å¢ä¸€çº§
        m1 = re.search(r'æ–°å¢ä¸€çº§ï¼š(.+?) *-> *\[(.+?)\]', llm_answer)
        if m1:
            new_main = m1.group(1).strip()
            new_subs = [s.strip() for s in m1.group(2).split(',')]
            self.category_tree[new_main] = new_subs
        else:
            # 2) æ–°å¢äºŒçº§
            m2 = re.search(r'æ–°å¢äºŒçº§ï¼š(.+?) *-> *(.+)', llm_answer)
            if m2:
                main_cat = m2.group(1).strip()
                new_sub = m2.group(2).strip()
                if main_cat in self.category_tree and new_sub not in self.category_tree[main_cat]:
                    self.category_tree[main_cat].append(new_sub)

        # å†™å›
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.category_tree, f, ensure_ascii=False, indent=4)

    def _clean_yaml_list(self, yaml_text: str, list_fields: List[str]) -> str:
        """æ¸…ç†YAMLæ–‡æœ¬ä¸­åˆ—è¡¨å­—æ®µçš„Noneå€¼"""
        import re
        for field in list_fields:
            # åŒ¹é…åˆ—è¡¨å­—æ®µçš„æ¨¡å¼
            pattern = rf"^{field}:\s*\[(.*?)\]\s*$"
            match = re.search(pattern, yaml_text, flags=re.MULTILINE)
            if match:
                inner_content = match.group(1).strip()
                if inner_content:
                    # è§£æåˆ—è¡¨å†…å®¹ï¼Œè¿‡æ»¤æ‰Noneå€¼
                    items = [item.strip().strip('"\'') for item in inner_content.split(',')]
                    # è¿‡æ»¤æ‰Noneã€ç©ºå­—ç¬¦ä¸²å’Œ"None"
                    filtered_items = [item for item in items if item and item.lower() != 'none']
                    if filtered_items:
                        # é‡æ–°æ„å»ºåˆ—è¡¨ï¼Œä¿æŒå¼•å·æ ¼å¼
                        rebuilt = ', '.join([f'"{item}"' for item in filtered_items])
                        yaml_text = re.sub(pattern, f"{field}: [{rebuilt}]", yaml_text, flags=re.MULTILINE)
                    else:
                        # å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œç§»é™¤è¯¥å­—æ®µ
                        yaml_text = re.sub(rf"^{field}:\s*\[.*?\]\s*$\n?", "", yaml_text, flags=re.MULTILINE)
        return yaml_text

    def _generate_yaml_header(self) -> Generator:
        """åŸºäºè®ºæ–‡å†…å®¹ä¸å·²å¾—åˆ†æï¼Œç”Ÿæˆ YAML å¤´éƒ¨ï¼ˆæ ¸å¿ƒå…ƒä¿¡æ¯ï¼‰"""
        try:
            prompt = (
                "è¯·åŸºäºä»¥ä¸‹è®ºæ–‡å†…å®¹ä¸åˆ†æè¦ç‚¹ï¼Œæå–è®ºæ–‡æ ¸å¿ƒå…ƒä¿¡æ¯å¹¶è¾“å‡º YAML Front Matterï¼š\n\n"
                f"è®ºæ–‡å…¨æ–‡å†…å®¹ç‰‡æ®µï¼š\n{self.paper_content}\n\n"
                "è‹¥æœ‰å¯ç”¨çš„åˆ†æè¦ç‚¹ï¼š\n"
            )

            # å°†å·²æœ‰ç»“æœç®€è¦æ‹¼æ¥ï¼Œè¾…åŠ©æå–
            for q in self.questions:
                if q.id in self.results:
                    prompt += f"- {q.description}: {self.results[q.id][:400]}\n"

            prompt += (
                "\nä¸¥æ ¼è¾“å‡º YAMLï¼ˆä¸ä½¿ç”¨ä»£ç å—å›´æ ï¼‰ï¼Œå­—æ®µå¦‚ä¸‹ï¼š\n"
                "title: åŸæ–‡æ ‡é¢˜ï¼ˆå°½é‡è‹±æ–‡åŸé¢˜,æ ‡é¢˜éœ€è¦æœ‰å¼•å·åŒ…è£¹ï¼‰\n"
                "title_zh: ä¸­æ–‡æ ‡é¢˜ï¼ˆè‹¥å¯ï¼‰\n"
                "authors: [ä½œè€…è‹±æ–‡ååˆ—è¡¨]\n"
                "affiliation_zh: ç¬¬ä¸€ä½œè€…å•ä½ï¼ˆä¸­æ–‡ï¼‰\n"
                "keywords: [è‹±æ–‡å…³é”®è¯åˆ—è¡¨]\n"
                "urls: [è®ºæ–‡é“¾æ¥, Githubé“¾æ¥æˆ–None]\n"
                "doi: [DOIé“¾æ¥, None]\n"
                "journal_or_conference: [æœŸåˆŠæˆ–ä¼šè®®åç§°, None]\n"
                "year: [å¹´ä»½, None]\n"
                "source_code: [æºç é“¾æ¥, None]\n"
                "read_status: [å·²é˜…è¯», æœªé˜…è¯»]\n"
                "ä»…è¾“å‡ºä»¥ --- å¼€å§‹ã€ä»¥ --- ç»“æŸçš„ YAML Front Matterï¼Œä¸è¦é™„åŠ å…¶ä»–æ–‡æœ¬ã€‚read_statusé»˜è®¤æœªé˜…è¯»ã€‚"
            )

            yaml_str = yield from request_gpt_model_in_new_thread_with_ui_alive(
                inputs=prompt,
                inputs_show_user="ç”Ÿæˆè®ºæ–‡æ ¸å¿ƒä¿¡æ¯ YAML å¤´",
                llm_kwargs=self.llm_kwargs,
                chatbot=self.chatbot,
                history=[],
                sys_prompt=(
                    "ä½ æ˜¯è®ºæ–‡ä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚è¯·ä»…è¾“å‡º YAML Front Matterï¼Œ"
                    "é”®åå›ºå®šä¸”é¡ºåºä¸é™ï¼Œæ³¨æ„ authors/keywords/urls åº”ä¸ºåˆ—è¡¨ã€‚"
                )
            )

            # ç®€å•æ ¡éªŒï¼Œç¡®ä¿åŒ…å« YAML åˆ†éš”ç¬¦
            if isinstance(yaml_str, str) and yaml_str.strip().startswith("---") and yaml_str.strip().endswith("---"):
                # è§£æå¹¶è§„èŒƒåŒ– keywords åˆ—è¡¨
                text = yaml_str.strip()
                m = re.search(r"^keywords:\s*\[(.*?)\]\s*$", text, flags=re.MULTILINE)
                if m:
                    inner = m.group(1).strip()
                    # ç®€å•è§£æåˆ—è¡¨å†…å®¹ï¼Œæ”¯æŒå¸¦å¼•å·æˆ–ä¸å¸¦å¼•å·çš„è‹±æ–‡å…³é”®è¯
                    # æ‹†åˆ†é€—å·ï¼ŒåŒæ—¶å»æ‰åŒ…è£¹å¼•å·
                    raw_list = [x.strip().strip('\"\'\'') for x in inner.split(',') if x.strip()]
                    merged, _ = self._merge_keywords_with_db(raw_list)
                    # ä»¥åŸæ ·å¼å†™å›ï¼ˆä½¿ç”¨å¼•å·åŒ…è£¹ï¼Œé¿å… YAML è§£æé—®é¢˜ï¼‰
                    rebuilt = ', '.join([f'\"{k}\"' for k in merged])
                    text = re.sub(r"^keywords:\s*\[(.*?)\]\s*$", f"keywords: [{rebuilt}]", text, flags=re.MULTILINE)
                
                # æ³¨å…¥"å½’å±"äºŒçº§åˆ†ç±»ï¼ˆè‹¥å¯ç”¨ï¼‰
                try:
                    if getattr(self, 'secondary_category', None):
                        escaped = self.secondary_category.replace('\"', '\\\"')
                        if text.endswith("---"):
                            text = text[:-3].rstrip() + f"\nsecondary_category: \"{escaped}\"\n---"
                except Exception:
                    pass
                
                # ç®€åŒ–æ˜Ÿçº§è¯„åˆ†æ˜ å°„ï¼ˆé€Ÿè¯»ç‰ˆï¼‰ï¼šå…ˆç§»é™¤å·²æœ‰ starsï¼Œå†æŒ‰è¯„çº§è¿½åŠ ä¸€æ¬¡
                try:
                    judge = self.results.get("worth_reading_judgment", "")
                    stars = "â­â­â­"  # é»˜è®¤
                    if isinstance(judge, str) and judge:
                        if "å¼ºçƒˆæ¨è" in judge:
                            stars = "â­â­â­â­â­"
                        elif "æ¨è" in judge:
                            stars = "â­â­â­â­"
                        elif "è°¨æ…" in judge:
                            stars = "â­â­"
                        elif "ä¸æ¨è" in judge:
                            stars = "â­"

                    # ç§»é™¤åŸæœ‰çš„ stars è¡Œï¼ˆæ ‡é‡æˆ–åˆ—è¡¨å½¢å¼ï¼‰
                    text = re.sub(r"^stars:\s*\[.*?\]\s*$\n?", "", text, flags=re.MULTILINE)
                    text = re.sub(r"^stars:\s*.*$\n?", "", text, flags=re.MULTILINE)

                    # ç»Ÿä¸€ä»…è¿½åŠ ä¸€æ¬¡åˆ—è¡¨å½¢å¼çš„ stars å­—æ®µ
                    if text.endswith("---"):
                        text = text[:-3].rstrip() + f"\nstars: [\"{stars}\"]\n---"
                except Exception:
                    pass
                
                # æ¸…ç†åˆ—è¡¨å­—æ®µä¸­çš„Noneå€¼
                list_fields = ["urls", "doi", "journal_or_conference", "year", "source_code"]
                text = self._clean_yaml_list(text, list_fields)
                
                # å¼ºåˆ¶è®¾ç½® read_status ä¸º æœªé˜…è¯»ï¼ˆæ— è®ºæ¨¡å‹å¦‚ä½•è¿”å›ï¼‰
                try:
                    if re.search(r"^read_status\s*:", text, flags=re.MULTILINE):
                        text = re.sub(r"^read_status\s*:.*$", 'read_status: "æœªé˜…è¯»"', text, flags=re.MULTILINE)
                    else:
                        if text.endswith("---"):
                            text = text[:-3].rstrip() + '\n' + 'read_status: "æœªé˜…è¯»"' + '\n---'
                except Exception:
                    pass
                
                return text
            return None

        except Exception as e:
            self.chatbot.append(["è­¦å‘Š", f"ç”Ÿæˆ YAML å¤´å¤±è´¥: {str(e)}"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return None

    def _load_paper(self, paper_path: str) -> Generator:
        from crazy_functions.doc_fns.text_content_loader import TextContentLoader
        """åŠ è½½è®ºæ–‡å†…å®¹"""
        yield from update_ui(chatbot=self.chatbot, history=self.history)

        # ä¿å­˜è®ºæ–‡æ–‡ä»¶è·¯å¾„
        self.paper_file_path = paper_path

        # ä½¿ç”¨TextContentLoaderè¯»å–æ–‡ä»¶
        loader = TextContentLoader(self.chatbot, self.history)

        yield from loader.execute_single_file(paper_path)

        # è·å–åŠ è½½çš„å†…å®¹
        if len(self.history) >= 2 and self.history[-2]:
            self.paper_content = self.history[-2]
            # æ³¨å…¥ä¸€æ¬¡å…¨æ–‡åˆ°ä¸Šä¸‹æ–‡å†å²ï¼Œåç»­å¤šè½®ä»…å‘é€é—®é¢˜
            try:
                remembered = (
                    "è¯·è®°ä½ä»¥ä¸‹è®ºæ–‡å…¨æ–‡ï¼Œåç»­æ‰€æœ‰é—®é¢˜ä»…åŸºäºæ­¤å†…å®¹å›ç­”ï¼Œä¸è¦é‡å¤è¾“å‡ºåŸæ–‡ï¼š\n\n"
                    f"{self.paper_content}"
                )
                self.context_history = [remembered, "å·²æ¥æ”¶å¹¶è®°ä½è®ºæ–‡å†…å®¹"]
            except Exception:
                self.context_history = []
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return True
        else:
            self.chatbot.append(["é”™è¯¯", "æ— æ³•è¯»å–è®ºæ–‡å†…å®¹ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return False

    def _analyze_question(self, question: PaperQuestion) -> Generator:
        """åˆ†æå•ä¸ªé—®é¢˜ - æ ¹æ®é¢†åŸŸåŠ¨æ€è°ƒæ•´åˆ†æç­–ç•¥"""
        try:
            # æ ¹æ®è®ºæ–‡é¢†åŸŸç”Ÿæˆç›¸åº”çš„åˆ†ææç¤º
            prompt = self._get_domain_specific_analysis_prompt(question)
            
            # è·å–é¢†åŸŸç‰¹å®šçš„ç³»ç»Ÿæç¤º
            sys_prompt = self._get_domain_specific_system_prompt()

            response = yield from request_gpt_model_in_new_thread_with_ui_alive(
                inputs=prompt,
                inputs_show_user=question.question,
                llm_kwargs=self.llm_kwargs,
                chatbot=self.chatbot,
                history=self.context_history or [],
                sys_prompt=sys_prompt
            )

            if response:
                self.results[question.id] = response
                # è®°å½•æœ¬è½®äº¤äº’çš„è¾“å…¥ä¸è¾“å‡ºç”¨äºtokenä¼°ç®—
                self._token_inputs.append(prompt)
                self._token_outputs.append(response)

                # å¦‚æœæ˜¯åˆ†ç±»å½’å±é—®é¢˜ï¼Œè‡ªåŠ¨æ›´æ–° paper.json
                if question.id == "category_assignment":
                    self._update_category_json(response)
                    try:
                        mcat = re.search(r"^å½’å±ï¼š\s*([^\r\n]+)", response, flags=re.MULTILINE)
                        if mcat:
                            self.secondary_category = mcat.group(1).strip()
                    except Exception:
                        pass

                return True
            return False

        except Exception as e:
            self.chatbot.append(["é”™è¯¯", f"åˆ†æé—®é¢˜æ—¶å‡ºé”™: {str(e)}"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return False

    def _generate_summary(self) -> Generator:
        """ç”Ÿæˆé€Ÿè¯»ç­›é€‰æŠ¥å‘Š"""
        domain_label = "RF IC" if self.paper_domain == "rf_ic" else "é€šç”¨"
        self.chatbot.append(["ç”Ÿæˆé€Ÿè¯»æŠ¥å‘Š", f"æ­£åœ¨æ•´åˆ{domain_label}è®ºæ–‡é€Ÿè¯»åˆ†æç»“æœï¼Œç”Ÿæˆç­›é€‰æŠ¥å‘Š..."])
        yield from update_ui(chatbot=self.chatbot, history=self.history)

        if self.paper_domain == "rf_ic":
            summary_prompt = """è¯·åŸºäºä»¥ä¸‹å¯¹RF ICè®ºæ–‡çš„é€Ÿè¯»åˆ†æï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„è®ºæ–‡ç­›é€‰æŠ¥å‘Šã€‚

æŠ¥å‘Šè¦æ±‚ï¼š
1. ç®€æ˜æ‰¼è¦åœ°æ€»ç»“è®ºæ–‡çš„æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹
2. çªå‡ºRF ICè®¾è®¡çš„åˆ›æ–°ç‚¹å’Œä»·å€¼
3. è¯„ä¼°æŠ€æœ¯çš„åº”ç”¨å‰æ™¯å’Œæˆç†Ÿåº¦
4. æ˜ç¡®ç»™å‡ºæ˜¯å¦å€¼å¾—ç²¾è¯»çš„å»ºè®®åŠç†ç”±

è¯·ä¿æŒç®€æ´æ˜äº†ï¼Œé€‚åˆå¿«é€Ÿå†³ç­–ã€‚"""
        else:
            summary_prompt = """è¯·åŸºäºä»¥ä¸‹å¯¹è®ºæ–‡çš„é€Ÿè¯»åˆ†æï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„è®ºæ–‡ç­›é€‰æŠ¥å‘Šã€‚

æŠ¥å‘Šè¦æ±‚ï¼š
1. ç®€æ˜æ‰¼è¦åœ°æ€»ç»“è®ºæ–‡çš„æ ¸å¿ƒå†…å®¹
2. çªå‡ºç ”ç©¶çš„ä¸»è¦åˆ›æ–°ç‚¹å’Œè´¡çŒ®
3. è¯„ä¼°ç ”ç©¶çš„ä»·å€¼å’Œå½±å“
4. æ˜ç¡®ç»™å‡ºæ˜¯å¦å€¼å¾—ç²¾è¯»çš„å»ºè®®åŠç†ç”±

è¯·ä¿æŒç®€æ´æ˜äº†ï¼Œé€‚åˆå¿«é€Ÿå†³ç­–ã€‚"""

        for q in self.questions:
            if q.id in self.results:
                summary_prompt += f"\n\n{q.description}:\n{self.results[q.id]}"

        try:
            # ä½¿ç”¨å•çº¿ç¨‹ç‰ˆæœ¬çš„è¯·æ±‚å‡½æ•°ï¼Œå¯ä»¥åœ¨å‰ç«¯å®æ—¶æ˜¾ç¤ºç”Ÿæˆç»“æœ
            response = yield from request_gpt_model_in_new_thread_with_ui_alive(
                inputs=summary_prompt,
                inputs_show_user=f"ç”Ÿæˆ{domain_label}è®ºæ–‡é€Ÿè¯»ç­›é€‰æŠ¥å‘Š",
                llm_kwargs=self.llm_kwargs,
                chatbot=self.chatbot,
                history=[],
                sys_prompt=f"ä½ æ˜¯ä¸€ä¸ª{'å°„é¢‘é›†æˆç”µè·¯é¢†åŸŸçš„ä¸“å®¶' if self.paper_domain == 'rf_ic' else 'ç§‘ç ”è®ºæ–‡è¯„å®¡ä¸“å®¶'}ï¼Œè¯·å°†é€Ÿè¯»åˆ†ææ•´åˆä¸ºä¸€ä»½ç®€æ´çš„ç­›é€‰æŠ¥å‘Šã€‚æŠ¥å‘Šåº”å½“é‡ç‚¹çªå‡ºè®ºæ–‡çš„æ ¸å¿ƒä»·å€¼å’Œåˆ›æ–°ç‚¹ï¼Œå¹¶æ˜ç¡®ç»™å‡ºæ˜¯å¦å€¼å¾—ç²¾è¯»çš„å»ºè®®ã€‚ä¿æŒç®€æ´æ˜äº†ï¼Œä¾¿äºå¿«é€Ÿå†³ç­–ã€‚"
            )

            if response:
                # è®°å½•æŠ¥å‘Šç”Ÿæˆçš„tokenä½¿ç”¨
                self._token_inputs.append(summary_prompt)
                self._token_outputs.append(response)
                return response
            return "é€Ÿè¯»æŠ¥å‘Šç”Ÿæˆå¤±è´¥"

        except Exception as e:
            self.chatbot.append(["é”™è¯¯", f"ç”Ÿæˆé€Ÿè¯»æŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return "é€Ÿè¯»æŠ¥å‘Šç”Ÿæˆå¤±è´¥: " + str(e)

    def save_report(self, report: str, paper_file_path: str = None) -> str:
        """ä¿å­˜åˆ†ææŠ¥å‘Šï¼Œè¿”å›ä¿å­˜çš„æ–‡ä»¶è·¯å¾„"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # è·å–PDFæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        domain_prefix = "RF_IC" if self.paper_domain == "rf_ic" else "é€šç”¨"
        pdf_filename = f"æœªçŸ¥{domain_prefix}è®ºæ–‡"
        if paper_file_path and os.path.exists(paper_file_path):
            pdf_filename = os.path.splitext(os.path.basename(paper_file_path))[0]
            # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸­æ–‡å’Œä¸‹åˆ’çº¿
            import re
            pdf_filename = re.sub(r'[^\w\u4e00-\u9fff]', '_', pdf_filename)
            # å¦‚æœæ–‡ä»¶åè¿‡é•¿ï¼Œæˆªå–å‰50ä¸ªå­—ç¬¦
            if len(pdf_filename) > 50:
                pdf_filename = pdf_filename[:50]

        # ä¿å­˜ä¸ºMarkdownæ–‡ä»¶
        try:
            md_parts = []
            # æ ‡é¢˜ä¸æ•´ä½“æŠ¥å‘Šï¼ˆç¨ååœ¨å‰åŠ å…¥ YAML å¤´ï¼‰
            domain_title = "å°„é¢‘é›†æˆç”µè·¯è®ºæ–‡é€Ÿè¯»ç­›é€‰æŠ¥å‘Š" if self.paper_domain == "rf_ic" else "å­¦æœ¯è®ºæ–‡é€Ÿè¯»ç­›é€‰æŠ¥å‘Š"
            md_parts.append(f"{domain_title}\n\n{report}")

            # é€Ÿè¯»æŠ¥å‘Šï¼šç®€æ´ç»„ç»‡å†…å®¹ï¼Œé‡ç‚¹å…³æ³¨ç­›é€‰å†³ç­–
            if self.paper_domain == "rf_ic":
                # RF ICè®ºæ–‡é€Ÿè¯»ï¼šæŒ‰é‡è¦æ€§ç»„ç»‡
                # 1. æ ¸å¿ƒåˆ†æ
                core_questions = ["research_methods_and_data", "findings_innovations_and_impact", "rf_ic_design_and_metrics", "rf_ic_applications_challenges_future"]
                for q_id in core_questions:
                    for q in self.questions:
                        if q.id == q_id and q.id in self.results:
                            md_parts.append(f"\n\n## ğŸ“‹ {q.description}\n\n{self.results[q.id]}")
                            break
                
                # 2. é˜…è¯»å»ºè®®ï¼ˆæœ€é‡è¦ï¼‰
                if "worth_reading_judgment" in self.results:
                    md_parts.append(f"\n\n## ğŸ¯ æ˜¯å¦å€¼å¾—ç²¾è¯»\n\n{self.results['worth_reading_judgment']}")
                
                # 3. åˆ†ç±»ä¿¡æ¯
                if "category_assignment" in self.results:
                    md_parts.append(f"\n\n## ğŸ“‚ è®ºæ–‡åˆ†ç±»\n\n{self.results['category_assignment']}")

                # 4. PPT æ‘˜è¦
                if "rf_ic_ppt_md_summary" in self.results:
                    md_parts.append(f"\n\n## ğŸ“ RF IC PPT æ‘˜è¦\n\n{self.results['rf_ic_ppt_md_summary']}")
            else:
                # é€šç”¨è®ºæ–‡é€Ÿè¯»ï¼šæŒ‰é‡è¦æ€§ç»„ç»‡
                # 1. æ ¸å¿ƒåˆ†æ
                core_questions = ["research_methods_and_data", "findings_innovations_and_impact"]
                for q_id in core_questions:
                    for q in self.questions:
                        if q.id == q_id and q.id in self.results:
                            md_parts.append(f"\n\n## ğŸ“‹ {q.description}\n\n{self.results[q.id]}")
                            break
                
                # 2. é˜…è¯»å»ºè®®ï¼ˆæœ€é‡è¦ï¼‰
                if "worth_reading_judgment" in self.results:
                    md_parts.append(f"\n\n## ğŸ¯ æ˜¯å¦å€¼å¾—ç²¾è¯»\n\n{self.results['worth_reading_judgment']}")
                
                # 3. åˆ†ç±»ä¿¡æ¯
                if "category_assignment" in self.results:
                    md_parts.append(f"\n\n## ğŸ“‚ è®ºæ–‡åˆ†ç±»\n\n{self.results['category_assignment']}")

                # 4. PPT æ‘˜è¦
                if "ppt_md_summary" in self.results:
                    md_parts.append(f"\n\n## ğŸ“ PPT æ‘˜è¦\n\n{self.results['ppt_md_summary']}")

            md_content = "".join(md_parts)

            # è‹¥å·²ç”Ÿæˆ YAML å¤´ï¼Œåˆ™ç½®äºæ–‡é¦–
            if hasattr(self, 'yaml_header') and self.yaml_header:
                md_content = f"{self.yaml_header}\n\n" + md_content

            # è¿½åŠ ç®€åŒ–çš„åˆ†æç»Ÿè®¡
            try:
                stats = estimate_token_usage(self._token_inputs, self._token_outputs, self.llm_kwargs.get('llm_model', 'gpt-3.5-turbo'))
                if stats and stats.get('sum_total_tokens', 0) > 0:
                    md_content += (
                        "\n\n## ğŸ“Š åˆ†æç»Ÿè®¡\n\n"
                        f"- åˆ†ææ¨¡å‹: {stats.get('model')}\n"
                        f"- Tokenæ¶ˆè€—: {stats.get('sum_total_tokens', 0)} tokens\n"
                    )
            except Exception:
                pass

            result_file = write_history_to_file(
                history=[md_content],
                file_basename=f"{timestamp}_{pdf_filename}_{domain_prefix}è§£è¯»æŠ¥å‘Š.md"
            )

            if result_file and os.path.exists(result_file):
                promote_file_to_downloadzone(result_file, chatbot=self.chatbot)
                return result_file
            else:
                return None
        except Exception as e:
            self.chatbot.append(["è­¦å‘Š", f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}"])
            update_ui(chatbot=self.chatbot, history=self.history)
            return None

    def analyze_paper(self, paper_path: str) -> Generator:
        """åˆ†æå•ç¯‡è®ºæ–‡ä¸»æµç¨‹"""
        # æ¯ç¯‡è®ºæ–‡ç‹¬ç«‹ç»Ÿè®¡ tokenï¼šé‡ç½®äº¤äº’è®°å½•
        self._token_inputs = []
        self._token_outputs = []
        # åŠ è½½è®ºæ–‡
        success = yield from self._load_paper(paper_path)
        if not success:
            return None

        # ä¸»é¢˜åˆ†ç±»åˆ¤æ–­
        yield from self._classify_paper_domain()

        # æ ¹æ®é¢†åŸŸè·å–ç›¸åº”çš„é—®é¢˜åˆ—è¡¨
        domain_questions = self._get_domain_specific_questions()

        # åˆ†æå…³é”®é—®é¢˜
        for question in domain_questions:
            yield from self._analyze_question(question)

        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        final_report = yield from self._generate_summary()

        # ç”Ÿæˆ YAML å¤´
        self.yaml_header = yield from self._generate_yaml_header()

        # ä¿å­˜æŠ¥å‘Š
        saved_file = self.save_report(final_report, self.paper_file_path)
        
        return saved_file


def _find_paper_files(path: str) -> List[str]:
    """æŸ¥æ‰¾è·¯å¾„ä¸­çš„æ‰€æœ‰è®ºæ–‡æ–‡ä»¶"""
    paper_files = []
    
    if os.path.isfile(path):
        # å¦‚æœæ˜¯å•ä¸ªæ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„æ ¼å¼
        file_ext = os.path.splitext(path)[1].lower()
        if file_ext in ['.pdf', '.docx', '.doc', '.txt', '.md', '.tex']:
            paper_files.append(path)
        return paper_files

    # å¦‚æœæ˜¯ç›®å½•ï¼Œé€’å½’æœç´¢æ‰€æœ‰æ”¯æŒçš„è®ºæ–‡æ–‡ä»¶
    if os.path.isdir(path):
        supported_extensions = ['.pdf', '.docx', '.doc', '.txt', '.md', '.tex']
        
        for root, dirs, files in os.walk(path):
            for file in files:
                file_path = os.path.join(root, file)
                file_ext = os.path.splitext(file)[1].lower()
                if file_ext in supported_extensions:
                    paper_files.append(file_path)
    
    return paper_files


def download_paper_by_id(paper_info, chatbot, history) -> str:
    """ä¸‹è½½è®ºæ–‡å¹¶è¿”å›ä¿å­˜è·¯å¾„"""
    from crazy_functions.review_fns.data_sources.scihub_source import SciHub
    id_type, paper_id = paper_info

    # åˆ›å»ºä¿å­˜ç›®å½• - ä½¿ç”¨æ—¶é—´æˆ³åˆ›å»ºå”¯ä¸€æ–‡ä»¶å¤¹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    user_name = chatbot.get_user() if hasattr(chatbot, 'get_user') else "default"
    from toolbox import get_log_folder, get_user
    base_save_dir = get_log_folder(get_user(chatbot), plugin_name='unified_paper_download')
    save_dir = os.path.join(base_save_dir, f"papers_{timestamp}")
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    save_path = Path(save_dir)

    chatbot.append([f"ä¸‹è½½è®ºæ–‡", f"æ­£åœ¨ä¸‹è½½{'arXiv' if id_type == 'arxiv' else 'DOI'} {paper_id} çš„è®ºæ–‡..."])
    update_ui(chatbot=chatbot, history=history)

    pdf_path = None

    try:
        if id_type == 'arxiv':
            # ä½¿ç”¨æ”¹è¿›çš„arxivæŸ¥è¯¢æ–¹æ³•
            formatted_id = format_arxiv_id(paper_id)
            paper_result = get_arxiv_paper(formatted_id)

            if not paper_result:
                chatbot.append([f"ä¸‹è½½å¤±è´¥", f"æœªæ‰¾åˆ°arXivè®ºæ–‡: {paper_id}"])
                update_ui(chatbot=chatbot, history=history)
                return None

            # ä¸‹è½½PDF
            filename = f"arxiv_{paper_id.replace('/', '_')}.pdf"
            pdf_path = str(save_path / filename)
            paper_result.download_pdf(filename=pdf_path)

        else:  # doi
            # ä¸‹è½½DOI
            sci_hub = SciHub(
                doi=paper_id,
                path=save_path
            )
            pdf_path = sci_hub.fetch()

        # æ£€æŸ¥ä¸‹è½½ç»“æœ
        if pdf_path and os.path.exists(pdf_path):
            promote_file_to_downloadzone(pdf_path, chatbot=chatbot)
            chatbot.append([f"ä¸‹è½½æˆåŠŸ", f"å·²æˆåŠŸä¸‹è½½è®ºæ–‡: {os.path.basename(pdf_path)}"])
            update_ui(chatbot=chatbot, history=history)
            return pdf_path
        else:
            chatbot.append([f"ä¸‹è½½å¤±è´¥", f"è®ºæ–‡ä¸‹è½½å¤±è´¥: {paper_id}"])
            update_ui(chatbot=chatbot, history=history)
            return None

    except Exception as e:
        chatbot.append([f"ä¸‹è½½é”™è¯¯", f"ä¸‹è½½è®ºæ–‡æ—¶å‡ºé”™: {str(e)}"])
        update_ui(chatbot=chatbot, history=history)
        return None


@CatchException
def ç»Ÿä¸€æ‰¹é‡è®ºæ–‡é€Ÿè¯»(txt: str, llm_kwargs: Dict, plugin_kwargs: Dict, chatbot: List,
             history: List, system_prompt: str, user_request: str):
    """ä¸»å‡½æ•° - ç»Ÿä¸€æ‰¹é‡è®ºæ–‡é€Ÿè¯»ï¼ˆæ”¯æŒä¸»é¢˜åˆ†ç±»ï¼‰"""
    # åˆå§‹åŒ–åˆ†æå™¨
    chatbot.append(["å‡½æ•°æ’ä»¶åŠŸèƒ½åŠä½¿ç”¨æ–¹å¼", "ç»Ÿä¸€æ‰¹é‡è®ºæ–‡é€Ÿè¯»ï¼šå¿«é€Ÿç­›é€‰è®ºæ–‡ï¼Œåˆ¤æ–­æ˜¯å¦å€¼å¾—ç²¾è¯»ã€‚æ™ºèƒ½è¯†åˆ«è®ºæ–‡ä¸»é¢˜ï¼ˆé€šç”¨/RF ICï¼‰ï¼Œä¸ºæ¯ç¯‡è®ºæ–‡ç”Ÿæˆç®€æ´çš„é€Ÿè¯»æŠ¥å‘Šã€‚ <br><br>ğŸ“‹ ä½¿ç”¨æ–¹å¼ï¼š<br>1ã€è¾“å…¥åŒ…å«å¤šä¸ªPDFæ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„<br>2ã€æˆ–è€…è¾“å…¥å¤šä¸ªè®ºæ–‡IDï¼ˆDOIæˆ–arXiv IDï¼‰ï¼Œç”¨é€—å·åˆ†éš”<br>3ã€ç‚¹å‡»æ’ä»¶å¼€å§‹å¿«é€Ÿç­›é€‰åˆ†æ<br><br>ğŸ¯ é€Ÿè¯»ç‰¹æ€§ï¼š<br>- å¿«é€Ÿè¯†åˆ«è®ºæ–‡æ ¸å¿ƒå†…å®¹å’Œåˆ›æ–°ç‚¹<br>- è‡ªåŠ¨ä¸»é¢˜åˆ†ç±»ï¼ˆé€šç”¨è®ºæ–‡ vs RF ICè®ºæ–‡ï¼‰<br>- æ˜ç¡®ç»™å‡ºæ˜¯å¦å€¼å¾—ç²¾è¯»çš„å»ºè®®<br>- ç®€æ´çš„æŠ¥å‘Šæ ¼å¼ï¼Œä¾¿äºå¿«é€Ÿå†³ç­–"])
    yield from update_ui(chatbot=chatbot, history=history)

    paper_files = []

    # æ£€æŸ¥è¾“å…¥æ˜¯å¦åŒ…å«è®ºæ–‡IDï¼ˆå¤šä¸ªIDç”¨é€—å·åˆ†éš”ï¼‰
    if ',' in txt:
        # å¤„ç†å¤šä¸ªè®ºæ–‡ID
        paper_ids = [id.strip() for id in txt.split(',') if id.strip()]
        chatbot.append(["æ£€æµ‹åˆ°å¤šä¸ªè®ºæ–‡ID", f"æ£€æµ‹åˆ° {len(paper_ids)} ä¸ªè®ºæ–‡IDï¼Œå‡†å¤‡æ‰¹é‡ä¸‹è½½..."])
        yield from update_ui(chatbot=chatbot, history=history)

        for i, paper_id in enumerate(paper_ids):
            paper_info = extract_paper_id(paper_id)
            if paper_info:
                chatbot.append([f"ä¸‹è½½è®ºæ–‡ {i+1}/{len(paper_ids)}", f"æ­£åœ¨ä¸‹è½½ {'arXiv' if paper_info[0] == 'arxiv' else 'DOI'} ID: {paper_info[1]}..."])
                yield from update_ui(chatbot=chatbot, history=history)

                paper_file = download_paper_by_id(paper_info, chatbot, history)
                if paper_file:
                    paper_files.append(paper_file)
                else:
                    chatbot.append([f"ä¸‹è½½å¤±è´¥", f"æ— æ³•ä¸‹è½½è®ºæ–‡: {paper_id}"])
                    yield from update_ui(chatbot=chatbot, history=history)
            else:
                chatbot.append([f"IDæ ¼å¼é”™è¯¯", f"æ— æ³•è¯†åˆ«è®ºæ–‡IDæ ¼å¼: {paper_id}"])
                yield from update_ui(chatbot=chatbot, history=history)
    else:
        # æ£€æŸ¥å•ä¸ªè®ºæ–‡ID
        paper_info = extract_paper_id(txt)
        if paper_info:
            # å•ä¸ªè®ºæ–‡ID
            chatbot.append(["æ£€æµ‹åˆ°è®ºæ–‡ID", f"æ£€æµ‹åˆ°{'arXiv' if paper_info[0] == 'arxiv' else 'DOI'} ID: {paper_info[1]}ï¼Œå‡†å¤‡ä¸‹è½½è®ºæ–‡..."])
            yield from update_ui(chatbot=chatbot, history=history)

            paper_file = download_paper_by_id(paper_info, chatbot, history)
            if paper_file:
                paper_files.append(paper_file)
            else:
                report_exception(chatbot, history, a=f"ä¸‹è½½è®ºæ–‡å¤±è´¥", b=f"æ— æ³•ä¸‹è½½{'arXiv' if paper_info[0] == 'arxiv' else 'DOI'}è®ºæ–‡: {paper_info[1]}")
                yield from update_ui(chatbot=chatbot, history=history)
                return
        else:
            # æ£€æŸ¥è¾“å…¥è·¯å¾„
            if not os.path.exists(txt):
                report_exception(chatbot, history, a=f"æ‰¹é‡è§£æè®ºæ–‡: {txt}", b=f"æ‰¾ä¸åˆ°æ–‡ä»¶æˆ–æ— æƒè®¿é—®: {txt}")
                yield from update_ui(chatbot=chatbot, history=history)
                return

            # éªŒè¯è·¯å¾„å®‰å…¨æ€§
            user_name = chatbot.get_user()
            validate_path_safety(txt, user_name)

            # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡æ–‡ä»¶
            paper_files = _find_paper_files(txt)

            if not paper_files:
                report_exception(chatbot, history, a=f"æ‰¹é‡è§£æè®ºæ–‡", b=f"åœ¨è·¯å¾„ {txt} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„è®ºæ–‡æ–‡ä»¶")
                yield from update_ui(chatbot=chatbot, history=history)
                return

    yield from update_ui(chatbot=chatbot, history=history)

    # å¼€å§‹æ‰¹é‡åˆ†æ
    if not paper_files:
        chatbot.append(["é”™è¯¯", "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯åˆ†æçš„è®ºæ–‡æ–‡ä»¶"])
        yield from update_ui(chatbot=chatbot, history=history)
        return

    chatbot.append(["å¼€å§‹æ™ºèƒ½æ‰¹é‡åˆ†æ", f"æ‰¾åˆ° {len(paper_files)} ç¯‡è®ºæ–‡ï¼Œå¼€å§‹æ™ºèƒ½ä¸»é¢˜åˆ†ç±»å’Œæ‰¹é‡åˆ†æ..."])
    yield from update_ui(chatbot=chatbot, history=history)

    # åˆ›å»ºç»Ÿä¸€åˆ†æå™¨
    analyzer = UnifiedBatchPaperAnalyzer(llm_kwargs, plugin_kwargs, chatbot, history, system_prompt)
    
    # æ‰¹é‡åˆ†ææ¯ç¯‡è®ºæ–‡
    successful_reports = []
    failed_papers = []
    domain_stats = {"general": 0, "rf_ic": 0}
    
    for i, paper_file in enumerate(paper_files):
        try:
            chatbot.append([f"åˆ†æè®ºæ–‡ {i+1}/{len(paper_files)}", f"æ­£åœ¨æ™ºèƒ½åˆ†æ: {os.path.basename(paper_file)}"])
            yield from update_ui(chatbot=chatbot, history=history)
            
            # åˆ†æå•ç¯‡è®ºæ–‡
            saved_file = yield from analyzer.analyze_paper(paper_file)
            
            if saved_file:
                successful_reports.append((os.path.basename(paper_file), saved_file, analyzer.paper_domain))
                domain_stats[analyzer.paper_domain] += 1
                chatbot.append([f"å®Œæˆè®ºæ–‡ {i+1}/{len(paper_files)}", f"æˆåŠŸåˆ†æå¹¶ä¿å­˜æŠ¥å‘Š: {os.path.basename(saved_file)} (é¢†åŸŸ: {analyzer.paper_domain})"])
            else:
                failed_papers.append(os.path.basename(paper_file))
                chatbot.append([f"å¤±è´¥è®ºæ–‡ {i+1}/{len(paper_files)}", f"åˆ†æå¤±è´¥: {os.path.basename(paper_file)}"])
            
            yield from update_ui(chatbot=chatbot, history=history)
            
        except Exception as e:
            failed_papers.append(os.path.basename(paper_file))
            chatbot.append([f"é”™è¯¯è®ºæ–‡ {i+1}/{len(paper_files)}", f"åˆ†æå‡ºé”™: {os.path.basename(paper_file)} - {str(e)}"])
            yield from update_ui(chatbot=chatbot, history=history)

    # ç”Ÿæˆæ‰¹é‡åˆ†ææ€»ç»“
    summary = f"æ™ºèƒ½æ‰¹é‡åˆ†æå®Œæˆï¼\n\n"
    summary += f"ğŸ“Š åˆ†æç»Ÿè®¡ï¼š\n"
    summary += f"- æ€»è®ºæ–‡æ•°ï¼š{len(paper_files)}\n"
    summary += f"- æˆåŠŸåˆ†æï¼š{len(successful_reports)}\n"
    summary += f"- åˆ†æå¤±è´¥ï¼š{len(failed_papers)}\n\n"
    
    summary += f"ğŸ¯ ä¸»é¢˜åˆ†ç±»ç»Ÿè®¡ï¼š\n"
    summary += f"- é€šç”¨è®ºæ–‡ï¼š{domain_stats['general']} ç¯‡\n"
    summary += f"- RF ICè®ºæ–‡ï¼š{domain_stats['rf_ic']} ç¯‡\n\n"
    
    if successful_reports:
        summary += f"âœ… æˆåŠŸç”ŸæˆæŠ¥å‘Šï¼š\n"
        for paper_name, report_path, domain in successful_reports:
            domain_label = "RF IC" if domain == "rf_ic" else "é€šç”¨"
            summary += f"- {paper_name} ({domain_label}) â†’ {os.path.basename(report_path)}\n"
    
    if failed_papers:
        summary += f"\nâŒ åˆ†æå¤±è´¥çš„è®ºæ–‡ï¼š\n"
        for paper_name in failed_papers:
            summary += f"- {paper_name}\n"

    chatbot.append(["æ™ºèƒ½æ‰¹é‡åˆ†æå®Œæˆ", summary])
    yield from update_ui(chatbot=chatbot, history=history)
