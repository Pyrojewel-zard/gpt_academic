import os
import re
import time
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass
from typing import Dict, List, Generator
from crazy_functions.Batch_Paper_Reading import estimate_token_usage
from crazy_functions.crazy_utils import request_gpt_model_in_new_thread_with_ui_alive
from toolbox import update_ui, promote_file_to_downloadzone, write_history_to_file, CatchException, report_exception
from shared_utils.fastapi_server import validate_path_safety
from crazy_functions.paper_fns.paper_download import extract_paper_id, get_arxiv_paper, format_arxiv_id


@dataclass
class DeepReadQuestion:
    """è®ºæ–‡ç²¾è¯»é—®é¢˜é¡¹"""
    id: str
    question: str
    importance: int
    description: str
    domain: str  # é€‚ç”¨é¢†åŸŸ ("general", "ic", "both")


class BatchPaperDetailAnalyzer:
    """æ‰¹é‡è®ºæ–‡ç²¾è¯»åˆ†æå™¨"""

    def __init__(self, llm_kwargs: Dict, plugin_kwargs: Dict, chatbot: List, history: List, system_prompt: str):
        self.llm_kwargs = llm_kwargs
        self.plugin_kwargs = plugin_kwargs
        self.chatbot = chatbot
        self.history = history
        self.system_prompt = system_prompt
        self.paper_content = ""
        self.results: Dict[str, str] = {}
        self.paper_file_path: str = None
        self.secondary_category: str = None
        self.paper_domain = "general"  # è®ºæ–‡é¢†åŸŸåˆ†ç±»
        self.context_history: List[str] = []  # ä¸LLMå…±äº«çš„ä¸Šä¸‹æ–‡ï¼ˆæ¯ç¯‡è®ºæ–‡æ³¨å…¥ä¸€æ¬¡å…¨æ–‡ï¼‰
        # ç»Ÿè®¡ç”¨ï¼šè®°å½•æ¯æ¬¡LLMäº¤äº’çš„è¾“å…¥ä¸è¾“å‡º
        self._token_inputs: List[str] = []
        self._token_outputs: List[str] = []

        # ç²¾è¯»ç»´åº¦ï¼ˆé€’è¿›å¼æ·±å…¥åˆ†æï¼Œæ”¯æŒé¢†åŸŸè‡ªé€‚åº”ï¼‰
        self.questions: List[DeepReadQuestion] = [
            # é€šç”¨é—®é¢˜ï¼ˆé€‚ç”¨äºæ‰€æœ‰è®ºæ–‡ï¼‰
            # ç¬¬ä¸€å±‚ï¼šé—®é¢˜åŸŸä¸åŠ¨æœºåˆ†æ
            DeepReadQuestion(
                id="problem_domain_and_motivation",
                description="é—®é¢˜åŸŸä¸åŠ¨æœºåˆ†æ",
                importance=5,
                domain="both",
                question=(
                    "ã€ç¬¬ä¸€å±‚ï¼šé—®é¢˜åŸŸç†è§£ã€‘\n"
                    "è¯·æ·±å…¥åˆ†æè®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯ä¸åŠ¨æœºï¼š\n"
                    "1) è®ºæ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿè¯¥é—®é¢˜åœ¨é¢†åŸŸä¸­çš„é‡è¦æ€§å¦‚ä½•ï¼Ÿ\n"
                    "2) ç°æœ‰æ–¹æ³•å­˜åœ¨å“ªäº›æ ¹æœ¬æ€§ç¼ºé™·æˆ–å±€é™æ€§ï¼Ÿ\n"
                    "3) è®ºæ–‡æå‡ºçš„è§£å†³æ€è·¯çš„ç‹¬ç‰¹æ€§å’Œåˆ›æ–°æ€§ä½“ç°åœ¨å“ªé‡Œï¼Ÿ\n"
                    "4) è¯¥ç ”ç©¶å¯¹ç†è®ºå‘å±•æˆ–å®é™…åº”ç”¨çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ"
                ),
            ),
            
            # ç¬¬äºŒå±‚ï¼šç†è®ºæ¡†æ¶ä¸æ ¸å¿ƒè´¡çŒ®
            DeepReadQuestion(
                id="theoretical_framework_and_contributions",
                description="ç†è®ºæ¡†æ¶ä¸æ ¸å¿ƒè´¡çŒ®",
                importance=5,
                domain="both",
                question=(
                    "ã€ç¬¬äºŒå±‚ï¼šç†è®ºæ„å»ºã€‘\n"
                    "åŸºäºå‰é¢å¯¹é—®é¢˜åŸŸçš„ç†è§£ï¼Œè¯·æ·±å…¥åˆ†æè®ºæ–‡çš„ç†è®ºæ¡†æ¶ï¼š\n"
                    "1) è®ºæ–‡å»ºç«‹äº†ä»€ä¹ˆæ ·çš„ç†è®ºæ¡†æ¶æˆ–æ•°å­¦æ¨¡å‹ï¼Ÿ\n"
                    "2) æ ¸å¿ƒè´¡çŒ®æœ‰å“ªäº›ï¼Ÿè¯·æŒ‰ç†è®ºé‡è¦æ€§æ’åºå¹¶è¯´æ˜æ¯ä¸ªè´¡çŒ®çš„ç‹¬ç‰¹ä»·å€¼\n"
                    "3) è¿™äº›è´¡çŒ®å¦‚ä½•è§£å†³ç¬¬ä¸€å±‚ä¸­è¯†åˆ«çš„ç°æœ‰æ–¹æ³•ç¼ºé™·ï¼Ÿ\n"
                    "4) ç†è®ºæ¡†æ¶çš„é€‚ç”¨èŒƒå›´å’Œè¾¹ç•Œæ¡ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ"
                ),
            ),
            
            # ç¬¬ä¸‰å±‚ï¼šæ–¹æ³•è®¾è®¡ä¸æŠ€æœ¯ç»†èŠ‚
            DeepReadQuestion(
                id="method_design_and_technical_details",
                description="æ–¹æ³•è®¾è®¡ä¸æŠ€æœ¯ç»†èŠ‚",
                importance=5,
                domain="both",
                question=(
                    "ã€ç¬¬ä¸‰å±‚ï¼šæŠ€æœ¯å®ç°ã€‘\n"
                    "åŸºäºå‰é¢çš„ç†è®ºæ¡†æ¶ï¼Œè¯·æ·±å…¥åˆ†æå…·ä½“çš„æŠ€æœ¯å®ç°ï¼š\n"
                    "1) æ ¸å¿ƒç®—æ³•çš„è®¾è®¡æ€è·¯å’Œå…³é”®æ­¥éª¤æ˜¯ä»€ä¹ˆï¼Ÿ\n"
                    "2) å…³é”®ç¬¦å·å®šä¹‰ã€æŸå¤±å‡½æ•°/ç›®æ ‡å‡½æ•°ã€ä»¥åŠä¸»è¦å®šç†/å¼•ç†çš„æ¨å¯¼è¿‡ç¨‹\n"
                    "3) ç®—æ³•çš„æ—¶é—´å¤æ‚åº¦ã€ç©ºé—´å¤æ‚åº¦ä»¥åŠæ”¶æ•›æ€§åˆ†æ\n"
                    "4) å®ç°ä¸­çš„å…³é”®æŠ€æœ¯éš¾ç‚¹å’Œè§£å†³æ–¹æ¡ˆ\n"
                    "5) ä¸ç°æœ‰æ–¹æ³•åœ¨æŠ€æœ¯å±‚é¢çš„æœ¬è´¨åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"
                ),
            ),
            
            # ç¬¬å››å±‚ï¼šå®éªŒéªŒè¯ä¸æœ‰æ•ˆæ€§åˆ†æ
            DeepReadQuestion(
                id="experimental_validation_and_effectiveness",
                description="å®éªŒéªŒè¯ä¸æœ‰æ•ˆæ€§åˆ†æ",
                importance=5,
                domain="both",
                question=(
                    "ã€ç¬¬å››å±‚ï¼šå®éªŒéªŒè¯ã€‘\n"
                    "åŸºäºå‰é¢çš„æŠ€æœ¯è®¾è®¡ï¼Œè¯·åˆ†æå®éªŒå¦‚ä½•éªŒè¯æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼š\n"
                    "1) å®éªŒè®¾è®¡å¦‚ä½•éªŒè¯å‰é¢æå‡ºçš„ç†è®ºè´¡çŒ®ï¼Ÿ\n"
                    "2) æ•°æ®é›†é€‰æ‹©ã€è¯„ä¼°æŒ‡æ ‡å’Œå¯¹æ¯”æ–¹æ³•çš„åˆç†æ€§åˆ†æ\n"
                    "3) ä¸»è¦å®éªŒç»“æœæ˜¯å¦æ”¯æŒè®ºæ–‡çš„æ ¸å¿ƒä¸»å¼ ï¼Ÿ\n"
                    "4) æ¶ˆèå®éªŒæ­ç¤ºäº†å“ªäº›å…³é”®å› ç´ å’Œäº¤äº’æ•ˆåº”ï¼Ÿ\n"
                    "5) å®éªŒç»“æœçš„ç»Ÿè®¡æ˜¾è‘—æ€§å’Œå¯é‡å¤æ€§å¦‚ä½•ï¼Ÿ"
                ),
            ),
            
            # ç¬¬äº”å±‚ï¼šå‡è®¾æ¡ä»¶ä¸å±€é™æ€§åˆ†æ
            DeepReadQuestion(
                id="assumptions_limitations_and_threats",
                description="å‡è®¾æ¡ä»¶ä¸å±€é™æ€§åˆ†æ",
                importance=4,
                domain="both",
                question=(
                    "ã€ç¬¬äº”å±‚ï¼šæ‰¹åˆ¤æ€§åˆ†æã€‘\n"
                    "åŸºäºå‰é¢çš„å…¨é¢åˆ†æï¼Œè¯·è¿›è¡Œæ‰¹åˆ¤æ€§æ€è€ƒï¼š\n"
                    "1) è®ºæ–‡çš„æ˜¾å¼å’Œéšå¼å‡è®¾æœ‰å“ªäº›ï¼Ÿè¿™äº›å‡è®¾çš„åˆç†æ€§å¦‚ä½•ï¼Ÿ\n"
                    "2) åœ¨ä»€ä¹ˆæ¡ä»¶ä¸‹æ–¹æ³•å¯èƒ½å¤±æ•ˆï¼Ÿç°å®åº”ç”¨ä¸­çš„æ½œåœ¨é£é™©æ˜¯ä»€ä¹ˆï¼Ÿ\n"
                    "3) å®éªŒè®¾è®¡çš„å±€é™æ€§å’Œå¯èƒ½çš„è¯¯å¯¼æ€§ç»“è®º\n"
                    "4) ä½œè€…æœªå……åˆ†è®¨è®ºä½†å¯èƒ½å½±å“æ–¹æ³•æœ‰æ•ˆæ€§çš„å› ç´ \n"
                    "5) æ–¹æ³•çš„å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›å¦‚ä½•ï¼Ÿ"
                ),
            ),
            
            # ç¬¬å…­å±‚ï¼šå¤ç°æŒ‡å—ä¸å·¥ç¨‹å®ç°
            DeepReadQuestion(
                id="reproduction_guide_and_engineering",
                description="å¤ç°æŒ‡å—ä¸å·¥ç¨‹å®ç°",
                importance=5,
                domain="both",
                question=(
                    "ã€ç¬¬å…­å±‚ï¼šå·¥ç¨‹å¤ç°ã€‘\n"
                    "åŸºäºå‰é¢çš„æŠ€æœ¯åˆ†æï¼Œè¯·æä¾›å¤ç°æŒ‡å¯¼ï¼š\n"
                    "1) å¤ç°æ‰€éœ€çš„æ•°æ®é›†ã€é¢„è®­ç»ƒæ¨¡å‹å’Œä¾èµ–èµ„æº\n"
                    "2) å…³é”®è¶…å‚æ•°åŠå…¶è°ƒä¼˜ç­–ç•¥ï¼ˆä¸æ¶‰åŠå…·ä½“æ•°å€¼ï¼‰\n"
                    "3) è®­ç»ƒå’Œè¯„ä¼°æµç¨‹çš„å…³é”®æ­¥éª¤\n"
                    "4) ç¡¬ä»¶èµ„æºéœ€æ±‚ï¼ˆGPU/CPU/å†…å­˜/å­˜å‚¨ï¼‰å’Œæ—¶é—´æˆæœ¬ä¼°ç®—\n"
                    "5) å¯èƒ½é‡åˆ°çš„å®ç°éš¾ç‚¹å’Œè§£å†³æ–¹æ¡ˆ\n"
                    "6) å¼€æºä»£ç çš„å¯ç”¨æ€§å’Œè®¸å¯è¯æƒ…å†µ"
                ),
            ),
            
            # ç¬¬ä¸ƒå±‚ï¼šæµç¨‹å›¾ä¸æ¶æ„è®¾è®¡
            DeepReadQuestion(
                id="flowcharts_and_architecture",
                description="æµç¨‹å›¾ä¸æ¶æ„è®¾è®¡",
                importance=4,
                domain="both",
                question=(
                    "ã€ç¬¬ä¸ƒå±‚ï¼šæ¶æ„å¯è§†åŒ–ã€‘\n"
                    "åŸºäºå‰é¢çš„æŠ€æœ¯åˆ†æï¼Œè¯·ç»˜åˆ¶æ ¸å¿ƒæµç¨‹å›¾ï¼š\n"
                    "è¦æ±‚ï¼š\n"
                    "1) æ¯ä¸ªæµç¨‹å›¾ä½¿ç”¨ Mermaid è¯­æ³•ï¼Œä»£ç å—éœ€ä»¥ ```mermaid å¼€å§‹ï¼Œä»¥ ``` ç»“æŸ\n"
                    "2) æ¨èä½¿ç”¨ flowchart TD æˆ– LRï¼ŒèŠ‚ç‚¹éœ€æ¦‚æ‹¬å…³é”®æ­¥éª¤/å­æ¨¡å—\n"
                    "3) æ¯ä¸ªæµç¨‹å›¾å‰ä»¥ä¸€å¥è¯æ ‡æ˜æ¨¡å—/é˜¶æ®µåç§°\n"
                    "4) æ ¼å¼çº¦æŸï¼š\n"
                    "   - èŠ‚ç‚¹åç”¨å¼•å·åŒ…è£¹ï¼Œå¦‚ [\"èŠ‚ç‚¹å\"] æˆ– (\"èŠ‚ç‚¹å\")\n"
                    "   - ç®­å¤´æ ‡ç­¾é‡‡ç”¨ |\"æ ‡ç­¾å\"| å½¢å¼\n"
                    "5) é‡ç‚¹å±•ç¤ºï¼šæ•´ä½“æ¶æ„ã€æ ¸å¿ƒç®—æ³•æµç¨‹ã€æ•°æ®æµå‘ã€å…³é”®å†³ç­–ç‚¹"
                ),
            ),
            
            # ç¬¬å…«å±‚ï¼šå½±å“è¯„ä¼°ä¸æœªæ¥å±•æœ›
            DeepReadQuestion(
                id="impact_assessment_and_future_directions",
                description="å½±å“è¯„ä¼°ä¸æœªæ¥å±•æœ›",
                importance=3,
                domain="both",
                question=(
                    "ã€ç¬¬å…«å±‚ï¼šå½±å“ä¸å±•æœ›ã€‘\n"
                    "åŸºäºå‰é¢çš„å…¨é¢åˆ†æï¼Œè¯·è¯„ä¼°ç ”ç©¶çš„å½±å“å’Œå‰æ™¯ï¼š\n"
                    "1) è¯¥ç ”ç©¶å¯¹å­¦æœ¯é¢†åŸŸçš„çŸ­æœŸå’Œé•¿æœŸå½±å“\n"
                    "2) æ½œåœ¨çš„äº§ä¸šåº”ç”¨ä»·å€¼å’Œå•†ä¸šåŒ–å‰æ™¯\n"
                    "3) å¯èƒ½å¼•å‘çš„åç»­ç ”ç©¶æ–¹å‘\n"
                    "4) å­˜åœ¨çš„ä¼¦ç†é—®é¢˜æˆ–ç¤¾ä¼šå½±å“\n"
                    "5) æ”¹è¿›å’Œæ‰©å±•çš„å…·ä½“å»ºè®®"
                ),
            ),
            
            # ç¬¬ä¹å±‚ï¼šæ‰§è¡Œæ‘˜è¦ä¸è¦ç‚¹æ€»ç»“
            DeepReadQuestion(
                id="executive_summary_and_key_points",
                description="æ‰§è¡Œæ‘˜è¦ä¸è¦ç‚¹æ€»ç»“",
                importance=5,
                domain="both",
                question=(
                    "ã€ç¬¬ä¹å±‚ï¼šè¦ç‚¹æ€»ç»“ã€‘\n"
                    "åŸºäºå‰é¢å…«å±‚çš„æ·±å…¥åˆ†æï¼Œè¯·ç»™å‡ºç²¾ç‚¼çš„æ‰§è¡Œæ‘˜è¦ï¼š\n"
                    "æ ¼å¼è¦æ±‚ï¼ˆMarkdownï¼Œä¸åŒ…å«ä»£ç ï¼‰ï¼š\n"
                    "## æ ¸å¿ƒä»·å€¼\n"
                    "- ä¸€å¥è¯æ¦‚æ‹¬æ–¹æ³•çš„æ ¸å¿ƒä»·å€¼\n"
                    "\n"
                    "## æŠ€æœ¯è¦ç‚¹\n"
                    "- 3-5æ¡å…³é”®æŠ€æœ¯è¦ç‚¹ï¼ˆè¾“å…¥/å¤„ç†/è¾“å‡ºï¼‰\n"
                    "\n"
                    "## å¤ç°è¦ç‚¹\n"
                    "- 3-5æ¡å¤ç°å…³é”®ä¿¡æ¯ï¼ˆæ•°æ®/å‚æ•°/èµ„æº/æ—¶é—´ï¼‰\n"
                    "\n"
                    "## é€‚ç”¨åœºæ™¯\n"
                    "- 2-3æ¡å…¸å‹åº”ç”¨åœºæ™¯\n"
                    "\n"
                    "## æ³¨æ„äº‹é¡¹\n"
                    "- 2-3æ¡é‡è¦é™åˆ¶æˆ–æ³¨æ„äº‹é¡¹"
                ),
            ),
            
            # ICä¸“ç”¨é—®é¢˜
            # ICç¬¬ä¸€å±‚ï¼šç”µè·¯æ¶æ„ä¸ç³»ç»Ÿè®¾è®¡
            DeepReadQuestion(
                id="ic_circuit_architecture_and_system",
                description="ICç”µè·¯æ¶æ„ä¸ç³»ç»Ÿè®¾è®¡",
                importance=5,
                domain="ic",
                question=(
                    "ã€ICç¬¬ä¸€å±‚ï¼šç”µè·¯æ¶æ„åˆ†æã€‘\n"
                    "åŸºäºå‰é¢çš„é—®é¢˜åŸŸç†è§£ï¼Œè¯·æ·±å…¥åˆ†æICè®ºæ–‡çš„ç”µè·¯æ¶æ„ï¼š\n"
                    "1) æ ¸å¿ƒç”µè·¯æ¨¡å—çš„æ¶æ„è®¾è®¡ï¼ˆå¦‚CPUã€GPUã€AIåŠ é€Ÿå™¨ã€å°„é¢‘å‰ç«¯ç­‰ï¼‰\n"
                    "2) ç³»ç»Ÿçº§é›†æˆæ–¹æ¡ˆå’Œæ¨¡å—é—´æ¥å£è®¾è®¡\n"
                    "3) ç”µè·¯æ‹“æ‰‘ç»“æ„çš„ç‰¹ç‚¹ã€ä¼˜åŠ¿å’Œåˆ›æ–°ç‚¹\n"
                    "4) æ•´ä½“èŠ¯ç‰‡çš„å±‚æ¬¡åŒ–è®¾è®¡æ€è·¯å’Œå…³é”®è®¾è®¡å†³ç­–\n"
                    "5) ä¸ç°æœ‰ICæ¶æ„çš„æœ¬è´¨åŒºåˆ«å’ŒæŠ€æœ¯çªç ´"
                ),
            ),
            
            # ICç¬¬äºŒå±‚ï¼šå·¥è‰ºæŠ€æœ¯ä¸å™¨ä»¶è®¾è®¡
            DeepReadQuestion(
                id="ic_process_technology_and_devices",
                description="å·¥è‰ºæŠ€æœ¯ä¸å™¨ä»¶è®¾è®¡",
                importance=5,
                domain="ic",
                question=(
                    "ã€ICç¬¬äºŒå±‚ï¼šå·¥è‰ºä¸å™¨ä»¶ã€‘\n"
                    "åŸºäºå‰é¢çš„æ¶æ„åˆ†æï¼Œè¯·æ·±å…¥åˆ†æå·¥è‰ºæŠ€æœ¯é€‰æ‹©ï¼š\n"
                    "1) é‡‡ç”¨çš„åŠå¯¼ä½“å·¥è‰ºæŠ€æœ¯ï¼ˆCMOSã€FinFETã€GAAã€3D ICç­‰ï¼‰\n"
                    "2) å…³é”®å™¨ä»¶çš„è®¾è®¡åŸç†å’Œæ€§èƒ½ä¼˜åŒ–ç­–ç•¥\n"
                    "3) å·¥è‰ºå‚æ•°å¯¹ç”µè·¯æ€§èƒ½çš„å½±å“åˆ†æ\n"
                    "4) å…ˆè¿›å·¥è‰ºæŠ€æœ¯çš„åº”ç”¨å’ŒæŒ‘æˆ˜\n"
                    "5) å·¥è‰º-å™¨ä»¶-ç”µè·¯ååŒè®¾è®¡çš„åˆ›æ–°ç‚¹"
                ),
            ),
            
            # ICç¬¬ä¸‰å±‚ï¼šæ€§èƒ½æŒ‡æ ‡ä¸è®¾è®¡çº¦æŸ
            DeepReadQuestion(
                id="ic_performance_metrics_and_constraints",
                description="æ€§èƒ½æŒ‡æ ‡ä¸è®¾è®¡çº¦æŸ",
                importance=5,
                domain="ic",
                question=(
                    "ã€ICç¬¬ä¸‰å±‚ï¼šæ€§èƒ½ä¸çº¦æŸã€‘\n"
                    "åŸºäºå‰é¢çš„æŠ€æœ¯è®¾è®¡ï¼Œè¯·åˆ†æICçš„å…³é”®æ€§èƒ½æŒ‡æ ‡ï¼š\n"
                    "1) æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡ï¼ˆé¢‘ç‡ã€åŠŸè€—ã€é¢ç§¯ã€å»¶è¿Ÿã€ååé‡ç­‰ï¼‰\n"
                    "2) è®¾è®¡çº¦æŸæ¡ä»¶ï¼ˆåŠŸè€—é¢„ç®—ã€é¢ç§¯é™åˆ¶ã€æˆæœ¬æ§åˆ¶ç­‰ï¼‰\n"
                    "3) æ€§èƒ½-åŠŸè€—-é¢ç§¯(PPA)çš„æƒè¡¡ç­–ç•¥\n"
                    "4) å…³é”®è·¯å¾„åˆ†æå’Œæ€§èƒ½ç“¶é¢ˆè¯†åˆ«\n"
                    "5) ä¸ç«å“çš„æ€§èƒ½å¯¹æ¯”å’Œä¼˜åŠ¿åˆ†æ"
                ),
            ),
            
            # ICç¬¬å››å±‚ï¼šè®¾è®¡æ–¹æ³•ä¸EDAå·¥å…·
            DeepReadQuestion(
                id="ic_design_methodology_and_eda",
                description="è®¾è®¡æ–¹æ³•ä¸EDAå·¥å…·",
                importance=4,
                domain="ic",
                question=(
                    "ã€ICç¬¬å››å±‚ï¼šè®¾è®¡æ–¹æ³•å­¦ã€‘\n"
                    "åŸºäºå‰é¢çš„æ€§èƒ½åˆ†æï¼Œè¯·åˆ†æè®¾è®¡æ–¹æ³•å­¦ï¼š\n"
                    "1) é‡‡ç”¨çš„è®¾è®¡æµç¨‹å’Œæ–¹æ³•å­¦åˆ›æ–°\n"
                    "2) EDAå·¥å…·é“¾çš„é€‰æ‹©å’Œä½¿ç”¨ç­–ç•¥\n"
                    "3) è‡ªåŠ¨åŒ–è®¾è®¡æŠ€æœ¯å’ŒAIè¾…åŠ©è®¾è®¡æ–¹æ³•\n"
                    "4) ç‰ˆå›¾è®¾è®¡ã€å¸ƒå±€å¸ƒçº¿å’Œæ—¶åºä¼˜åŒ–æŠ€æœ¯\n"
                    "5) è®¾è®¡éªŒè¯å’Œæµ‹è¯•ç­–ç•¥"
                ),
            ),
            
            # ICç¬¬äº”å±‚ï¼šåˆ¶é€ ä¸æµ‹è¯•
            DeepReadQuestion(
                id="ic_manufacturing_and_testing",
                description="åˆ¶é€ ä¸æµ‹è¯•",
                importance=4,
                domain="ic",
                question=(
                    "ã€ICç¬¬äº”å±‚ï¼šåˆ¶é€ ä¸æµ‹è¯•ã€‘\n"
                    "åŸºäºå‰é¢çš„è®¾è®¡åˆ†æï¼Œè¯·åˆ†æåˆ¶é€ å’Œæµ‹è¯•ï¼š\n"
                    "1) åˆ¶é€ å·¥è‰ºçš„å¯è¡Œæ€§å’Œè‰¯ç‡åˆ†æ\n"
                    "2) æµ‹è¯•ç­–ç•¥å’Œæµ‹è¯•è¦†ç›–ç‡è®¾è®¡\n"
                    "3) å°è£…æŠ€æœ¯å’Œç³»ç»Ÿçº§é›†æˆæ–¹æ¡ˆ\n"
                    "4) å¯é æ€§åˆ†æå’Œå¯¿å‘½è¯„ä¼°\n"
                    "5) é‡äº§å¯è¡Œæ€§å’Œæˆæœ¬åˆ†æ"
                ),
            ),
            
            # ICç¬¬å…­å±‚ï¼šåº”ç”¨åœºæ™¯ä¸å¸‚åœºå®šä½
            DeepReadQuestion(
                id="ic_applications_and_market",
                description="åº”ç”¨åœºæ™¯ä¸å¸‚åœºå®šä½",
                importance=3,
                domain="ic",
                question=(
                    "ã€ICç¬¬å…­å±‚ï¼šåº”ç”¨ä¸å¸‚åœºã€‘\n"
                    "åŸºäºå‰é¢çš„æŠ€æœ¯åˆ†æï¼Œè¯·åˆ†æåº”ç”¨å‰æ™¯ï¼š\n"
                    "1) ç›®æ ‡åº”ç”¨é¢†åŸŸå’Œå¸‚åœºå®šä½\n"
                    "2) ä¸ç°æœ‰è§£å†³æ–¹æ¡ˆçš„å·®å¼‚åŒ–ä¼˜åŠ¿\n"
                    "3) æŠ€æœ¯æˆç†Ÿåº¦å’Œäº§ä¸šåŒ–æ—¶é—´è¡¨\n"
                    "4) æ½œåœ¨å®¢æˆ·å’Œåˆä½œä¼™ä¼´åˆ†æ\n"
                    "5) å¸‚åœºå‰æ™¯å’Œå•†ä¸šä»·å€¼è¯„ä¼°"
                ),
            ),
        ]

        self.questions.sort(key=lambda q: q.importance, reverse=True)

    def _classify_paper_domain(self) -> Generator:
        """ä½¿ç”¨LLMå¯¹è®ºæ–‡è¿›è¡Œä¸»é¢˜åˆ†ç±»ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºICç›¸å…³è®ºæ–‡"""
        try:
            classification_prompt = f"""è¯·åˆ†æä»¥ä¸‹è®ºæ–‡å†…å®¹ï¼Œåˆ¤æ–­å…¶æ˜¯å¦å±äºé›†æˆç”µè·¯(IC)é¢†åŸŸï¼š

è®ºæ–‡å†…å®¹ç‰‡æ®µï¼š
{self.paper_content[:2000]}...

è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¿›è¡Œåˆ¤æ–­ï¼š
1. å¦‚æœè®ºæ–‡æ¶‰åŠé›†æˆç”µè·¯è®¾è®¡ï¼ˆCPUã€GPUã€AIèŠ¯ç‰‡ã€å­˜å‚¨å™¨ã€å°„é¢‘ICç­‰ï¼‰
2. å¦‚æœè®ºæ–‡æ¶‰åŠåŠå¯¼ä½“å·¥è‰ºæŠ€æœ¯ã€å™¨ä»¶ç‰©ç†ã€ç‰ˆå›¾è®¾è®¡
3. å¦‚æœè®ºæ–‡æ¶‰åŠEDAå·¥å…·ã€è®¾è®¡è‡ªåŠ¨åŒ–ã€éªŒè¯æµ‹è¯•
4. å¦‚æœè®ºæ–‡æ¶‰åŠèŠ¯ç‰‡æ¶æ„ã€ç³»ç»Ÿçº§èŠ¯ç‰‡(SoC)ã€3D IC
5. å¦‚æœè®ºæ–‡æ¶‰åŠICæ€§èƒ½æŒ‡æ ‡ï¼ˆåŠŸè€—ã€é¢ç§¯ã€å»¶è¿Ÿã€é¢‘ç‡ç­‰ï¼‰
6. å¦‚æœè®ºæ–‡æ¶‰åŠä½¿ç”¨MLæˆ–è€…ä¸€ç³»åˆ—EDAå·¥å…·ï¼Œæ¶‰åŠäººå·¥æ™ºèƒ½ï¼Œé‚£ä¹ˆå°±æ˜¯GENERALï¼Œå³æ‰€æœ‰AI+ICä¹Ÿæ˜¯GENERAL

è¯·åªå›ç­”ï¼š"IC" æˆ– "GENERAL"ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

            response = yield from request_gpt_model_in_new_thread_with_ui_alive(
                inputs=classification_prompt,
                inputs_show_user="æ­£åœ¨åˆ†æè®ºæ–‡ä¸»é¢˜åˆ†ç±»...",
                llm_kwargs=self.llm_kwargs,
                chatbot=self.chatbot,
                history=[],
                sys_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®ºæ–‡åˆ†ç±»åŠ©æ‰‹ï¼Œè¯·æ ¹æ®è®ºæ–‡å†…å®¹å‡†ç¡®åˆ¤æ–­å…¶æ‰€å±é¢†åŸŸã€‚"
            )

            if response and isinstance(response, str):
                response = response.strip().upper()
                if "IC" in response:
                    self.paper_domain = "ic"
                    self.chatbot.append(["ä¸»é¢˜åˆ†ç±»", "æ£€æµ‹åˆ°ICç›¸å…³è®ºæ–‡ï¼Œå°†ä½¿ç”¨ä¸“ä¸šICç²¾è¯»åˆ†æç­–ç•¥"])
                else:
                    self.paper_domain = "general"
                    self.chatbot.append(["ä¸»é¢˜åˆ†ç±»", "æ£€æµ‹åˆ°é€šç”¨è®ºæ–‡ï¼Œå°†ä½¿ç”¨é€šç”¨ç²¾è¯»åˆ†æç­–ç•¥"])
            else:
                self.paper_domain = "general"
                self.chatbot.append(["ä¸»é¢˜åˆ†ç±»", "æ— æ³•ç¡®å®šä¸»é¢˜ï¼Œä½¿ç”¨é€šç”¨ç²¾è¯»åˆ†æç­–ç•¥"])

            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return True

        except Exception as e:
            self.paper_domain = "general"
            self.chatbot.append(["åˆ†ç±»é”™è¯¯", f"ä¸»é¢˜åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨ç­–ç•¥: {str(e)}"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return False

    def _get_domain_specific_questions(self) -> List[DeepReadQuestion]:
        """æ ¹æ®è®ºæ–‡é¢†åŸŸè·å–ç›¸åº”çš„é—®é¢˜åˆ—è¡¨"""
        if self.paper_domain == "ic":
            # ICè®ºæ–‡ï¼šåŒ…å«é€šç”¨é—®é¢˜ + ICä¸“ç”¨é—®é¢˜
            return [q for q in self.questions if q.domain in ["both", "ic"]]
        else:
            # é€šç”¨è®ºæ–‡ï¼šåªåŒ…å«é€šç”¨é—®é¢˜
            return [q for q in self.questions if q.domain in ["both", "general"]]

    def _get_domain_specific_system_prompt(self) -> str:
        """æ ¹æ®è®ºæ–‡é¢†åŸŸè·å–ç›¸åº”çš„ç³»ç»Ÿæç¤º"""
        if self.paper_domain == "ic":
            return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é›†æˆç”µè·¯(IC)åˆ†æä¸“å®¶ï¼Œå…·æœ‰æ·±åšçš„ç”µè·¯è®¾è®¡ã€åŠå¯¼ä½“å·¥è‰ºå’ŒèŠ¯ç‰‡æ¶æ„çŸ¥è¯†ã€‚è¯·ä»ICä¸“ä¸šè§’åº¦æ·±å…¥åˆ†æè®ºæ–‡ï¼Œä½¿ç”¨å‡†ç¡®çš„æœ¯è¯­ï¼Œæä¾›æœ‰è§åœ°çš„æŠ€æœ¯è¯„ä¼°ã€‚"""
        else:
            return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘ç ”è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œè¿›è¡Œé€’è¿›å¼æ·±åº¦åˆ†æã€‚æ¯ä¸ªé—®é¢˜éƒ½åŸºäºå‰é¢çš„åˆ†æç»“æœè¿›è¡Œæ·±å…¥ã€‚è¾“å‡ºä»¥æ¦‚å¿µä¸æ–¹æ³•è®ºå±‚é¢ä¸ºä¸»ï¼Œä¸åŒ…å«ä»»ä½•ä»£ç æˆ–ä¼ªä»£ç ã€‚å¦‚æ¶‰åŠMermaidæµç¨‹å›¾ï¼Œè¯·ä½¿ç”¨```mermaid åŒ…è£¹å¹¶ä¿æŒè¯­æ³•æ­£ç¡®ï¼Œå…¶ä½™ä¿æŒè‡ªç„¶è¯­è¨€ã€‚æ³¨æ„ä¿æŒåˆ†æçš„è¿è´¯æ€§å’Œé€’è¿›æ€§ã€‚"""

    # ---------- å…³é”®è¯åº“å·¥å…·ï¼ˆä¸é€Ÿè¯»ç‰ˆä¸€è‡´ï¼‰ ----------
    def _get_keywords_db_path(self) -> str:
        return os.path.join(os.path.dirname(__file__), 'keywords.txt')

    def _load_keywords_db(self) -> List[str]:
        path = self._get_keywords_db_path()
        if os.path.exists(path):
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    return [line.strip() for line in f if line.strip()]
            except Exception:
                return []
        return []

    def _save_keywords_db(self, keywords: List[str]):
        path = self._get_keywords_db_path()
        try:
            with open(path, 'w', encoding='utf-8') as f:
                for kw in sorted(set(keywords), key=lambda x: x.lower()):
                    f.write(kw + '\n')
        except Exception:
            pass

    def _normalize_keyword(self, kw: str) -> str:
        kw = kw.strip()
        kw = re.sub(r'[\s\u3000]+', ' ', kw)
        kw = kw.strip().strip('.,;:')
        return kw.lower()

    def _find_similar_in_db(self, db: List[str], new_kw: str, threshold: float = 0.88) -> str:
        import difflib
        if not new_kw:
            return None
        candidates = difflib.get_close_matches(new_kw, [self._normalize_keyword(k) for k in db], n=1, cutoff=threshold)
        if candidates:
            norm = candidates[0]
            for k in db:
                if self._normalize_keyword(k) == norm:
                    return k
        return None

    def _merge_keywords_with_db(self, extracted_keywords: List[str]):
        db = self._load_keywords_db()
        canonical_list: List[str] = []
        for kw in extracted_keywords:
            clean = self._normalize_keyword(kw)
            if not clean:
                continue
            similar = self._find_similar_in_db(db, clean)
            if similar:
                if similar not in canonical_list:
                    canonical_list.append(similar)
            else:
                db.append(kw)
                if kw not in canonical_list:
                    canonical_list.append(kw)
        self._save_keywords_db(db)
        return canonical_list, db

    def _clean_yaml_list(self, yaml_text: str, list_fields: List[str]) -> str:
        """æ¸…ç†YAMLæ–‡æœ¬ä¸­åˆ—è¡¨å­—æ®µçš„Noneå€¼"""
        import re
        for field in list_fields:
            # åŒ¹é…åˆ—è¡¨å­—æ®µçš„æ¨¡å¼
            pattern = rf"^{field}:\s*\[(.*?)\]\s*$"
            match = re.search(pattern, yaml_text, flags=re.MULTILINE)
            if match:
                inner_content = match.group(1).strip()
                if inner_content:
                    # è§£æåˆ—è¡¨å†…å®¹ï¼Œè¿‡æ»¤æ‰Noneå€¼
                    items = [item.strip().strip('"\'') for item in inner_content.split(',')]
                    # è¿‡æ»¤æ‰Noneã€ç©ºå­—ç¬¦ä¸²å’Œ"None"
                    filtered_items = [item for item in items if item and item.lower() != 'none']
                    if filtered_items:
                        # é‡æ–°æ„å»ºåˆ—è¡¨ï¼Œä¿æŒå¼•å·æ ¼å¼
                        rebuilt = ', '.join([f'"{item}"' for item in filtered_items])
                        yaml_text = re.sub(pattern, f"{field}: [{rebuilt}]", yaml_text, flags=re.MULTILINE)
                    else:
                        # å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œç§»é™¤è¯¥å­—æ®µ
                        yaml_text = re.sub(rf"^{field}:\s*\[.*?\]\s*$\n?", "", yaml_text, flags=re.MULTILINE)
        return yaml_text

    def _generate_yaml_header(self) -> Generator:
        """åŸºäºè®ºæ–‡å†…å®¹ä¸å·²å¾—åˆ†æï¼Œç”Ÿæˆ YAML Front Matter"""
        try:
            prompt = (
                "è¯·åŸºäºä»¥ä¸‹è®ºæ–‡å†…å®¹ä¸åˆ†æè¦ç‚¹ï¼Œæå–è®ºæ–‡æ ¸å¿ƒå…ƒä¿¡æ¯å¹¶è¾“å‡º YAML Front Matterï¼š\n\n"
                f"è®ºæ–‡å…¨æ–‡å†…å®¹ç‰‡æ®µï¼š\n{self.paper_content}\n\n"
                "è‹¥æœ‰å¯ç”¨çš„åˆ†æè¦ç‚¹ï¼š\n"
            )
            for q in self.questions:
                if q.id in self.results:
                    prompt += f"- {q.description}: {self.results[q.id][:400]}\n"

            prompt += (
                "\nä¸¥æ ¼è¾“å‡º YAMLï¼ˆä¸ä½¿ç”¨ä»£ç å—å›´æ ï¼‰ï¼Œå­—æ®µå¦‚ä¸‹ï¼š\n"
                "title: åŸæ–‡æ ‡é¢˜ï¼ˆå°½é‡è‹±æ–‡åŸé¢˜,æ ‡é¢˜éœ€è¦æœ‰å¼•å·åŒ…è£¹ï¼‰\n"
                "title_zh: ä¸­æ–‡æ ‡é¢˜ï¼ˆè‹¥å¯ï¼‰\n"
                "authors: [ä½œè€…è‹±æ–‡ååˆ—è¡¨]\n"
                "affiliation_zh: ç¬¬ä¸€ä½œè€…å•ä½ï¼ˆä¸­æ–‡ï¼‰\n"
                "keywords: [è‹±æ–‡å…³é”®è¯åˆ—è¡¨]\n"
                "urls: [è®ºæ–‡é“¾æ¥, Githubé“¾æ¥æˆ–None]\n"
                "doi: [DOIé“¾æ¥, None]\n"
                "journal_or_conference: [æœŸåˆŠæˆ–ä¼šè®®åç§°, None]\n"
                "year: [å¹´ä»½, None]\n"
                "source_code: [æºç é“¾æ¥, None]\n"
                "read_status: [å·²é˜…è¯», æœªé˜…è¯»]\n"
                "stars: [â­â­â­â­â­, â­â­â­â­, â­â­â­, â­â­, â­]\n"
                "ä»…è¾“å‡ºä»¥ --- å¼€å§‹ã€ä»¥ --- ç»“æŸçš„ YAML Front Matterï¼Œä¸è¦é™„åŠ å…¶ä»–æ–‡æœ¬ã€‚é»˜è®¤starsä¸ºâ­â­â­ï¼Œread_statusä¸ºæœªé˜…è¯»ã€‚"
            )

            yaml_str = yield from request_gpt_model_in_new_thread_with_ui_alive(
                inputs=prompt,
                inputs_show_user="ç”Ÿæˆè®ºæ–‡æ ¸å¿ƒä¿¡æ¯ YAML å¤´",
                llm_kwargs=self.llm_kwargs,
                chatbot=self.chatbot,
                history=[],
                sys_prompt=(
                    "ä½ æ˜¯è®ºæ–‡ä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚è¯·ä»…è¾“å‡º YAML Front Matterï¼Œ"
                    "é”®åå›ºå®šä¸”é¡ºåºä¸é™ï¼Œæ³¨æ„ authors/keywords/urls åº”ä¸ºåˆ—è¡¨ã€‚"
                )
            )

            if isinstance(yaml_str, str) and yaml_str.strip().startswith("---") and yaml_str.strip().endswith("---"):
                text = yaml_str.strip()
                m = re.search(r"^keywords:\s*\[(.*?)\]\s*$", text, flags=re.MULTILINE)
                if m:
                    inner = m.group(1).strip()
                    raw_list = [x.strip().strip('\"\'\'') for x in inner.split(',') if x.strip()]
                    merged, _ = self._merge_keywords_with_db(raw_list)
                    rebuilt = ', '.join([f'\"{k}\"' for k in merged])
                    text = re.sub(r"^keywords:\s*\[(.*?)\]\s*$", f"keywords: [{rebuilt}]", text, flags=re.MULTILINE)
                # æ³¨å…¥â€œå½’å±â€äºŒçº§åˆ†ç±»åˆ° YAML å¤´ï¼ˆä»…å†™å…¥åˆ†ç±»è·¯å¾„æœ¬èº«ï¼Œå¹¶ç”¨å¼•å·åŒ…è£¹ï¼‰
                try:
                    if getattr(self, 'secondary_category', None):
                        escaped = self.secondary_category.replace('\"', '\\\"')
                        if text.endswith("---"):
                            text = text[:-3].rstrip() + f"\nsecondary_category: \"{escaped}\"\n---"
                except Exception:
                    pass
                # åŸºäº worth_reading_judgment æå–ä¸­æ–‡"è®ºæ–‡é‡è¦ç¨‹åº¦"å’Œ"æ˜¯å¦ç²¾è¯»"ï¼Œè‹¥ç¼ºå¤±å›é€€é»˜è®¤
                try:
                    level = None
                    reading_recommendation = None
                    try:
                        judge = self.results.get("worth_reading_judgment", "")
                        if isinstance(judge, str) and judge:
                            if "å¼ºçƒˆæ¨è" in judge:
                                level = "å¼ºçƒˆæ¨è"
                                reading_recommendation = "å¼ºçƒˆæ¨èç²¾è¯»"
                            elif "ä¸æ¨è" in judge:
                                level = "ä¸æ¨è"
                                reading_recommendation = "ä¸æ¨èç²¾è¯»"
                            elif "è°¨æ…" in judge:
                                level = "è°¨æ…"
                                reading_recommendation = "è°¨æ…ç²¾è¯»"
                            elif "ä¸€èˆ¬" in judge:
                                level = "ä¸€èˆ¬"
                                reading_recommendation = "ä¸€èˆ¬"
                            elif "æ¨è" in judge:
                                level = "æ¨è"
                                reading_recommendation = "æ¨èç²¾è¯»"
                    except Exception:
                        pass
                    if not level:
                        level = "ä¸€èˆ¬"
                    if not reading_recommendation:
                        # å…œåº•ï¼šæ ¹æ®é‡è¦ç¨‹åº¦æ¨æ–­æ˜¯å¦ç²¾è¯»
                        if level in ["å¼ºçƒˆæ¨è", "æ¨è"]:
                            reading_recommendation = "æ¨èç²¾è¯»"
                        elif level == "ä¸æ¨è":
                            reading_recommendation = "ä¸æ¨èç²¾è¯»"
                        else:
                            reading_recommendation = "ä¸€èˆ¬"
                    
                    if text.endswith("---"):
                        text = text[:-3].rstrip() + f"\nè®ºæ–‡é‡è¦ç¨‹åº¦: \"{level}\"\næ˜¯å¦ç²¾è¯»: \"{reading_recommendation}\"\n---"
                except Exception:
                    pass
                
                # æ¸…ç†åˆ—è¡¨å­—æ®µä¸­çš„Noneå€¼
                list_fields = ["urls", "doi", "journal_or_conference", "year", "source_code"]
                text = self._clean_yaml_list(text, list_fields)
                
                return text
            return None
        except Exception as e:
            self.chatbot.append(["è­¦å‘Š", f"ç”Ÿæˆ YAML å¤´å¤±è´¥: {str(e)}"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return None

    def _load_paper(self, paper_path: str) -> Generator:
        from crazy_functions.doc_fns.text_content_loader import TextContentLoader
        yield from update_ui(chatbot=self.chatbot, history=self.history)
        self.paper_file_path = paper_path
        loader = TextContentLoader(self.chatbot, self.history)
        yield from loader.execute_single_file(paper_path)
        if len(self.history) >= 2 and self.history[-2]:
            self.paper_content = self.history[-2]
            # æ³¨å…¥ä¸€æ¬¡å…¨æ–‡åˆ°ä¸Šä¸‹æ–‡å†å²ï¼Œåç»­å¤šè½®ä»…å‘é€é—®é¢˜
            try:
                remembered = (
                    "è¯·è®°ä½ä»¥ä¸‹è®ºæ–‡å…¨æ–‡ï¼Œåç»­æ‰€æœ‰é—®é¢˜ä»…åŸºäºæ­¤å†…å®¹å›ç­”ï¼Œä¸è¦é‡å¤è¾“å‡ºåŸæ–‡ï¼š\n\n"
                    f"{self.paper_content}"
                )
                self.context_history = [remembered, "å·²æ¥æ”¶å¹¶è®°ä½è®ºæ–‡å†…å®¹"]
            except Exception:
                self.context_history = []
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return True
        self.chatbot.append(["é”™è¯¯", "æ— æ³•è¯»å–è®ºæ–‡å†…å®¹ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"])
        yield from update_ui(chatbot=self.chatbot, history=self.history)
        return False

    def _ask(self, q: DeepReadQuestion) -> Generator:
        try:
            # æ„å»ºé€’è¿›å¼åˆ†æçš„ä¸Šä¸‹æ–‡
            context_parts = [
                "è¯·åŸºäºå·²è®°ä½çš„è®ºæ–‡å…¨æ–‡è¿›è¡Œé€’è¿›å¼ç²¾è¯»åˆ†æï¼Œå¹¶ä¸¥æ ¼å›´ç»•é—®é¢˜ä½œç­”ã€‚\n"
                "æ³¨æ„ï¼šè¯·é¿å…æä¾›ä»»ä½•ä»£ç ã€ä¼ªä»£ç ã€å‘½ä»¤è¡Œæˆ–å…·ä½“å®ç°ç»†èŠ‚ï¼›"
                "è‹¥è¾“å‡ºæµç¨‹å›¾ï¼Œé¡»ä½¿ç”¨ ```mermaid ä»£ç å—ï¼Œå…¶ä½™å›ç­”ä¿æŒè‡ªç„¶è¯­è¨€ã€‚\n"
            ]
            
            # æ·»åŠ å‰é¢åˆ†æçš„ç»“æœä½œä¸ºä¸Šä¸‹æ–‡
            if self.results:
                context_parts.append("\nã€å‰é¢åˆ†æçš„å…³é”®å‘ç°ã€‘")
                for prev_q in self.questions:
                    if prev_q.id in self.results and prev_q.id != q.id:
                        # åªæ·»åŠ å‰é¢å·²åˆ†æçš„é—®é¢˜ç»“æœ
                        if any(prev_q.id == existing_id for existing_id in self.results.keys()):
                            context_parts.append(f"\n{prev_q.description}ï¼š{self.results[prev_q.id][:300]}...")
            
            context_parts.append(f"\n\nã€å½“å‰åˆ†æä»»åŠ¡ã€‘\n{q.question}")
            
            prompt = "".join(context_parts)
            
            # è·å–é¢†åŸŸç‰¹å®šçš„ç³»ç»Ÿæç¤º
            sys_prompt = self._get_domain_specific_system_prompt()
            
            resp = yield from request_gpt_model_in_new_thread_with_ui_alive(
                inputs=prompt,
                inputs_show_user=q.question,
                llm_kwargs=self.llm_kwargs,
                chatbot=self.chatbot,
                history=self.context_history or [],
                sys_prompt=sys_prompt,
            )
            if resp:
                self.results[q.id] = resp
                # è®°å½•æœ¬è½®äº¤äº’çš„è¾“å…¥ä¸è¾“å‡ºç”¨äºtokenä¼°ç®—
                try:
                    self._token_inputs.append(prompt)
                    self._token_outputs.append(resp)
                except Exception:
                    pass
                return True
            return False
        except Exception as e:
            self.chatbot.append(["é”™è¯¯", f"ç²¾è¯»é—®é¢˜åˆ†æå¤±è´¥: {str(e)}"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return False

    def _generate_report(self) -> Generator:
        domain_label = "ICä¸“ä¸š" if self.paper_domain == "ic" else "é€šç”¨"
        self.chatbot.append(["ç”ŸæˆæŠ¥å‘Š", f"æ­£åœ¨æ•´åˆ{domain_label}é€’è¿›å¼ç²¾è¯»ç»“æœï¼Œç”Ÿæˆæ·±åº¦æŠ€æœ¯æŠ¥å‘Š..."])
        yield from update_ui(chatbot=self.chatbot, history=self.history)

        if self.paper_domain == "ic":
            prompt = (
                "è¯·å°†ä»¥ä¸‹å¯¹ICè®ºæ–‡çš„é€’è¿›å¼ç²¾è¯»åˆ†ææ•´ç†ä¸ºå®Œæ•´çš„æŠ€æœ¯æŠ¥å‘Šã€‚"
                "æŠ¥å‘Šåº”ä½“ç°ICä¸“ä¸šåˆ†æçš„é€’è¿›é€»è¾‘ï¼šä»é—®é¢˜åŸŸç†è§£â†’ç†è®ºæ„å»ºâ†’æŠ€æœ¯å®ç°â†’å®éªŒéªŒè¯â†’æ‰¹åˆ¤åˆ†æâ†’å·¥ç¨‹å¤ç°â†’æ¶æ„å¯è§†åŒ–â†’å½±å“è¯„ä¼°â†’è¦ç‚¹æ€»ç»“ã€‚"
                "åŒæ—¶åŒ…å«ICä¸“ä¸šåˆ†æï¼šç”µè·¯æ¶æ„â†’å·¥è‰ºæŠ€æœ¯â†’æ€§èƒ½æŒ‡æ ‡â†’è®¾è®¡æ–¹æ³•â†’åˆ¶é€ æµ‹è¯•â†’åº”ç”¨å¸‚åœºã€‚"
                "å±‚æ¬¡æ¸…æ™°ï¼Œçªå‡ºICè®¾è®¡çš„æŠ€æœ¯æ·±åº¦å’Œå·¥ç¨‹ä»·å€¼ï¼Œä¸åŒ…å«ä»»ä½•ä»£ç /ä¼ªä»£ç /å‘½ä»¤è¡Œã€‚"
                "è‹¥åŒ…å«```mermaid ä»£ç å—ï¼Œè¯·åŸæ ·ä¿ç•™ã€‚\n\n"
                "ã€é€’è¿›å¼ICåˆ†æç»“æœã€‘"
            )
        else:
            prompt = (
                "è¯·å°†ä»¥ä¸‹é€’è¿›å¼ç²¾è¯»åˆ†ææ•´ç†ä¸ºå®Œæ•´çš„æŠ€æœ¯æŠ¥å‘Šã€‚"
                "æŠ¥å‘Šåº”ä½“ç°åˆ†æçš„é€’è¿›é€»è¾‘ï¼šä»é—®é¢˜åŸŸç†è§£â†’ç†è®ºæ„å»ºâ†’æŠ€æœ¯å®ç°â†’å®éªŒéªŒè¯â†’æ‰¹åˆ¤åˆ†æâ†’å·¥ç¨‹å¤ç°â†’æ¶æ„å¯è§†åŒ–â†’å½±å“è¯„ä¼°â†’è¦ç‚¹æ€»ç»“ã€‚"
                "å±‚æ¬¡æ¸…æ™°ï¼Œçªå‡ºæ ¸å¿ƒæ€æƒ³ä¸å®éªŒè®¾è®¡è¦ç‚¹ï¼Œä¸åŒ…å«ä»»ä½•ä»£ç /ä¼ªä»£ç /å‘½ä»¤è¡Œã€‚"
                "è‹¥åŒ…å«```mermaid ä»£ç å—ï¼Œè¯·åŸæ ·ä¿ç•™ã€‚\n\n"
                "ã€é€’è¿›å¼åˆ†æç»“æœã€‘"
            )
        
        # æŒ‰ç…§é€’è¿›é¡ºåºç»„ç»‡åˆ†æç»“æœ
        layer_order = [
            "problem_domain_and_motivation",
            "theoretical_framework_and_contributions", 
            "method_design_and_technical_details",
            "experimental_validation_and_effectiveness",
            "assumptions_limitations_and_threats",
            "reproduction_guide_and_engineering",
            "flowcharts_and_architecture",
            "impact_assessment_and_future_directions",
            "executive_summary_and_key_points"
        ]
        
        # å¦‚æœæ˜¯ICè®ºæ–‡ï¼Œæ·»åŠ ICä¸“ç”¨é—®é¢˜
        if self.paper_domain == "ic":
            ic_layer_order = [
                "ic_circuit_architecture_and_system",
                "ic_process_technology_and_devices",
                "ic_performance_metrics_and_constraints",
                "ic_design_methodology_and_eda",
                "ic_manufacturing_and_testing",
                "ic_applications_and_market"
            ]
            layer_order.extend(ic_layer_order)
        
        for layer_id in layer_order:
            for q in self.questions:
                if q.id == layer_id and q.id in self.results:
                    prompt += f"\n\n## {q.description}\n{self.results[q.id]}"
                    break

        if self.paper_domain == "ic":
            sys_prompt = (
                "ä»¥é€’è¿›å¼ICæ·±åº¦åˆ†æä¸ºä¸»çº¿ç»„ç»‡æŠ¥å‘Šï¼šä½“ç°ä»å®è§‚åˆ°å¾®è§‚ã€ä»ç†è®ºåˆ°å®è·µçš„å®Œæ•´åˆ†æé“¾æ¡ï¼Œ"
                "åŒæ—¶çªå‡ºICä¸“ä¸šç‰¹è‰²ã€‚æ¯ä¸ªéƒ¨åˆ†éƒ½è¦ä¸å‰é¢çš„åˆ†æå½¢æˆé€»è¾‘å…³è”ï¼Œçªå‡ºé€’è¿›å…³ç³»ã€‚"
                "ä»¥ICå·¥ç¨‹å®ç°ä¸ºç›®æ ‡ï¼ŒèƒŒæ™¯æç®€ï¼Œæ–¹æ³•ä¸å®ç°ç»†èŠ‚å……åˆ†ï¼Œæ¡ç†åˆ†æ˜ï¼ŒåŒ…å«å¿…è¦çš„æ¸…å•ä¸æ­¥éª¤ã€‚"
                "å¼ºè°ƒæŠ€æœ¯æ·±åº¦ã€å·¥ç¨‹ä»·å€¼å’Œäº§ä¸šåŒ–å‰æ™¯ã€‚"
            )
        else:
            sys_prompt = (
                "ä»¥é€’è¿›å¼æ·±åº¦åˆ†æä¸ºä¸»çº¿ç»„ç»‡æŠ¥å‘Šï¼šä½“ç°ä»å®è§‚åˆ°å¾®è§‚ã€ä»ç†è®ºåˆ°å®è·µçš„å®Œæ•´åˆ†æé“¾æ¡ã€‚"
                "æ¯ä¸ªéƒ¨åˆ†éƒ½è¦ä¸å‰é¢çš„åˆ†æå½¢æˆé€»è¾‘å…³è”ï¼Œçªå‡ºé€’è¿›å…³ç³»ã€‚"
                "ä»¥å·¥ç¨‹å¤ç°ä¸ºç›®æ ‡ï¼ŒèƒŒæ™¯æç®€ï¼Œæ–¹æ³•ä¸å®ç°ç»†èŠ‚å……åˆ†ï¼Œæ¡ç†åˆ†æ˜ï¼ŒåŒ…å«å¿…è¦çš„æ¸…å•ä¸æ­¥éª¤ã€‚"
            )

        resp = yield from request_gpt_model_in_new_thread_with_ui_alive(
            inputs=prompt,
            inputs_show_user=f"ç”Ÿæˆ{domain_label}é€’è¿›å¼è®ºæ–‡ç²¾è¯»æŠ€æœ¯æŠ¥å‘Š",
            llm_kwargs=self.llm_kwargs,
            chatbot=self.chatbot,
            history=[],
            sys_prompt=sys_prompt,
        )
        return resp or "æŠ¥å‘Šç”Ÿæˆå¤±è´¥"

    def _extract_secondary_category(self, report: str) -> str:
        """ä»æŠ¥å‘Šä¸­æå–â€œå½’å±ï¼šâ€åçš„äºŒçº§åˆ†ç±»æ–‡æœ¬ï¼Œåªä¿ç•™ç±»ä¼¼
        â€œ7. æœºå™¨å­¦ä¹ è¾…åŠ©è®¾è®¡ (ML-Aided RF Design) -> ç³»ç»Ÿçº§å»ºæ¨¡ä¸å¿«é€Ÿç»¼åˆâ€ã€‚
        """
        try:
            if not isinstance(report, str):
                return None
            m = re.search(r"^å½’å±ï¼š\s*([^\r\n]+)", report, flags=re.MULTILINE)
            if not m:
                return None
            category_line = m.group(1).strip()
            category_line = re.sub(r"[\s\u3000]+$", "", category_line)
            return category_line if category_line else None
        except Exception:
            return None

    def save_report(self, report: str) -> str:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        domain_prefix = "IC" if self.paper_domain == "ic" else "é€šç”¨"
        pdf_basename = f"æœªçŸ¥{domain_prefix}è®ºæ–‡"
        if self.paper_file_path and os.path.exists(self.paper_file_path):
            pdf_basename = os.path.splitext(os.path.basename(self.paper_file_path))[0]
            pdf_basename = re.sub(r'[^\w\u4e00-\u9fff]', '_', pdf_basename)
            if len(pdf_basename) > 50:
                pdf_basename = pdf_basename[:50]

        parts: List[str] = []
        domain_title = "é›†æˆç”µè·¯è®ºæ–‡ä¸“ä¸šç²¾è¯»æŠ¥å‘Š" if self.paper_domain == "ic" else "è®ºæ–‡ç²¾è¯»æŠ€æœ¯æŠ¥å‘Š"
        parts.append(f"{domain_title}\n\n{report}")
        
        # ä¼˜å…ˆè¿½åŠ æ‰§è¡Œçº§æ‘˜è¦ä¸æµç¨‹å›¾
        if "executive_summary_and_key_points" in self.results:
            parts.append(f"\n\n## æ‰§è¡Œçº§æ‘˜è¦\n\n{self.results['executive_summary_and_key_points']}")
        if "flowcharts_and_architecture" in self.results:
            parts.append(f"\n\n## æ ¸å¿ƒæµç¨‹å›¾\n\n{self.results['flowcharts_and_architecture']}")
        
        # è¿½åŠ å…¶ä½™ç»´åº¦
        for q in self.questions:
            if q.id in self.results and q.id not in {"executive_summary_and_key_points", "flowcharts_and_architecture"}:
                parts.append(f"\n\n## {q.description}\n\n{self.results[q.id]}")

        # è¿½åŠ  Token ä¼°ç®—ç»“æœ
        try:
            stats = estimate_token_usage(self._token_inputs, self._token_outputs, self.llm_kwargs.get('llm_model', 'gpt-3.5-turbo'))
            if stats and stats.get('sum_total_tokens', 0) > 0:
                parts.append(
                    "\n\n## Token ä¼°ç®—\n\n"
                    f"- æ¨¡å‹: {stats.get('model')}\n\n"
                    f"- è¾“å…¥ tokens: {stats.get('sum_input_tokens', 0)}\n"
                    f"- è¾“å‡º tokens: {stats.get('sum_output_tokens', 0)}\n"
                    f"- æ€» tokens: {stats.get('sum_total_tokens', 0)}\n"
                )
        except Exception:
            pass

        content = "".join(parts)
        if hasattr(self, 'yaml_header') and self.yaml_header:
            content = f"{self.yaml_header}\n\n" + content
        result_file = write_history_to_file(
            history=[content],
            file_basename=f"{timestamp}_{pdf_basename}_{domain_prefix}ç²¾è¯»æŠ¥å‘Š.md",
        )
        if result_file and os.path.exists(result_file):
            promote_file_to_downloadzone(result_file, chatbot=self.chatbot)
            return result_file
        return None

    def analyze_paper(self, paper_path: str) -> Generator:
        # åŠ è½½è®ºæ–‡
        ok = yield from self._load_paper(paper_path)
        if not ok:
            return None
        
        # ä¸»é¢˜åˆ†ç±»åˆ¤æ–­
        yield from self._classify_paper_domain()
        
        # æ ¹æ®é¢†åŸŸè·å–ç›¸åº”çš„é—®é¢˜åˆ—è¡¨
        domain_questions = self._get_domain_specific_questions()
        
        # åˆ†æå…³é”®é—®é¢˜
        for q in domain_questions:
            yield from self._ask(q)
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        report = yield from self._generate_report()
        
        # ä»æŠ¥å‘Šä¸­æå–äºŒçº§åˆ†ç±»å½’å±
        self.secondary_category = self._extract_secondary_category(report)
        
        # ç”Ÿæˆ YAML å¤´
        self.yaml_header = yield from self._generate_yaml_header()
        
        # ä¿å­˜æŠ¥å‘Š
        saved = self.save_report(report)
        return saved


def _find_paper_files(path: str) -> List[str]:
    files: List[str] = []
    if os.path.isfile(path):
        ext = os.path.splitext(path)[1].lower()
        if ext in [".pdf", ".docx", ".doc", ".txt", ".md", ".tex"]:
            files.append(path)
        return files
    if os.path.isdir(path):
        exts = [".pdf", ".docx", ".doc", ".txt", ".md", ".tex"]
        for root, _dirs, fnames in os.walk(path):
            for fname in fnames:
                fpath = os.path.join(root, fname)
                if os.path.splitext(fname)[1].lower() in exts:
                    files.append(fpath)
    return files


def _download_paper_by_id(paper_info, chatbot, history) -> str:
    from crazy_functions.review_fns.data_sources.scihub_source import SciHub
    id_type, paper_id = paper_info

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    user_name = chatbot.get_user() if hasattr(chatbot, 'get_user') else "default"
    from toolbox import get_log_folder, get_user
    base_save_dir = get_log_folder(get_user(chatbot), plugin_name='paper_download')
    save_dir = os.path.join(base_save_dir, f"papers_{timestamp}")
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    save_path = Path(save_dir)

    chatbot.append(["ä¸‹è½½è®ºæ–‡", f"æ­£åœ¨ä¸‹è½½{'arXiv' if id_type == 'arxiv' else 'DOI'} {paper_id} çš„è®ºæ–‡..."])
    update_ui(chatbot=chatbot, history=history)

    pdf_path = None
    try:
        if id_type == 'arxiv':
            formatted_id = format_arxiv_id(paper_id)
            paper_result = get_arxiv_paper(formatted_id)
            if not paper_result:
                chatbot.append(["ä¸‹è½½å¤±è´¥", f"æœªæ‰¾åˆ°arXivè®ºæ–‡: {paper_id}"])
                update_ui(chatbot=chatbot, history=history)
                return None
            filename = f"arxiv_{paper_id.replace('/', '_')}.pdf"
            pdf_path = str(save_path / filename)
            paper_result.download_pdf(filename=pdf_path)
        else:
            sci_hub = SciHub(doi=paper_id, path=save_path)
            pdf_path = sci_hub.fetch()

        if pdf_path and os.path.exists(pdf_path):
            promote_file_to_downloadzone(pdf_path, chatbot=chatbot)
            chatbot.append(["ä¸‹è½½æˆåŠŸ", f"å·²æˆåŠŸä¸‹è½½è®ºæ–‡: {os.path.basename(pdf_path)}"])
            update_ui(chatbot=chatbot, history=history)
            return pdf_path
        chatbot.append(["ä¸‹è½½å¤±è´¥", f"è®ºæ–‡ä¸‹è½½å¤±è´¥: {paper_id}"])
        update_ui(chatbot=chatbot, history=history)
        return None
    except Exception as e:
        chatbot.append(["ä¸‹è½½é”™è¯¯", f"ä¸‹è½½è®ºæ–‡æ—¶å‡ºé”™: {str(e)}"])
        update_ui(chatbot=chatbot, history=history)
        return None


@CatchException
def æ‰¹é‡è®ºæ–‡ç²¾è¯»(txt: str, llm_kwargs: Dict, plugin_kwargs: Dict, chatbot: List,
            history: List, system_prompt: str, user_request: str):
    """ä¸»å‡½æ•° - æ‰¹é‡è®ºæ–‡ç²¾è¯»ï¼ˆæ”¯æŒé¢†åŸŸè‡ªé€‚åº”ï¼‰"""
    chatbot.append([
        "å‡½æ•°æ’ä»¶åŠŸèƒ½åŠä½¿ç”¨æ–¹å¼",
        (
            "æ‰¹é‡è®ºæ–‡ç²¾è¯»ï¼šæ™ºèƒ½è¯†åˆ«è®ºæ–‡ä¸»é¢˜ï¼ˆé€šç”¨/ICï¼‰ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„é€’è¿›å¼ç²¾è¯»åˆ†æç­–ç•¥ï¼Œ"
            "ä¸ºæ¯ç¯‡è®ºæ–‡ç”Ÿæˆé¢å‘å®ç°ä¸å¤ç°çš„æ·±åº¦æŠ€æœ¯æŠ¥å‘Šã€‚\n\n"
            "ä½¿ç”¨æ–¹å¼ï¼š\n1) è¾“å…¥åŒ…å«å¤šä¸ªPDFçš„æ–‡ä»¶å¤¹è·¯å¾„ï¼›\n2) æˆ–è¾“å…¥å¤šä¸ªè®ºæ–‡IDï¼ˆDOIæˆ–arXivï¼‰ï¼Œç”¨é€—å·åˆ†éš”ï¼›\n3) ç‚¹å‡»å¼€å§‹ã€‚\n\n"
            "æ™ºèƒ½åˆ†æç‰¹æ€§ï¼š\n- è‡ªåŠ¨ä¸»é¢˜åˆ†ç±»ï¼ˆé€šç”¨è®ºæ–‡ vs ICè®ºæ–‡ï¼‰\n- é€’è¿›å¼æ·±åº¦åˆ†æï¼ˆ9å±‚é€’è¿›é€»è¾‘ï¼‰\n- é¢†åŸŸç‰¹å®šä¸“ä¸šé—®é¢˜\n- ç»Ÿä¸€çš„æŠ¥å‘Šæ ¼å¼å’ŒYAMLå…ƒä¿¡æ¯\n\n"
            "æ³¨æ„äº‹é¡¹ï¼š\n- è‹¥éœ€è¦è¾“å‡ºå…¬å¼ï¼Œè¯·ä½¿ç”¨ LaTeX æ•°å­¦æ ¼å¼ï¼šè¡Œå†…å…¬å¼ç”¨ $...$ï¼Œè¡Œé—´å…¬å¼ç”¨ $$...$$ã€‚"
        ),
    ])
    yield from update_ui(chatbot=chatbot, history=history)

    paper_files: List[str] = []

    if ',' in txt:
        paper_ids = [pid.strip() for pid in txt.split(',') if pid.strip()]
        chatbot.append(["æ£€æµ‹åˆ°å¤šä¸ªè®ºæ–‡ID", f"æ£€æµ‹åˆ° {len(paper_ids)} ä¸ªè®ºæ–‡IDï¼Œå‡†å¤‡æ‰¹é‡ä¸‹è½½..."])
        yield from update_ui(chatbot=chatbot, history=history)
        for i, pid in enumerate(paper_ids):
            paper_info = extract_paper_id(pid)
            if paper_info:
                chatbot.append([f"ä¸‹è½½è®ºæ–‡ {i+1}/{len(paper_ids)}", f"æ­£åœ¨ä¸‹è½½ {'arXiv' if paper_info[0] == 'arxiv' else 'DOI'} ID: {paper_info[1]}..."])
                yield from update_ui(chatbot=chatbot, history=history)
                p = _download_paper_by_id(paper_info, chatbot, history)
                if p:
                    paper_files.append(p)
                else:
                    chatbot.append(["ä¸‹è½½å¤±è´¥", f"æ— æ³•ä¸‹è½½è®ºæ–‡: {pid}"])
                    yield from update_ui(chatbot=chatbot, history=history)
            else:
                chatbot.append(["IDæ ¼å¼é”™è¯¯", f"æ— æ³•è¯†åˆ«è®ºæ–‡IDæ ¼å¼: {pid}"])
                yield from update_ui(chatbot=chatbot, history=history)
    else:
        paper_info = extract_paper_id(txt)
        if paper_info:
            chatbot.append(["æ£€æµ‹åˆ°è®ºæ–‡ID", f"æ£€æµ‹åˆ°{'arXiv' if paper_info[0] == 'arxiv' else 'DOI'} ID: {paper_info[1]}ï¼Œå‡†å¤‡ä¸‹è½½è®ºæ–‡..."])
            yield from update_ui(chatbot=chatbot, history=history)
            p = _download_paper_by_id(paper_info, chatbot, history)
            if p:
                paper_files.append(p)
            else:
                report_exception(chatbot, history, a="ä¸‹è½½è®ºæ–‡å¤±è´¥", b=f"æ— æ³•ä¸‹è½½{'arXiv' if paper_info[0] == 'arxiv' else 'DOI'}è®ºæ–‡: {paper_info[1]}")
                yield from update_ui(chatbot=chatbot, history=history)
                return
        else:
            if not os.path.exists(txt):
                report_exception(chatbot, history, a=f"æ‰¹é‡ç²¾è¯»è®ºæ–‡: {txt}", b=f"æ‰¾ä¸åˆ°æ–‡ä»¶æˆ–æ— æƒè®¿é—®: {txt}")
                yield from update_ui(chatbot=chatbot, history=history)
                return
            user_name = chatbot.get_user()
            validate_path_safety(txt, user_name)
            paper_files = _find_paper_files(txt)
            if not paper_files:
                report_exception(chatbot, history, a="æ‰¹é‡ç²¾è¯»è®ºæ–‡", b=f"åœ¨è·¯å¾„ {txt} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„è®ºæ–‡æ–‡ä»¶")
                yield from update_ui(chatbot=chatbot, history=history)
                return

    yield from update_ui(chatbot=chatbot, history=history)

    if not paper_files:
        chatbot.append(["é”™è¯¯", "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯åˆ†æçš„è®ºæ–‡æ–‡ä»¶"])
        yield from update_ui(chatbot=chatbot, history=history)
        return

    chatbot.append(["å¼€å§‹æ™ºèƒ½æ‰¹é‡ç²¾è¯»", f"æ‰¾åˆ° {len(paper_files)} ç¯‡è®ºæ–‡ï¼Œå¼€å§‹æ™ºèƒ½ä¸»é¢˜åˆ†ç±»å’Œé€’è¿›å¼æ·±åº¦åˆ†æ..."])
    yield from update_ui(chatbot=chatbot, history=history)

    analyzer = BatchPaperDetailAnalyzer(llm_kwargs, plugin_kwargs, chatbot, history, system_prompt)

    successes: List[str] = []
    failures: List[str] = []
    domain_stats = {"general": 0, "ic": 0}

    for i, paper_file in enumerate(paper_files):
        try:
            chatbot.append([f"ç²¾è¯»è®ºæ–‡ {i+1}/{len(paper_files)}", f"æ­£åœ¨æ™ºèƒ½ç²¾è¯»: {os.path.basename(paper_file)}"])
            yield from update_ui(chatbot=chatbot, history=history)
            outfile = yield from analyzer.analyze_paper(paper_file)
            if outfile:
                successes.append((os.path.basename(paper_file), outfile, analyzer.paper_domain))
                domain_stats[analyzer.paper_domain] += 1
                domain_label = "IC" if analyzer.paper_domain == "ic" else "é€šç”¨"
                chatbot.append([f"å®Œæˆè®ºæ–‡ {i+1}/{len(paper_files)}", f"æˆåŠŸç”Ÿæˆ{domain_label}ç²¾è¯»æŠ¥å‘Š: {os.path.basename(outfile)}"])
            else:
                failures.append(os.path.basename(paper_file))
                chatbot.append([f"å¤±è´¥è®ºæ–‡ {i+1}/{len(paper_files)}", f"åˆ†æå¤±è´¥: {os.path.basename(paper_file)}"])
            yield from update_ui(chatbot=chatbot, history=history)
        except Exception as e:
            failures.append(os.path.basename(paper_file))
            chatbot.append([f"é”™è¯¯è®ºæ–‡ {i+1}/{len(paper_files)}", f"åˆ†æå‡ºé”™: {os.path.basename(paper_file)} - {str(e)}"])
            yield from update_ui(chatbot=chatbot, history=history)

    summary = "æ™ºèƒ½æ‰¹é‡ç²¾è¯»å®Œæˆï¼\n\n"
    summary += "ğŸ“Š åˆ†æç»Ÿè®¡ï¼š\n"
    summary += f"- æ€»è®ºæ–‡æ•°ï¼š{len(paper_files)}\n"
    summary += f"- æˆåŠŸåˆ†æï¼š{len(successes)}\n"
    summary += f"- åˆ†æå¤±è´¥ï¼š{len(failures)}\n\n"
    
    summary += "ğŸ¯ ä¸»é¢˜åˆ†ç±»ç»Ÿè®¡ï¼š\n"
    summary += f"- é€šç”¨è®ºæ–‡ï¼š{domain_stats['general']} ç¯‡\n"
    summary += f"- ICè®ºæ–‡ï¼š{domain_stats['ic']} ç¯‡\n\n"
    
    if successes:
        summary += "âœ… æˆåŠŸç”ŸæˆæŠ¥å‘Šï¼š\n"
        for paper_name, report_path, domain in successes:
            domain_label = "IC" if domain == "ic" else "é€šç”¨"
            summary += f"- {paper_name} ({domain_label}) â†’ {os.path.basename(report_path)}\n"
    if failures:
        summary += "\nâŒ åˆ†æå¤±è´¥çš„è®ºæ–‡ï¼š\n"
        for name in failures:
            summary += f"- {name}\n"

    chatbot.append(["æ™ºèƒ½æ‰¹é‡ç²¾è¯»å®Œæˆ", summary])
    yield from update_ui(chatbot=chatbot, history=history)


