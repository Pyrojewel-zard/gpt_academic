import json
import re
import os
import time
import glob
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass
from typing import Dict, List, Generator, Tuple
from crazy_functions.crazy_utils import request_gpt_model_in_new_thread_with_ui_alive
from toolbox import update_ui, promote_file_to_downloadzone, write_history_to_file, CatchException, report_exception
from shared_utils.fastapi_server import validate_path_safety
from crazy_functions.paper_fns.paper_download import extract_paper_id, extract_paper_ids, get_arxiv_paper, format_arxiv_id
import difflib
import re


@dataclass
class PaperQuestion:
    """è®ºæ–‡åˆ†æé—®é¢˜ç±»"""
    id: str  # é—®é¢˜ID
    question: str  # é—®é¢˜å†…å®¹
    importance: int  # é‡è¦æ€§ (1-5ï¼Œ5æœ€é«˜)
    description: str  # é—®é¢˜æè¿°


class BatchPaperAnalyzer:
    """æ‰¹é‡è®ºæ–‡å¿«é€Ÿåˆ†æå™¨"""

    def __init__(self, llm_kwargs: Dict, plugin_kwargs: Dict, chatbot: List, history: List, system_prompt: str):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.llm_kwargs = llm_kwargs
        self.plugin_kwargs = plugin_kwargs
        self.chatbot = chatbot
        self.history = history
        self.system_prompt = system_prompt
        self.paper_content = ""
        self.results = {}
        self.paper_file_path = None
        # ---------- è¯»å–åˆ†ç±»æ ‘ ----------
        json_path = os.path.join(os.path.dirname(__file__), 'paper.json')
        with open(json_path, 'r', encoding='utf-8') as f:
            self.category_tree = json.load(f)          # Dict[str, List[str]]

        # ç”Ÿæˆç»™ LLM çš„å½“å‰åˆ†ç±»æ¸…å•
        category_lines = [f"{main} -> {', '.join(subs)}"
                        for main, subs in self.category_tree.items()]
        self.category_prompt_str = '\n'.join(category_lines)
        # å®šä¹‰è®ºæ–‡åˆ†æé—®é¢˜åº“ï¼ˆä¸Paper_Readingä¿æŒä¸€è‡´ï¼‰
        self.questions = [
            PaperQuestion(
                id="research_and_methods",
                question="è¿™ç¯‡è®ºæ–‡çš„ä¸»è¦ç ”ç©¶é—®é¢˜ã€ç›®æ ‡å’Œæ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿè¯·åˆ†æï¼š1)è®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶é—®é¢˜å’Œç ”ç©¶åŠ¨æœºï¼›2)è®ºæ–‡æå‡ºçš„å…³é”®æ–¹æ³•ã€æ¨¡å‹æˆ–ç†è®ºæ¡†æ¶ï¼›3)è¿™äº›æ–¹æ³•å¦‚ä½•è§£å†³ç ”ç©¶é—®é¢˜ã€‚",
                importance=5,
                description="ç ”ç©¶é—®é¢˜ä¸æ–¹æ³•"
            ),
            PaperQuestion(
                id="findings_and_innovation",
                question="è®ºæ–‡çš„ä¸»è¦å‘ç°ã€ç»“è®ºåŠåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿè¯·åˆ†æï¼š1)è®ºæ–‡çš„æ ¸å¿ƒç»“æœä¸ä¸»è¦å‘ç°ï¼›2)ä½œè€…å¾—å‡ºçš„å…³é”®ç»“è®ºï¼›3)ç ”ç©¶çš„åˆ›æ–°ç‚¹ä¸å¯¹é¢†åŸŸçš„è´¡çŒ®ï¼›4)ä¸å·²æœ‰å·¥ä½œçš„åŒºåˆ«ã€‚",
                importance=4,
                description="ç ”ç©¶å‘ç°ä¸åˆ›æ–°"
            ),
            PaperQuestion(
                id="methodology_and_data",
                question="è®ºæ–‡ä½¿ç”¨äº†ä»€ä¹ˆç ”ç©¶æ–¹æ³•å’Œæ•°æ®ï¼Ÿè¯·è¯¦ç»†åˆ†æï¼š1)ç ”ç©¶è®¾è®¡ä¸å®éªŒè®¾ç½®ï¼›2)æ•°æ®æ”¶é›†æ–¹æ³•ä¸æ•°æ®é›†ç‰¹ç‚¹ï¼›3)åˆ†ææŠ€æœ¯ä¸è¯„ä¼°æ–¹æ³•ï¼›4)æ–¹æ³•å­¦ä¸Šçš„åˆç†æ€§ã€‚",
                importance=3,
                description="ç ”ç©¶æ–¹æ³•ä¸æ•°æ®"
            ),
            PaperQuestion(
                id="limitations_and_impact",
                question="è®ºæ–‡çš„å±€é™æ€§ã€æœªæ¥æ–¹å‘åŠæ½œåœ¨å½±å“æ˜¯ä»€ä¹ˆï¼Ÿè¯·åˆ†æï¼š1)ç ”ç©¶çš„ä¸è¶³ä¸é™åˆ¶å› ç´ ï¼›2)ä½œè€…æå‡ºçš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼›3)è¯¥ç ”ç©¶å¯¹å­¦æœ¯ç•Œå’Œè¡Œä¸šå¯èƒ½äº§ç”Ÿçš„å½±å“ï¼›4)ç ”ç©¶ç»“æœçš„é€‚ç”¨èŒƒå›´ä¸æ¨å¹¿ä»·å€¼ã€‚",
                importance=2,
                description="å±€é™æ€§ä¸å½±å“"
            ),
            PaperQuestion(
                id="worth_reading_judgment",
                question="è¯·ç»¼åˆè¯„ä¼°è¿™ç¯‡è®ºæ–‡æ˜¯å¦å€¼å¾—ç²¾è¯»ï¼Œå¹¶ä»å¤šä¸ªè§’åº¦ç»™å‡ºåˆ¤æ–­ä¾æ®ï¼š1) **åˆ›æ–°æ€§ä¸é‡è¦æ€§**ï¼šè®ºæ–‡çš„ç ”ç©¶æ˜¯å¦å…·æœ‰å¼€åˆ›æ€§ï¼Ÿæ˜¯å¦è§£å†³äº†é¢†åŸŸå†…çš„å…³é”®é—®é¢˜ï¼Ÿ2) **æ–¹æ³•å¯é æ€§**ï¼šç ”ç©¶æ–¹æ³•æ˜¯å¦ä¸¥è°¨ã€å¯é ï¼Ÿå®éªŒè®¾è®¡æ˜¯å¦åˆç†ï¼Ÿ3) **è®ºè¿°æ¸…æ™°åº¦**ï¼šè®ºæ–‡çš„å†™ä½œé£æ ¼ã€å›¾è¡¨è´¨é‡å’Œé€»è¾‘ç»“æ„æ˜¯å¦æ¸…æ™°æ˜“æ‡‚ï¼Ÿ4) **æ½œåœ¨å½±å“**ï¼šç ”ç©¶æˆæœæ˜¯å¦å¯èƒ½å¯¹å­¦æœ¯ç•Œæˆ–å·¥ä¸šç•Œäº§ç”Ÿè¾ƒå¤§å½±å“ï¼Ÿ5) **ç»¼åˆå»ºè®®**ï¼šç»“åˆä»¥ä¸Šå‡ ç‚¹ï¼Œç»™å‡ºâ€œå¼ºçƒˆæ¨èâ€ã€â€œæ¨èâ€ã€â€œä¸€èˆ¬â€æˆ–â€œä¸æ¨èâ€çš„æœ€ç»ˆè¯„çº§ï¼Œå¹¶ç®€è¦è¯´æ˜ç†ç”±ã€‚",
                importance=2,
                description="æ˜¯å¦å€¼å¾—ç²¾è¯»"
            ),
            PaperQuestion(
                id="category_assignment",
                question=(
                    "è¯·æ ¹æ®è®ºæ–‡å†…å®¹ï¼Œåˆ¤æ–­å…¶æœ€å‡†ç¡®çš„äºŒçº§åˆ†ç±»å½’å±ã€‚\n\n"
                    "å½“å‰åˆ†ç±»æ ‘å¦‚ä¸‹ï¼ˆä¸€çº§ -> äºŒçº§ï¼‰ï¼š\n"
                    f"{self.category_prompt_str}\n\n"
                    "è¦æ±‚ï¼š\n"
                    "1) è‹¥å®Œå…¨åŒ¹é…ç°æœ‰äºŒçº§åˆ†ç±»ï¼Œç›´æ¥å›ç­”ï¼š\n"
                    "   å½’å±ï¼š<ä¸€çº§ç±»åˆ«> -> <äºŒçº§å­åˆ†ç±»>\n"
                    "2) è‹¥éœ€æ–°å»ºäºŒçº§åˆ†ç±»ï¼Œå›ç­”ï¼š\n"
                    "   æ–°å¢äºŒçº§ï¼š<ä¸€çº§ç±»åˆ«> -> <æ–°å­åˆ†ç±»å>\n"
                    "3) è‹¥éœ€æ–°å»ºä¸€çº§ç±»åˆ«ï¼Œå›ç­”ï¼š\n"
                    "   æ–°å¢ä¸€çº§ï¼š<æ–°ä¸€çº§ç±»åˆ«> -> [<å­åˆ†ç±»1>, <å­åˆ†ç±»2>, ...]\n"
                    "4) ç”¨ä¸€å¥è¯è¯´æ˜åˆ¤æ–­ç†ç”±ã€‚"
                ),
                importance=1,
                description="è®ºæ–‡äºŒçº§åˆ†ç±»å½’å±"
            ),              

            PaperQuestion(
                id="core_algorithm_flowcharts",
                question=(
                    "è¯·åŸºäºè®ºæ–‡å†…å®¹ï¼Œç»˜åˆ¶è®ºæ–‡æ ¸å¿ƒç®—æ³•æˆ–æ ¸å¿ƒæ€è·¯çš„æµç¨‹å›¾ï¼Œè‹¥è®ºæ–‡åŒ…å«å¤šä¸ªç›¸å¯¹ç‹¬ç«‹çš„æ¨¡å—æˆ–é˜¶æ®µï¼Œè¯·åˆ†åˆ«ç»™å‡ºå¤šä¸ªæµç¨‹å›¾ã€‚\n\n"
                    "è¦æ±‚ï¼š\n"
                    "1) æ¯ä¸ªæµç¨‹å›¾ä½¿ç”¨ Mermaid è¯­æ³•ï¼Œä»£ç å—éœ€ä»¥ ```mermaid å¼€å§‹ï¼Œä»¥ ``` ç»“æŸï¼›\n"
                    "2) æ¨èä½¿ç”¨ flowchart TD æˆ– LRï¼ŒèŠ‚ç‚¹éœ€æ¦‚æ‹¬å…³é”®æ­¥éª¤/å­æ¨¡å—ï¼ŒåŒ…å«ä¸»è¦æ•°æ®æµä¸å…³é”®åˆ†æ”¯/åˆ¤å®šï¼›\n"
                    "3) æ¯ä¸ªæµç¨‹å›¾å‰ä»¥ä¸€å¥è¯æ ‡æ˜æ¨¡å—/é˜¶æ®µåç§°ï¼Œä¾‹å¦‚ï¼šæ¨¡å—ï¼šè®­ç»ƒé˜¶æ®µï¼›\n"
                    "4) ä»…èšç„¦æ ¸å¿ƒé€»è¾‘ï¼Œé¿å…è¿‡åº¦ç»†èŠ‚ï¼›\n"
                    "5) è‹¥åªæœ‰å•ä¸€æ ¸å¿ƒæµç¨‹ï¼Œä»…è¾“å‡ºä¸€ä¸ªæµç¨‹å›¾ï¼›\n"
                    "6) æ ¼å¼çº¦æŸï¼š\n"
                    "   - èŠ‚ç‚¹åç”¨å¼•å·åŒ…è£¹ï¼Œå¦‚ [\"èŠ‚ç‚¹å\"] æˆ– (\"èŠ‚ç‚¹å\")ï¼›\n"
                    "   - ç®­å¤´æ ‡ç­¾é‡‡ç”¨ |\"æ ‡ç­¾å\"| å½¢å¼ï¼Œä¸” | ä¸ \" ä¹‹é—´ä¸è¦æœ‰ç©ºæ ¼ï¼›\n"
                    "   - æ ¹æ®é€»è¾‘é€‰æ‹© flowchart LRï¼ˆä»å·¦åˆ°å³ï¼‰æˆ– flowchart TDï¼ˆä»ä¸Šåˆ°ä¸‹ï¼‰ã€‚\n"
                    "7) ç¤ºä¾‹ï¼š\n"
                    "```mermaid\n"
                    "flowchart LR\n"
                    "    A[\"è¾“å…¥\"] --> B(\"å¤„ç†\")\n"
                    "    B --> C{\"æ˜¯å¦æ»¡è¶³æ¡ä»¶\"}\n"
                    "    C --> D[\"è¾“å‡º1\"]\n"
                    "    C --> |\"å¦\"| E[\"è¾“å‡º2\"]\n"
                    "```"
                ),
                importance=5,
                description="æ ¸å¿ƒç®—æ³•/æ€è·¯æµç¨‹å›¾ï¼ˆMermaidï¼‰"
            ),
            PaperQuestion(
                id="core_idea_ppt_md",
                question=(
                    "è¯·ç”Ÿæˆä¸€ä»½ç”¨äº PPT çš„â€˜è®ºæ–‡æ ¸å¿ƒæ€è·¯ä¸ç®—æ³•â€™æç®€ Markdown æ‘˜è¦ï¼Œå¹¶ä¸å·²ç”Ÿæˆçš„ Mermaid æµç¨‹å›¾å½¢æˆé…å¥—è¯´æ˜ã€‚\n\n"
                    "è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š\n"
                    "# æ€»è¿°ï¼ˆ1 è¡Œï¼‰\n"
                    "- ç”¨æœ€ç®€ä¸€å¥è¯æ¦‚æ‹¬è®ºæ–‡åšäº†ä»€ä¹ˆã€ä¸ºä½•æœ‰æ•ˆã€‚\n\n"
                    "# æ¨¡å—è¦ç‚¹ï¼ˆä¸æµç¨‹å›¾å¯¹åº”ï¼‰\n"
                    "- è‹¥å­˜åœ¨å¤šä¸ªæµç¨‹å›¾/æ¨¡å—ï¼šæŒ‰â€œæ¨¡å—ï¼šåç§°â€åˆ†ç»„ï¼Œæ¯ç»„åˆ—å‡º 3-5 æ¡â€˜å›¾è§£è¦ç‚¹â€™ï¼Œæ¯æ¡ â‰¤ 14 å­—ï¼Œæ¦‚æ‹¬æ ¸å¿ƒè¾“å…¥â†’å¤„ç†â†’è¾“å‡ºä¸å…³é”®åˆ†æ”¯ã€‚\n"
                    "- è‹¥ä»…æœ‰ä¸€ä¸ªæµç¨‹å›¾ï¼šä»…è¾“å‡ºè¯¥æµç¨‹å›¾çš„ 3-5 æ¡â€˜å›¾è§£è¦ç‚¹â€™ã€‚\n\n"
                    "# å…³é”®ç®—æ³•æ‘˜è¦ï¼ˆ5-8 æ¡ï¼‰\n"
                    "- æ¯æ¡ â‰¤ 16 å­—ï¼Œèšç„¦è¾“å…¥/æ­¥éª¤/è¾“å‡º/åˆ›æ–°ï¼Œä¸å†™èƒŒæ™¯ã€‚\n\n"
                    "# åº”ç”¨ä¸æ•ˆæœï¼ˆâ‰¤ 3 æ¡ï¼Œå¯çœç•¥ï¼‰\n"
                    "- åœºæ™¯/æŒ‡æ ‡/æ”¶ç›Šã€‚\n\n"
                    "æ³¨æ„ï¼šä»…è¾“å‡ºä¸Šè¿° Markdown ç»“æ„ï¼Œä¸åµŒå…¥ä»£ç ï¼Œä¸é‡å¤æµç¨‹å›¾æœ¬èº«ã€‚"
                ),
                importance=5,
                description="PPT ç”¨æ ¸å¿ƒæ€è·¯ä¸ç®—æ³•ï¼ˆMarkdown æç®€ç‰ˆï¼‰"
            ),
        ]

        # æŒ‰é‡è¦æ€§æ’åº
        self.questions.sort(key=lambda q: q.importance, reverse=True)

    # ---------- å…³é”®è¯åº“å·¥å…· ----------
    def _get_keywords_db_path(self) -> str:
        return os.path.join(os.path.dirname(__file__), 'keywords.txt')

    def _load_keywords_db(self) -> List[str]:
        path = self._get_keywords_db_path()
        if os.path.exists(path):
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    return [line.strip() for line in f if line.strip()]
            except Exception:
                return []
        return []

    def _save_keywords_db(self, keywords: List[str]):
        path = self._get_keywords_db_path()
        try:
            with open(path, 'w', encoding='utf-8') as f:
                for kw in sorted(set(keywords), key=lambda x: x.lower()):
                    f.write(kw + '\n')
        except Exception:
            pass

    def _normalize_keyword(self, kw: str) -> str:
        kw = kw.strip()
        # è‹±æ–‡å…³é”®è¯ï¼šç»Ÿä¸€å°å†™ï¼Œå»é™¤å¤šä½™ç©ºç™½ä¸å°¾éƒ¨æ ‡ç‚¹
        kw = re.sub(r'[\s\u3000]+', ' ', kw)
        kw = kw.strip().strip('.,;:')
        return kw.lower()

    def _find_similar_in_db(self, db: List[str], new_kw: str, threshold: float = 0.88) -> str:
        if not new_kw:
            return None
        candidates = difflib.get_close_matches(new_kw, [self._normalize_keyword(k) for k in db], n=1, cutoff=threshold)
        if candidates:
            # æ˜ å°„å›åŸå§‹å¤§å°å†™å½¢å¼ï¼ˆä¼˜å…ˆç¬¬ä¸€ä¸ªåŒ¹é…é¡¹ï¼‰
            norm = candidates[0]
            for k in db:
                if self._normalize_keyword(k) == norm:
                    return k
        return None

    def _merge_keywords_with_db(self, extracted_keywords: List[str]) -> Tuple[List[str], List[str]]:
        """
        å°†æå–çš„å…³é”®è¯ä¸å…³é”®è¯åº“è¿›è¡Œåˆå¹¶å»é‡ï¼Œè¿”å›ï¼š
        - canonical_keywords: æ›¿æ¢/åˆå¹¶åçš„å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨äºå†™å› YAMLï¼‰
        - updated_db: æ›´æ–°åçš„å…³é”®è¯åº“ï¼ˆè‹¥æœ‰æ–°å¢ï¼‰
        """
        db = self._load_keywords_db()
        canonical_list: List[str] = []

        for kw in extracted_keywords:
            clean = self._normalize_keyword(kw)
            if not clean:
                continue
            similar = self._find_similar_in_db(db, clean)
            if similar:
                # ä½¿ç”¨åº“ä¸­çš„æ ‡å‡†è¯å½¢
                if similar not in canonical_list:
                    canonical_list.append(similar)
            else:
                # æ–°å…³é”®è¯ï¼šåŠ å…¥åº“ä¸ç»“æœ
                db.append(kw)
                if kw not in canonical_list:
                    canonical_list.append(kw)

        # ä¿å­˜æ›´æ–°çš„å…³é”®è¯åº“
        self._save_keywords_db(db)
        return canonical_list, db

    def _generate_yaml_header(self) -> Generator:
        """åŸºäºè®ºæ–‡å†…å®¹ä¸å·²å¾—åˆ†æï¼Œç”Ÿæˆ YAML å¤´éƒ¨ï¼ˆæ ¸å¿ƒå…ƒä¿¡æ¯ï¼‰"""
        try:
            prompt = (
                "è¯·åŸºäºä»¥ä¸‹è®ºæ–‡å†…å®¹ä¸åˆ†æè¦ç‚¹ï¼Œæå–è®ºæ–‡æ ¸å¿ƒå…ƒä¿¡æ¯å¹¶è¾“å‡º YAML Front Matterï¼š\n\n"
                f"è®ºæ–‡å…¨æ–‡å†…å®¹ç‰‡æ®µï¼š\n{self.paper_content}\n\n"
                "è‹¥æœ‰å¯ç”¨çš„åˆ†æè¦ç‚¹ï¼š\n"
            )

            # å°†å·²æœ‰ç»“æœç®€è¦æ‹¼æ¥ï¼Œè¾…åŠ©æå–
            for q in self.questions:
                if q.id in self.results:
                    prompt += f"- {q.description}: {self.results[q.id][:400]}\n"

            prompt += (
                "\nä¸¥æ ¼è¾“å‡º YAMLï¼ˆä¸ä½¿ç”¨ä»£ç å—å›´æ ï¼‰ï¼Œå­—æ®µå¦‚ä¸‹ï¼š\n"
                "title: åŸæ–‡æ ‡é¢˜ï¼ˆå°½é‡è‹±æ–‡åŸé¢˜,æ ‡é¢˜éœ€è¦æœ‰å¼•å·åŒ…è£¹ï¼‰\n"
                "title_zh: ä¸­æ–‡æ ‡é¢˜ï¼ˆè‹¥å¯ï¼‰\n"
                "authors: [ä½œè€…è‹±æ–‡ååˆ—è¡¨]\n"
                "affiliation_zh: ç¬¬ä¸€ä½œè€…å•ä½ï¼ˆä¸­æ–‡ï¼‰\n"
                "keywords: [è‹±æ–‡å…³é”®è¯åˆ—è¡¨]\n"
                "urls: [è®ºæ–‡é“¾æ¥, Githubé“¾æ¥æˆ–None]\n"
                "source_code: [æºç é“¾æ¥, None]\n"
                "read_status: [å·²é˜…è¯», æœªé˜…è¯»]\n"
                "stars: [â­â­â­â­â­, â­â­â­â­, â­â­â­, â­â­, â­]\n"
                "ä»…è¾“å‡ºä»¥ --- å¼€å§‹ã€ä»¥ --- ç»“æŸçš„ YAML Front Matterï¼Œä¸è¦é™„åŠ å…¶ä»–æ–‡æœ¬ã€‚é»˜è®¤starsä¸ºâ­â­â­ï¼Œread_statusä¸ºæœªé˜…è¯»ã€‚"
            )

            yaml_str = yield from request_gpt_model_in_new_thread_with_ui_alive(
                inputs=prompt,
                inputs_show_user="ç”Ÿæˆè®ºæ–‡æ ¸å¿ƒä¿¡æ¯ YAML å¤´",
                llm_kwargs=self.llm_kwargs,
                chatbot=self.chatbot,
                history=[],
                sys_prompt=(
                    "ä½ æ˜¯è®ºæ–‡ä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚è¯·ä»…è¾“å‡º YAML Front Matterï¼Œ"
                    "é”®åå›ºå®šä¸”é¡ºåºä¸é™ï¼Œæ³¨æ„ authors/keywords/urls åº”ä¸ºåˆ—è¡¨ã€‚"
                )
            )

            # ç®€å•æ ¡éªŒï¼Œç¡®ä¿åŒ…å« YAML åˆ†éš”ç¬¦
            if isinstance(yaml_str, str) and yaml_str.strip().startswith("---") and yaml_str.strip().endswith("---"):
                # è§£æå¹¶è§„èŒƒåŒ– keywords åˆ—è¡¨
                text = yaml_str.strip()
                m = re.search(r"^keywords:\s*\[(.*?)\]\s*$", text, flags=re.MULTILINE)
                if m:
                    inner = m.group(1).strip()
                    # ç®€å•è§£æåˆ—è¡¨å†…å®¹ï¼Œæ”¯æŒå¸¦å¼•å·æˆ–ä¸å¸¦å¼•å·çš„è‹±æ–‡å…³é”®è¯
                    # æ‹†åˆ†é€—å·ï¼ŒåŒæ—¶å»æ‰åŒ…è£¹å¼•å·
                    raw_list = [x.strip().strip('"\'\'') for x in inner.split(',') if x.strip()]
                    merged, _ = self._merge_keywords_with_db(raw_list)
                    # ä»¥åŸæ ·å¼å†™å›ï¼ˆä½¿ç”¨å¼•å·åŒ…è£¹ï¼Œé¿å… YAML è§£æé—®é¢˜ï¼‰
                    rebuilt = ', '.join([f'"{k}"' for k in merged])
                    text = re.sub(r"^keywords:\s*\[(.*?)\]\s*$", f"keywords: [{rebuilt}]", text, flags=re.MULTILINE)
                return text
            return None

        except Exception as e:
            self.chatbot.append(["è­¦å‘Š", f"ç”Ÿæˆ YAML å¤´å¤±è´¥: {str(e)}"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return None
    def _update_category_json(self, llm_answer: str):
        """
        è§£æ LLM è¿”å›çš„å½’å±/æ–°å¢æŒ‡ä»¤ï¼Œå¹¶æ›´æ–° paper.json
        """
        json_path = os.path.join(os.path.dirname(__file__), 'paper.json')

        # 1) æ–°å¢ä¸€çº§
        m1 = re.search(r'æ–°å¢ä¸€çº§ï¼š(.+?) *-> *\[(.+?)\]', llm_answer)
        if m1:
            new_main = m1.group(1).strip()
            new_subs = [s.strip() for s in m1.group(2).split(',')]
            self.category_tree[new_main] = new_subs
        else:
            # 2) æ–°å¢äºŒçº§
            m2 = re.search(r'æ–°å¢äºŒçº§ï¼š(.+?) *-> *(.+)', llm_answer)
            if m2:
                main_cat = m2.group(1).strip()
                new_sub = m2.group(2).strip()
                if main_cat in self.category_tree and new_sub not in self.category_tree[main_cat]:
                    self.category_tree[main_cat].append(new_sub)

        # å†™å›
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.category_tree, f, ensure_ascii=False, indent=4)


    def _load_paper(self, paper_path: str) -> Generator:
        from crazy_functions.doc_fns.text_content_loader import TextContentLoader
        """åŠ è½½è®ºæ–‡å†…å®¹"""
        yield from update_ui(chatbot=self.chatbot, history=self.history)

        # ä¿å­˜è®ºæ–‡æ–‡ä»¶è·¯å¾„
        self.paper_file_path = paper_path

        # ä½¿ç”¨TextContentLoaderè¯»å–æ–‡ä»¶
        loader = TextContentLoader(self.chatbot, self.history)

        yield from loader.execute_single_file(paper_path)

        # è·å–åŠ è½½çš„å†…å®¹
        if len(self.history) >= 2 and self.history[-2]:
            self.paper_content = self.history[-2]
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return True
        else:
            self.chatbot.append(["é”™è¯¯", "æ— æ³•è¯»å–è®ºæ–‡å†…å®¹ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return False

    def _analyze_question(self, question: PaperQuestion) -> Generator:
        """åˆ†æå•ä¸ªé—®é¢˜ - ç›´æ¥æ˜¾ç¤ºé—®é¢˜å’Œç­”æ¡ˆ"""
        try:
            prompt = f"è¯·åŸºäºä»¥ä¸‹è®ºæ–‡å†…å®¹å›ç­”é—®é¢˜ï¼š\n\n{self.paper_content}\n\né—®é¢˜ï¼š{question.question}"

            response = yield from request_gpt_model_in_new_thread_with_ui_alive(
                inputs=prompt,
                inputs_show_user=question.question,
                llm_kwargs=self.llm_kwargs,
                chatbot=self.chatbot,
                history=[],
                sys_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘ç ”è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œéœ€è¦ä»”ç»†é˜…è¯»è®ºæ–‡å†…å®¹å¹¶å›ç­”é—®é¢˜ã€‚è¯·ä¿æŒå®¢è§‚ã€å‡†ç¡®ï¼Œå¹¶åŸºäºè®ºæ–‡å†…å®¹æä¾›æ·±å…¥åˆ†æã€‚"
            )

            if response:
                self.results[question.id] = response

                # å¦‚æœæ˜¯åˆ†ç±»å½’å±é—®é¢˜ï¼Œè‡ªåŠ¨æ›´æ–° paper.json
                if question.id == "category_assignment":
                    self._update_category_json(response)

                return True
            return False

        except Exception as e:
            self.chatbot.append(["é”™è¯¯", f"åˆ†æé—®é¢˜æ—¶å‡ºé”™: {str(e)}"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return False


    def _generate_summary(self) -> Generator:
        """ç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Š"""
        self.chatbot.append(["ç”ŸæˆæŠ¥å‘Š", "æ­£åœ¨æ•´åˆåˆ†æç»“æœï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š..."])
        yield from update_ui(chatbot=self.chatbot, history=self.history)

        summary_prompt = "è¯·åŸºäºä»¥ä¸‹å¯¹è®ºæ–‡çš„å„ä¸ªæ–¹é¢çš„åˆ†æï¼Œç”Ÿæˆä¸€ä»½å…¨é¢çš„è®ºæ–‡è§£è¯»æŠ¥å‘Šã€‚æŠ¥å‘Šåº”è¯¥ç®€æ˜æ‰¼è¦åœ°å‘ˆç°è®ºæ–‡çš„å…³é”®å†…å®¹ï¼Œå¹¶ä¿æŒé€»è¾‘è¿è´¯æ€§ã€‚"

        for q in self.questions:
            if q.id in self.results:
                summary_prompt += f"\n\nå…³äº{q.description}çš„åˆ†æ:\n{self.results[q.id]}"

        try:
            # ä½¿ç”¨å•çº¿ç¨‹ç‰ˆæœ¬çš„è¯·æ±‚å‡½æ•°ï¼Œå¯ä»¥åœ¨å‰ç«¯å®æ—¶æ˜¾ç¤ºç”Ÿæˆç»“æœ
            response = yield from request_gpt_model_in_new_thread_with_ui_alive(
                inputs=summary_prompt,
                inputs_show_user="ç”Ÿæˆè®ºæ–‡è§£è¯»æŠ¥å‘Š",
                llm_kwargs=self.llm_kwargs,
                chatbot=self.chatbot,
                history=[],
                sys_prompt="ä½ æ˜¯ä¸€ä¸ªç§‘ç ”è®ºæ–‡è§£è¯»ä¸“å®¶ï¼Œè¯·å°†å¤šä¸ªæ–¹é¢çš„åˆ†ææ•´åˆä¸ºä¸€ä»½å®Œæ•´ã€è¿è´¯ã€æœ‰æ¡ç†çš„æŠ¥å‘Šã€‚æŠ¥å‘Šåº”å½“é‡ç‚¹çªå‡ºï¼Œå±‚æ¬¡åˆ†æ˜ï¼Œå¹¶ä¸”ä¿æŒå­¦æœ¯æ€§å’Œå®¢è§‚æ€§ã€‚è‹¥åˆ†æä¸­åŒ…å« Mermaid ä»£ç å—ï¼ˆ```mermaid ...```ï¼‰ï¼Œè¯·åŸæ ·ä¿ç•™ï¼Œä¸è¦æ”¹å†™ä¸ºå…¶ä»–æ ¼å¼ã€‚"
            )

            if response:
                return response
            return "æŠ¥å‘Šç”Ÿæˆå¤±è´¥"

        except Exception as e:
            self.chatbot.append(["é”™è¯¯", f"ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return "æŠ¥å‘Šç”Ÿæˆå¤±è´¥: " + str(e)

    def save_report(self, report: str, paper_file_path: str = None) -> str:
        """ä¿å­˜åˆ†ææŠ¥å‘Šï¼Œè¿”å›ä¿å­˜çš„æ–‡ä»¶è·¯å¾„"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # è·å–PDFæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        pdf_filename = "æœªçŸ¥è®ºæ–‡"
        if paper_file_path and os.path.exists(paper_file_path):
            pdf_filename = os.path.splitext(os.path.basename(paper_file_path))[0]
            # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸­æ–‡å’Œä¸‹åˆ’çº¿
            import re
            pdf_filename = re.sub(r'[^\w\u4e00-\u9fff]', '_', pdf_filename)
            # å¦‚æœæ–‡ä»¶åè¿‡é•¿ï¼Œæˆªå–å‰50ä¸ªå­—ç¬¦
            if len(pdf_filename) > 50:
                pdf_filename = pdf_filename[:50]

        # ä¿å­˜ä¸ºMarkdownæ–‡ä»¶
        try:
            md_parts = []
            # æ ‡é¢˜ä¸æ•´ä½“æŠ¥å‘Šï¼ˆç¨ååœ¨å‰åŠ å…¥ YAML å¤´ï¼‰
            md_parts.append(f"è®ºæ–‡å¿«é€Ÿè§£è¯»æŠ¥å‘Š\n\n{report}")

            # ä¼˜å…ˆå†™å…¥ï¼šPPT æç®€æ‘˜è¦ï¼ˆè‹¥æœ‰ï¼‰
            if "core_idea_ppt_md" in self.results:
                md_parts.append(f"\n\n## PPT æ‘˜è¦\n\n{self.results['core_idea_ppt_md']}")

            # å…¶æ¬¡å†™å…¥ï¼šæ ¸å¿ƒæµç¨‹å›¾ï¼ˆMermaidï¼‰ï¼ˆè‹¥æœ‰ï¼Œä¿æŒä»£ç å—åŸæ ·ï¼‰
            if "core_algorithm_flowcharts" in self.results:
                md_parts.append(f"\n\n## æ ¸å¿ƒæµç¨‹å›¾\n\n{self.results['core_algorithm_flowcharts']}")

            # å…¶ä½™åˆ†æé¡¹æŒ‰é—®é¢˜åˆ—è¡¨é¡ºåºå†™å…¥ï¼Œä½†è·³è¿‡å·²å†™å…¥çš„ä¸¤ä¸ª
            for q in self.questions:
                if q.id in self.results and q.id not in {"core_idea_ppt_md", "core_algorithm_flowcharts"}:
                    md_parts.append(f"\n\n## {q.description}\n\n{self.results[q.id]}")

            md_content = "".join(md_parts)

            # è‹¥å·²ç”Ÿæˆ YAML å¤´ï¼Œåˆ™ç½®äºæ–‡é¦–
            if hasattr(self, 'yaml_header') and self.yaml_header:
                md_content = f"{self.yaml_header}\n\n" + md_content

            result_file = write_history_to_file(
                history=[md_content],
                file_basename=f"{timestamp}_{pdf_filename}_è§£è¯»æŠ¥å‘Š.md"
            )

            if result_file and os.path.exists(result_file):
                promote_file_to_downloadzone(result_file, chatbot=self.chatbot)
                return result_file
            else:
                return None
        except Exception as e:
            self.chatbot.append(["è­¦å‘Š", f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}"])
            update_ui(chatbot=self.chatbot, history=self.history)
            return None

    def analyze_paper(self, paper_path: str) -> Generator:
        """åˆ†æå•ç¯‡è®ºæ–‡ä¸»æµç¨‹"""
        # åŠ è½½è®ºæ–‡
        success = yield from self._load_paper(paper_path)
        if not success:
            return None

        # åˆ†æå…³é”®é—®é¢˜ - ç›´æ¥è¯¢é—®æ¯ä¸ªé—®é¢˜ï¼Œä¸æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
        for question in self.questions:
            yield from self._analyze_question(question)

        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        final_report = yield from self._generate_summary()

        # ç”Ÿæˆ YAML å¤´
        self.yaml_header = yield from self._generate_yaml_header()

        # ä¿å­˜æŠ¥å‘Š
        saved_file = self.save_report(final_report, self.paper_file_path)
        
        return saved_file


def _find_paper_files(path: str) -> List[str]:
    """æŸ¥æ‰¾è·¯å¾„ä¸­çš„æ‰€æœ‰è®ºæ–‡æ–‡ä»¶"""
    paper_files = []
    
    if os.path.isfile(path):
        # å¦‚æœæ˜¯å•ä¸ªæ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„æ ¼å¼
        file_ext = os.path.splitext(path)[1].lower()
        if file_ext in ['.pdf', '.docx', '.doc', '.txt', '.md', '.tex']:
            paper_files.append(path)
        return paper_files

    # å¦‚æœæ˜¯ç›®å½•ï¼Œé€’å½’æœç´¢æ‰€æœ‰æ”¯æŒçš„è®ºæ–‡æ–‡ä»¶
    if os.path.isdir(path):
        supported_extensions = ['.pdf', '.docx', '.doc', '.txt', '.md', '.tex']
        
        for root, dirs, files in os.walk(path):
            for file in files:
                file_path = os.path.join(root, file)
                file_ext = os.path.splitext(file)[1].lower()
                if file_ext in supported_extensions:
                    paper_files.append(file_path)
    
    return paper_files


def download_paper_by_id(paper_info, chatbot, history) -> str:
    """ä¸‹è½½è®ºæ–‡å¹¶è¿”å›ä¿å­˜è·¯å¾„"""
    from crazy_functions.review_fns.data_sources.scihub_source import SciHub
    id_type, paper_id = paper_info

    # åˆ›å»ºä¿å­˜ç›®å½• - ä½¿ç”¨æ—¶é—´æˆ³åˆ›å»ºå”¯ä¸€æ–‡ä»¶å¤¹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    user_name = chatbot.get_user() if hasattr(chatbot, 'get_user') else "default"
    from toolbox import get_log_folder, get_user
    base_save_dir = get_log_folder(get_user(chatbot), plugin_name='paper_download')
    save_dir = os.path.join(base_save_dir, f"papers_{timestamp}")
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    save_path = Path(save_dir)

    chatbot.append([f"ä¸‹è½½è®ºæ–‡", f"æ­£åœ¨ä¸‹è½½{'arXiv' if id_type == 'arxiv' else 'DOI'} {paper_id} çš„è®ºæ–‡..."])
    update_ui(chatbot=chatbot, history=history)

    pdf_path = None

    try:
        if id_type == 'arxiv':
            # ä½¿ç”¨æ”¹è¿›çš„arxivæŸ¥è¯¢æ–¹æ³•
            formatted_id = format_arxiv_id(paper_id)
            paper_result = get_arxiv_paper(formatted_id)

            if not paper_result:
                chatbot.append([f"ä¸‹è½½å¤±è´¥", f"æœªæ‰¾åˆ°arXivè®ºæ–‡: {paper_id}"])
                update_ui(chatbot=chatbot, history=history)
                return None

            # ä¸‹è½½PDF
            filename = f"arxiv_{paper_id.replace('/', '_')}.pdf"
            pdf_path = str(save_path / filename)
            paper_result.download_pdf(filename=pdf_path)

        else:  # doi
            # ä¸‹è½½DOI
            sci_hub = SciHub(
                doi=paper_id,
                path=save_path
            )
            pdf_path = sci_hub.fetch()

        # æ£€æŸ¥ä¸‹è½½ç»“æœ
        if pdf_path and os.path.exists(pdf_path):
            promote_file_to_downloadzone(pdf_path, chatbot=chatbot)
            chatbot.append([f"ä¸‹è½½æˆåŠŸ", f"å·²æˆåŠŸä¸‹è½½è®ºæ–‡: {os.path.basename(pdf_path)}"])
            update_ui(chatbot=chatbot, history=history)
            return pdf_path
        else:
            chatbot.append([f"ä¸‹è½½å¤±è´¥", f"è®ºæ–‡ä¸‹è½½å¤±è´¥: {paper_id}"])
            update_ui(chatbot=chatbot, history=history)
            return None

    except Exception as e:
        chatbot.append([f"ä¸‹è½½é”™è¯¯", f"ä¸‹è½½è®ºæ–‡æ—¶å‡ºé”™: {str(e)}"])
        update_ui(chatbot=chatbot, history=history)
        return None


@CatchException
def æ‰¹é‡è®ºæ–‡é€Ÿè¯»(txt: str, llm_kwargs: Dict, plugin_kwargs: Dict, chatbot: List,
             history: List, system_prompt: str, user_request: str):
    """ä¸»å‡½æ•° - æ‰¹é‡è®ºæ–‡é€Ÿè¯»"""
    # åˆå§‹åŒ–åˆ†æå™¨
    chatbot.append(["å‡½æ•°æ’ä»¶åŠŸèƒ½åŠä½¿ç”¨æ–¹å¼", "æ‰¹é‡è®ºæ–‡é€Ÿè¯»ï¼šæ‰¹é‡åˆ†æå¤šä¸ªè®ºæ–‡æ–‡ä»¶ï¼Œä¸ºæ¯ç¯‡è®ºæ–‡ç”Ÿæˆç‹¬ç«‹çš„é€Ÿè¯»æŠ¥å‘Šï¼Œé€‚ç”¨äºå¤§é‡è®ºæ–‡çš„å¿«é€Ÿç†è§£ã€‚ <br><br>ğŸ“‹ ä½¿ç”¨æ–¹å¼ï¼š<br>1ã€è¾“å…¥åŒ…å«å¤šä¸ªPDFæ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„<br>2ã€æˆ–è€…è¾“å…¥å¤šä¸ªè®ºæ–‡IDï¼ˆDOIæˆ–arXiv IDï¼‰ï¼Œç”¨é€—å·åˆ†éš”<br>3ã€ç‚¹å‡»æ’ä»¶å¼€å§‹æ‰¹é‡åˆ†æ"])
    yield from update_ui(chatbot=chatbot, history=history)

    paper_files = []

    # æ£€æŸ¥è¾“å…¥æ˜¯å¦åŒ…å«è®ºæ–‡IDï¼ˆå¤šä¸ªIDç”¨é€—å·åˆ†éš”ï¼‰
    if ',' in txt:
        # å¤„ç†å¤šä¸ªè®ºæ–‡ID
        paper_ids = [id.strip() for id in txt.split(',') if id.strip()]
        chatbot.append(["æ£€æµ‹åˆ°å¤šä¸ªè®ºæ–‡ID", f"æ£€æµ‹åˆ° {len(paper_ids)} ä¸ªè®ºæ–‡IDï¼Œå‡†å¤‡æ‰¹é‡ä¸‹è½½..."])
        yield from update_ui(chatbot=chatbot, history=history)

        for i, paper_id in enumerate(paper_ids):
            paper_info = extract_paper_id(paper_id)
            if paper_info:
                chatbot.append([f"ä¸‹è½½è®ºæ–‡ {i+1}/{len(paper_ids)}", f"æ­£åœ¨ä¸‹è½½ {'arXiv' if paper_info[0] == 'arxiv' else 'DOI'} ID: {paper_info[1]}..."])
                yield from update_ui(chatbot=chatbot, history=history)

                paper_file = download_paper_by_id(paper_info, chatbot, history)
                if paper_file:
                    paper_files.append(paper_file)
                else:
                    chatbot.append([f"ä¸‹è½½å¤±è´¥", f"æ— æ³•ä¸‹è½½è®ºæ–‡: {paper_id}"])
                    yield from update_ui(chatbot=chatbot, history=history)
            else:
                chatbot.append([f"IDæ ¼å¼é”™è¯¯", f"æ— æ³•è¯†åˆ«è®ºæ–‡IDæ ¼å¼: {paper_id}"])
                yield from update_ui(chatbot=chatbot, history=history)
    else:
        # æ£€æŸ¥å•ä¸ªè®ºæ–‡ID
        paper_info = extract_paper_id(txt)
        if paper_info:
            # å•ä¸ªè®ºæ–‡ID
            chatbot.append(["æ£€æµ‹åˆ°è®ºæ–‡ID", f"æ£€æµ‹åˆ°{'arXiv' if paper_info[0] == 'arxiv' else 'DOI'} ID: {paper_info[1]}ï¼Œå‡†å¤‡ä¸‹è½½è®ºæ–‡..."])
            yield from update_ui(chatbot=chatbot, history=history)

            paper_file = download_paper_by_id(paper_info, chatbot, history)
            if paper_file:
                paper_files.append(paper_file)
            else:
                report_exception(chatbot, history, a=f"ä¸‹è½½è®ºæ–‡å¤±è´¥", b=f"æ— æ³•ä¸‹è½½{'arXiv' if paper_info[0] == 'arxiv' else 'DOI'}è®ºæ–‡: {paper_info[1]}")
                yield from update_ui(chatbot=chatbot, history=history)
                return
        else:
            # æ£€æŸ¥è¾“å…¥è·¯å¾„
            if not os.path.exists(txt):
                report_exception(chatbot, history, a=f"æ‰¹é‡è§£æè®ºæ–‡: {txt}", b=f"æ‰¾ä¸åˆ°æ–‡ä»¶æˆ–æ— æƒè®¿é—®: {txt}")
                yield from update_ui(chatbot=chatbot, history=history)
                return

            # éªŒè¯è·¯å¾„å®‰å…¨æ€§
            user_name = chatbot.get_user()
            validate_path_safety(txt, user_name)

            # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡æ–‡ä»¶
            paper_files = _find_paper_files(txt)

            if not paper_files:
                report_exception(chatbot, history, a=f"æ‰¹é‡è§£æè®ºæ–‡", b=f"åœ¨è·¯å¾„ {txt} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„è®ºæ–‡æ–‡ä»¶")
                yield from update_ui(chatbot=chatbot, history=history)
                return

    yield from update_ui(chatbot=chatbot, history=history)

    # å¼€å§‹æ‰¹é‡åˆ†æ
    if not paper_files:
        chatbot.append(["é”™è¯¯", "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯åˆ†æçš„è®ºæ–‡æ–‡ä»¶"])
        yield from update_ui(chatbot=chatbot, history=history)
        return

    chatbot.append(["å¼€å§‹æ‰¹é‡åˆ†æ", f"æ‰¾åˆ° {len(paper_files)} ç¯‡è®ºæ–‡ï¼Œå¼€å§‹æ‰¹é‡åˆ†æ..."])
    yield from update_ui(chatbot=chatbot, history=history)

    # åˆ›å»ºæ‰¹é‡åˆ†æå™¨
    analyzer = BatchPaperAnalyzer(llm_kwargs, plugin_kwargs, chatbot, history, system_prompt)
    
    # æ‰¹é‡åˆ†ææ¯ç¯‡è®ºæ–‡
    successful_reports = []
    failed_papers = []
    
    for i, paper_file in enumerate(paper_files):
        try:
            chatbot.append([f"åˆ†æè®ºæ–‡ {i+1}/{len(paper_files)}", f"æ­£åœ¨åˆ†æ: {os.path.basename(paper_file)}"])
            yield from update_ui(chatbot=chatbot, history=history)
            
            # åˆ†æå•ç¯‡è®ºæ–‡
            saved_file = yield from analyzer.analyze_paper(paper_file)
            
            if saved_file:
                successful_reports.append((os.path.basename(paper_file), saved_file))
                chatbot.append([f"å®Œæˆè®ºæ–‡ {i+1}/{len(paper_files)}", f"æˆåŠŸåˆ†æå¹¶ä¿å­˜æŠ¥å‘Š: {os.path.basename(saved_file)}"])
            else:
                failed_papers.append(os.path.basename(paper_file))
                chatbot.append([f"å¤±è´¥è®ºæ–‡ {i+1}/{len(paper_files)}", f"åˆ†æå¤±è´¥: {os.path.basename(paper_file)}"])
            
            yield from update_ui(chatbot=chatbot, history=history)
            
        except Exception as e:
            failed_papers.append(os.path.basename(paper_file))
            chatbot.append([f"é”™è¯¯è®ºæ–‡ {i+1}/{len(paper_files)}", f"åˆ†æå‡ºé”™: {os.path.basename(paper_file)} - {str(e)}"])
            yield from update_ui(chatbot=chatbot, history=history)

    # ç”Ÿæˆæ‰¹é‡åˆ†ææ€»ç»“
    summary = f"æ‰¹é‡åˆ†æå®Œæˆï¼\n\n"
    summary += f"ğŸ“Š åˆ†æç»Ÿè®¡ï¼š\n"
    summary += f"- æ€»è®ºæ–‡æ•°ï¼š{len(paper_files)}\n"
    summary += f"- æˆåŠŸåˆ†æï¼š{len(successful_reports)}\n"
    summary += f"- åˆ†æå¤±è´¥ï¼š{len(failed_papers)}\n\n"
    
    if successful_reports:
        summary += f"âœ… æˆåŠŸç”ŸæˆæŠ¥å‘Šï¼š\n"
        for paper_name, report_path in successful_reports:
            summary += f"- {paper_name} â†’ {os.path.basename(report_path)}\n"
    
    if failed_papers:
        summary += f"\nâŒ åˆ†æå¤±è´¥çš„è®ºæ–‡ï¼š\n"
        for paper_name in failed_papers:
            summary += f"- {paper_name}\n"

    chatbot.append(["æ‰¹é‡åˆ†æå®Œæˆ", summary])
    yield from update_ui(chatbot=chatbot, history=history) 