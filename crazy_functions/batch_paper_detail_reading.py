import os
import re
import time
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass
from typing import Dict, List, Generator
from crazy_functions.Batch_Paper_Reading import estimate_token_usage
from crazy_functions.crazy_utils import request_gpt_model_in_new_thread_with_ui_alive
from toolbox import update_ui, promote_file_to_downloadzone, write_history_to_file, CatchException, report_exception
from shared_utils.fastapi_server import validate_path_safety
from crazy_functions.paper_fns.paper_download import extract_paper_id, get_arxiv_paper, format_arxiv_id


@dataclass
class DeepReadQuestion:
    """è®ºæ–‡ç²¾è¯»é—®é¢˜é¡¹"""
    id: str
    question: str
    importance: int
    description: str


class BatchPaperDetailAnalyzer:
    """æ‰¹é‡è®ºæ–‡ç²¾è¯»åˆ†æå™¨"""

    def __init__(self, llm_kwargs: Dict, plugin_kwargs: Dict, chatbot: List, history: List, system_prompt: str):
        self.llm_kwargs = llm_kwargs
        self.plugin_kwargs = plugin_kwargs
        self.chatbot = chatbot
        self.history = history
        self.system_prompt = system_prompt
        self.paper_content = ""
        self.results: Dict[str, str] = {}
        self.paper_file_path: str = None
        self.secondary_category: str = None
        self.context_history: List[str] = []  # ä¸LLMå…±äº«çš„ä¸Šä¸‹æ–‡ï¼ˆæ¯ç¯‡è®ºæ–‡æ³¨å…¥ä¸€æ¬¡å…¨æ–‡ï¼‰
        # ç»Ÿè®¡ç”¨ï¼šè®°å½•æ¯æ¬¡LLMäº¤äº’çš„è¾“å…¥ä¸è¾“å‡º
        self._token_inputs: List[str] = []
        self._token_outputs: List[str] = []

        # ç²¾è¯»ç»´åº¦ï¼ˆé€’è¿›å¼æ·±å…¥åˆ†æï¼Œä»å®è§‚åˆ°å¾®è§‚ï¼Œä»ç†è®ºåˆ°å®è·µï¼‰
        self.questions: List[DeepReadQuestion] = [
            # ç¬¬ä¸€å±‚ï¼šé—®é¢˜åŸŸä¸åŠ¨æœºåˆ†æ
            DeepReadQuestion(
                id="problem_domain_and_motivation",
                description="é—®é¢˜åŸŸä¸åŠ¨æœºåˆ†æ",
                importance=5,
                question=(
                    "ã€ç¬¬ä¸€å±‚ï¼šé—®é¢˜åŸŸç†è§£ã€‘\n"
                    "è¯·æ·±å…¥åˆ†æè®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯ä¸åŠ¨æœºï¼š\n"
                    "1) è®ºæ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿè¯¥é—®é¢˜åœ¨é¢†åŸŸä¸­çš„é‡è¦æ€§å¦‚ä½•ï¼Ÿ\n"
                    "2) ç°æœ‰æ–¹æ³•å­˜åœ¨å“ªäº›æ ¹æœ¬æ€§ç¼ºé™·æˆ–å±€é™æ€§ï¼Ÿ\n"
                    "3) è®ºæ–‡æå‡ºçš„è§£å†³æ€è·¯çš„ç‹¬ç‰¹æ€§å’Œåˆ›æ–°æ€§ä½“ç°åœ¨å“ªé‡Œï¼Ÿ\n"
                    "4) è¯¥ç ”ç©¶å¯¹ç†è®ºå‘å±•æˆ–å®é™…åº”ç”¨çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ"
                ),
            ),
            
            # ç¬¬äºŒå±‚ï¼šç†è®ºæ¡†æ¶ä¸æ ¸å¿ƒè´¡çŒ®
            DeepReadQuestion(
                id="theoretical_framework_and_contributions",
                description="ç†è®ºæ¡†æ¶ä¸æ ¸å¿ƒè´¡çŒ®",
                importance=5,
                question=(
                    "ã€ç¬¬äºŒå±‚ï¼šç†è®ºæ„å»ºã€‘\n"
                    "åŸºäºå‰é¢å¯¹é—®é¢˜åŸŸçš„ç†è§£ï¼Œè¯·æ·±å…¥åˆ†æè®ºæ–‡çš„ç†è®ºæ¡†æ¶ï¼š\n"
                    "1) è®ºæ–‡å»ºç«‹äº†ä»€ä¹ˆæ ·çš„ç†è®ºæ¡†æ¶æˆ–æ•°å­¦æ¨¡å‹ï¼Ÿ\n"
                    "2) æ ¸å¿ƒè´¡çŒ®æœ‰å“ªäº›ï¼Ÿè¯·æŒ‰ç†è®ºé‡è¦æ€§æ’åºå¹¶è¯´æ˜æ¯ä¸ªè´¡çŒ®çš„ç‹¬ç‰¹ä»·å€¼\n"
                    "3) è¿™äº›è´¡çŒ®å¦‚ä½•è§£å†³ç¬¬ä¸€å±‚ä¸­è¯†åˆ«çš„ç°æœ‰æ–¹æ³•ç¼ºé™·ï¼Ÿ\n"
                    "4) ç†è®ºæ¡†æ¶çš„é€‚ç”¨èŒƒå›´å’Œè¾¹ç•Œæ¡ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ"
                ),
            ),
            
            # ç¬¬ä¸‰å±‚ï¼šæ–¹æ³•è®¾è®¡ä¸æŠ€æœ¯ç»†èŠ‚
            DeepReadQuestion(
                id="method_design_and_technical_details",
                description="æ–¹æ³•è®¾è®¡ä¸æŠ€æœ¯ç»†èŠ‚",
                importance=5,
                question=(
                    "ã€ç¬¬ä¸‰å±‚ï¼šæŠ€æœ¯å®ç°ã€‘\n"
                    "åŸºäºå‰é¢çš„ç†è®ºæ¡†æ¶ï¼Œè¯·æ·±å…¥åˆ†æå…·ä½“çš„æŠ€æœ¯å®ç°ï¼š\n"
                    "1) æ ¸å¿ƒç®—æ³•çš„è®¾è®¡æ€è·¯å’Œå…³é”®æ­¥éª¤æ˜¯ä»€ä¹ˆï¼Ÿ\n"
                    "2) å…³é”®ç¬¦å·å®šä¹‰ã€æŸå¤±å‡½æ•°/ç›®æ ‡å‡½æ•°ã€ä»¥åŠä¸»è¦å®šç†/å¼•ç†çš„æ¨å¯¼è¿‡ç¨‹\n"
                    "3) ç®—æ³•çš„æ—¶é—´å¤æ‚åº¦ã€ç©ºé—´å¤æ‚åº¦ä»¥åŠæ”¶æ•›æ€§åˆ†æ\n"
                    "4) å®ç°ä¸­çš„å…³é”®æŠ€æœ¯éš¾ç‚¹å’Œè§£å†³æ–¹æ¡ˆ\n"
                    "5) ä¸ç°æœ‰æ–¹æ³•åœ¨æŠ€æœ¯å±‚é¢çš„æœ¬è´¨åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"
                ),
            ),
            
            # ç¬¬å››å±‚ï¼šå®éªŒéªŒè¯ä¸æœ‰æ•ˆæ€§åˆ†æ
            DeepReadQuestion(
                id="experimental_validation_and_effectiveness",
                description="å®éªŒéªŒè¯ä¸æœ‰æ•ˆæ€§åˆ†æ",
                importance=5,
                question=(
                    "ã€ç¬¬å››å±‚ï¼šå®éªŒéªŒè¯ã€‘\n"
                    "åŸºäºå‰é¢çš„æŠ€æœ¯è®¾è®¡ï¼Œè¯·åˆ†æå®éªŒå¦‚ä½•éªŒè¯æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼š\n"
                    "1) å®éªŒè®¾è®¡å¦‚ä½•éªŒè¯å‰é¢æå‡ºçš„ç†è®ºè´¡çŒ®ï¼Ÿ\n"
                    "2) æ•°æ®é›†é€‰æ‹©ã€è¯„ä¼°æŒ‡æ ‡å’Œå¯¹æ¯”æ–¹æ³•çš„åˆç†æ€§åˆ†æ\n"
                    "3) ä¸»è¦å®éªŒç»“æœæ˜¯å¦æ”¯æŒè®ºæ–‡çš„æ ¸å¿ƒä¸»å¼ ï¼Ÿ\n"
                    "4) æ¶ˆèå®éªŒæ­ç¤ºäº†å“ªäº›å…³é”®å› ç´ å’Œäº¤äº’æ•ˆåº”ï¼Ÿ\n"
                    "5) å®éªŒç»“æœçš„ç»Ÿè®¡æ˜¾è‘—æ€§å’Œå¯é‡å¤æ€§å¦‚ä½•ï¼Ÿ"
                ),
            ),
            
            # ç¬¬äº”å±‚ï¼šå‡è®¾æ¡ä»¶ä¸å±€é™æ€§åˆ†æ
            DeepReadQuestion(
                id="assumptions_limitations_and_threats",
                description="å‡è®¾æ¡ä»¶ä¸å±€é™æ€§åˆ†æ",
                importance=4,
                question=(
                    "ã€ç¬¬äº”å±‚ï¼šæ‰¹åˆ¤æ€§åˆ†æã€‘\n"
                    "åŸºäºå‰é¢çš„å…¨é¢åˆ†æï¼Œè¯·è¿›è¡Œæ‰¹åˆ¤æ€§æ€è€ƒï¼š\n"
                    "1) è®ºæ–‡çš„æ˜¾å¼å’Œéšå¼å‡è®¾æœ‰å“ªäº›ï¼Ÿè¿™äº›å‡è®¾çš„åˆç†æ€§å¦‚ä½•ï¼Ÿ\n"
                    "2) åœ¨ä»€ä¹ˆæ¡ä»¶ä¸‹æ–¹æ³•å¯èƒ½å¤±æ•ˆï¼Ÿç°å®åº”ç”¨ä¸­çš„æ½œåœ¨é£é™©æ˜¯ä»€ä¹ˆï¼Ÿ\n"
                    "3) å®éªŒè®¾è®¡çš„å±€é™æ€§å’Œå¯èƒ½çš„è¯¯å¯¼æ€§ç»“è®º\n"
                    "4) ä½œè€…æœªå……åˆ†è®¨è®ºä½†å¯èƒ½å½±å“æ–¹æ³•æœ‰æ•ˆæ€§çš„å› ç´ \n"
                    "5) æ–¹æ³•çš„å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›å¦‚ä½•ï¼Ÿ"
                ),
            ),
            
            # ç¬¬å…­å±‚ï¼šå¤ç°æŒ‡å—ä¸å·¥ç¨‹å®ç°
            DeepReadQuestion(
                id="reproduction_guide_and_engineering",
                description="å¤ç°æŒ‡å—ä¸å·¥ç¨‹å®ç°",
                importance=5,
                question=(
                    "ã€ç¬¬å…­å±‚ï¼šå·¥ç¨‹å¤ç°ã€‘\n"
                    "åŸºäºå‰é¢çš„æŠ€æœ¯åˆ†æï¼Œè¯·æä¾›å¤ç°æŒ‡å¯¼ï¼š\n"
                    "1) å¤ç°æ‰€éœ€çš„æ•°æ®é›†ã€é¢„è®­ç»ƒæ¨¡å‹å’Œä¾èµ–èµ„æº\n"
                    "2) å…³é”®è¶…å‚æ•°åŠå…¶è°ƒä¼˜ç­–ç•¥ï¼ˆä¸æ¶‰åŠå…·ä½“æ•°å€¼ï¼‰\n"
                    "3) è®­ç»ƒå’Œè¯„ä¼°æµç¨‹çš„å…³é”®æ­¥éª¤\n"
                    "4) ç¡¬ä»¶èµ„æºéœ€æ±‚ï¼ˆGPU/CPU/å†…å­˜/å­˜å‚¨ï¼‰å’Œæ—¶é—´æˆæœ¬ä¼°ç®—\n"
                    "5) å¯èƒ½é‡åˆ°çš„å®ç°éš¾ç‚¹å’Œè§£å†³æ–¹æ¡ˆ\n"
                    "6) å¼€æºä»£ç çš„å¯ç”¨æ€§å’Œè®¸å¯è¯æƒ…å†µ"
                ),
            ),
            
            # ç¬¬ä¸ƒå±‚ï¼šæµç¨‹å›¾ä¸æ¶æ„è®¾è®¡
            DeepReadQuestion(
                id="flowcharts_and_architecture",
                description="æµç¨‹å›¾ä¸æ¶æ„è®¾è®¡",
                importance=4,
                question=(
                    "ã€ç¬¬ä¸ƒå±‚ï¼šæ¶æ„å¯è§†åŒ–ã€‘\n"
                    "åŸºäºå‰é¢çš„æŠ€æœ¯åˆ†æï¼Œè¯·ç»˜åˆ¶æ ¸å¿ƒæµç¨‹å›¾ï¼š\n"
                    "è¦æ±‚ï¼š\n"
                    "1) æ¯ä¸ªæµç¨‹å›¾ä½¿ç”¨ Mermaid è¯­æ³•ï¼Œä»£ç å—éœ€ä»¥ ```mermaid å¼€å§‹ï¼Œä»¥ ``` ç»“æŸ\n"
                    "2) æ¨èä½¿ç”¨ flowchart TD æˆ– LRï¼ŒèŠ‚ç‚¹éœ€æ¦‚æ‹¬å…³é”®æ­¥éª¤/å­æ¨¡å—\n"
                    "3) æ¯ä¸ªæµç¨‹å›¾å‰ä»¥ä¸€å¥è¯æ ‡æ˜æ¨¡å—/é˜¶æ®µåç§°\n"
                    "4) æ ¼å¼çº¦æŸï¼š\n"
                    "   - èŠ‚ç‚¹åç”¨å¼•å·åŒ…è£¹ï¼Œå¦‚ [\"èŠ‚ç‚¹å\"] æˆ– (\"èŠ‚ç‚¹å\")\n"
                    "   - ç®­å¤´æ ‡ç­¾é‡‡ç”¨ |\"æ ‡ç­¾å\"| å½¢å¼\n"
                    "5) é‡ç‚¹å±•ç¤ºï¼šæ•´ä½“æ¶æ„ã€æ ¸å¿ƒç®—æ³•æµç¨‹ã€æ•°æ®æµå‘ã€å…³é”®å†³ç­–ç‚¹"
                ),
            ),
            
            # ç¬¬å…«å±‚ï¼šå½±å“è¯„ä¼°ä¸æœªæ¥å±•æœ›
            DeepReadQuestion(
                id="impact_assessment_and_future_directions",
                description="å½±å“è¯„ä¼°ä¸æœªæ¥å±•æœ›",
                importance=3,
                question=(
                    "ã€ç¬¬å…«å±‚ï¼šå½±å“ä¸å±•æœ›ã€‘\n"
                    "åŸºäºå‰é¢çš„å…¨é¢åˆ†æï¼Œè¯·è¯„ä¼°ç ”ç©¶çš„å½±å“å’Œå‰æ™¯ï¼š\n"
                    "1) è¯¥ç ”ç©¶å¯¹å­¦æœ¯é¢†åŸŸçš„çŸ­æœŸå’Œé•¿æœŸå½±å“\n"
                    "2) æ½œåœ¨çš„äº§ä¸šåº”ç”¨ä»·å€¼å’Œå•†ä¸šåŒ–å‰æ™¯\n"
                    "3) å¯èƒ½å¼•å‘çš„åç»­ç ”ç©¶æ–¹å‘\n"
                    "4) å­˜åœ¨çš„ä¼¦ç†é—®é¢˜æˆ–ç¤¾ä¼šå½±å“\n"
                    "5) æ”¹è¿›å’Œæ‰©å±•çš„å…·ä½“å»ºè®®"
                ),
            ),
            
            # ç¬¬ä¹å±‚ï¼šæ‰§è¡Œæ‘˜è¦ä¸è¦ç‚¹æ€»ç»“
            DeepReadQuestion(
                id="executive_summary_and_key_points",
                description="æ‰§è¡Œæ‘˜è¦ä¸è¦ç‚¹æ€»ç»“",
                importance=5,
                question=(
                    "ã€ç¬¬ä¹å±‚ï¼šè¦ç‚¹æ€»ç»“ã€‘\n"
                    "åŸºäºå‰é¢å…«å±‚çš„æ·±å…¥åˆ†æï¼Œè¯·ç»™å‡ºç²¾ç‚¼çš„æ‰§è¡Œæ‘˜è¦ï¼š\n"
                    "æ ¼å¼è¦æ±‚ï¼ˆMarkdownï¼Œä¸åŒ…å«ä»£ç ï¼‰ï¼š\n"
                    "## æ ¸å¿ƒä»·å€¼\n"
                    "- ä¸€å¥è¯æ¦‚æ‹¬æ–¹æ³•çš„æ ¸å¿ƒä»·å€¼\n"
                    "\n"
                    "## æŠ€æœ¯è¦ç‚¹\n"
                    "- 3-5æ¡å…³é”®æŠ€æœ¯è¦ç‚¹ï¼ˆè¾“å…¥/å¤„ç†/è¾“å‡ºï¼‰\n"
                    "\n"
                    "## å¤ç°è¦ç‚¹\n"
                    "- 3-5æ¡å¤ç°å…³é”®ä¿¡æ¯ï¼ˆæ•°æ®/å‚æ•°/èµ„æº/æ—¶é—´ï¼‰\n"
                    "\n"
                    "## é€‚ç”¨åœºæ™¯\n"
                    "- 2-3æ¡å…¸å‹åº”ç”¨åœºæ™¯\n"
                    "\n"
                    "## æ³¨æ„äº‹é¡¹\n"
                    "- 2-3æ¡é‡è¦é™åˆ¶æˆ–æ³¨æ„äº‹é¡¹"
                ),
            ),
        ]

        self.questions.sort(key=lambda q: q.importance, reverse=True)

    # ---------- å…³é”®è¯åº“å·¥å…·ï¼ˆä¸é€Ÿè¯»ç‰ˆä¸€è‡´ï¼‰ ----------
    def _get_keywords_db_path(self) -> str:
        return os.path.join(os.path.dirname(__file__), 'keywords.txt')

    def _load_keywords_db(self) -> List[str]:
        path = self._get_keywords_db_path()
        if os.path.exists(path):
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    return [line.strip() for line in f if line.strip()]
            except Exception:
                return []
        return []

    def _save_keywords_db(self, keywords: List[str]):
        path = self._get_keywords_db_path()
        try:
            with open(path, 'w', encoding='utf-8') as f:
                for kw in sorted(set(keywords), key=lambda x: x.lower()):
                    f.write(kw + '\n')
        except Exception:
            pass

    def _normalize_keyword(self, kw: str) -> str:
        kw = kw.strip()
        kw = re.sub(r'[\s\u3000]+', ' ', kw)
        kw = kw.strip().strip('.,;:')
        return kw.lower()

    def _find_similar_in_db(self, db: List[str], new_kw: str, threshold: float = 0.88) -> str:
        import difflib
        if not new_kw:
            return None
        candidates = difflib.get_close_matches(new_kw, [self._normalize_keyword(k) for k in db], n=1, cutoff=threshold)
        if candidates:
            norm = candidates[0]
            for k in db:
                if self._normalize_keyword(k) == norm:
                    return k
        return None

    def _merge_keywords_with_db(self, extracted_keywords: List[str]):
        db = self._load_keywords_db()
        canonical_list: List[str] = []
        for kw in extracted_keywords:
            clean = self._normalize_keyword(kw)
            if not clean:
                continue
            similar = self._find_similar_in_db(db, clean)
            if similar:
                if similar not in canonical_list:
                    canonical_list.append(similar)
            else:
                db.append(kw)
                if kw not in canonical_list:
                    canonical_list.append(kw)
        self._save_keywords_db(db)
        return canonical_list, db

    def _clean_yaml_list(self, yaml_text: str, list_fields: List[str]) -> str:
        """æ¸…ç†YAMLæ–‡æœ¬ä¸­åˆ—è¡¨å­—æ®µçš„Noneå€¼"""
        import re
        for field in list_fields:
            # åŒ¹é…åˆ—è¡¨å­—æ®µçš„æ¨¡å¼
            pattern = rf"^{field}:\s*\[(.*?)\]\s*$"
            match = re.search(pattern, yaml_text, flags=re.MULTILINE)
            if match:
                inner_content = match.group(1).strip()
                if inner_content:
                    # è§£æåˆ—è¡¨å†…å®¹ï¼Œè¿‡æ»¤æ‰Noneå€¼
                    items = [item.strip().strip('"\'') for item in inner_content.split(',')]
                    # è¿‡æ»¤æ‰Noneã€ç©ºå­—ç¬¦ä¸²å’Œ"None"
                    filtered_items = [item for item in items if item and item.lower() != 'none']
                    if filtered_items:
                        # é‡æ–°æ„å»ºåˆ—è¡¨ï¼Œä¿æŒå¼•å·æ ¼å¼
                        rebuilt = ', '.join([f'"{item}"' for item in filtered_items])
                        yaml_text = re.sub(pattern, f"{field}: [{rebuilt}]", yaml_text, flags=re.MULTILINE)
                    else:
                        # å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œç§»é™¤è¯¥å­—æ®µ
                        yaml_text = re.sub(rf"^{field}:\s*\[.*?\]\s*$\n?", "", yaml_text, flags=re.MULTILINE)
        return yaml_text

    def _generate_yaml_header(self) -> Generator:
        """åŸºäºè®ºæ–‡å†…å®¹ä¸å·²å¾—åˆ†æï¼Œç”Ÿæˆ YAML Front Matter"""
        try:
            prompt = (
                "è¯·åŸºäºä»¥ä¸‹è®ºæ–‡å†…å®¹ä¸åˆ†æè¦ç‚¹ï¼Œæå–è®ºæ–‡æ ¸å¿ƒå…ƒä¿¡æ¯å¹¶è¾“å‡º YAML Front Matterï¼š\n\n"
                f"è®ºæ–‡å…¨æ–‡å†…å®¹ç‰‡æ®µï¼š\n{self.paper_content}\n\n"
                "è‹¥æœ‰å¯ç”¨çš„åˆ†æè¦ç‚¹ï¼š\n"
            )
            for q in self.questions:
                if q.id in self.results:
                    prompt += f"- {q.description}: {self.results[q.id][:400]}\n"

            prompt += (
                "\nä¸¥æ ¼è¾“å‡º YAMLï¼ˆä¸ä½¿ç”¨ä»£ç å—å›´æ ï¼‰ï¼Œå­—æ®µå¦‚ä¸‹ï¼š\n"
                "title: åŸæ–‡æ ‡é¢˜ï¼ˆå°½é‡è‹±æ–‡åŸé¢˜,æ ‡é¢˜éœ€è¦æœ‰å¼•å·åŒ…è£¹ï¼‰\n"
                "title_zh: ä¸­æ–‡æ ‡é¢˜ï¼ˆè‹¥å¯ï¼‰\n"
                "authors: [ä½œè€…è‹±æ–‡ååˆ—è¡¨]\n"
                "affiliation_zh: ç¬¬ä¸€ä½œè€…å•ä½ï¼ˆä¸­æ–‡ï¼‰\n"
                "keywords: [è‹±æ–‡å…³é”®è¯åˆ—è¡¨]\n"
                "urls: [è®ºæ–‡é“¾æ¥, Githubé“¾æ¥æˆ–None]\n"
                "doi: [DOIé“¾æ¥, None]\n"
                "journal_or_conference: [æœŸåˆŠæˆ–ä¼šè®®åç§°, None]\n"
                "year: [å¹´ä»½, None]\n"
                "source_code: [æºç é“¾æ¥, None]\n"
                "read_status: [å·²é˜…è¯», æœªé˜…è¯»]\n"
                "stars: [â­â­â­â­â­, â­â­â­â­, â­â­â­, â­â­, â­]\n"
                "ä»…è¾“å‡ºä»¥ --- å¼€å§‹ã€ä»¥ --- ç»“æŸçš„ YAML Front Matterï¼Œä¸è¦é™„åŠ å…¶ä»–æ–‡æœ¬ã€‚é»˜è®¤starsä¸ºâ­â­â­ï¼Œread_statusä¸ºæœªé˜…è¯»ã€‚"
            )

            yaml_str = yield from request_gpt_model_in_new_thread_with_ui_alive(
                inputs=prompt,
                inputs_show_user="ç”Ÿæˆè®ºæ–‡æ ¸å¿ƒä¿¡æ¯ YAML å¤´",
                llm_kwargs=self.llm_kwargs,
                chatbot=self.chatbot,
                history=[],
                sys_prompt=(
                    "ä½ æ˜¯è®ºæ–‡ä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚è¯·ä»…è¾“å‡º YAML Front Matterï¼Œ"
                    "é”®åå›ºå®šä¸”é¡ºåºä¸é™ï¼Œæ³¨æ„ authors/keywords/urls åº”ä¸ºåˆ—è¡¨ã€‚"
                )
            )

            if isinstance(yaml_str, str) and yaml_str.strip().startswith("---") and yaml_str.strip().endswith("---"):
                text = yaml_str.strip()
                m = re.search(r"^keywords:\s*\[(.*?)\]\s*$", text, flags=re.MULTILINE)
                if m:
                    inner = m.group(1).strip()
                    raw_list = [x.strip().strip('\"\'\'') for x in inner.split(',') if x.strip()]
                    merged, _ = self._merge_keywords_with_db(raw_list)
                    rebuilt = ', '.join([f'\"{k}\"' for k in merged])
                    text = re.sub(r"^keywords:\s*\[(.*?)\]\s*$", f"keywords: [{rebuilt}]", text, flags=re.MULTILINE)
                # æ³¨å…¥â€œå½’å±â€äºŒçº§åˆ†ç±»åˆ° YAML å¤´ï¼ˆä»…å†™å…¥åˆ†ç±»è·¯å¾„æœ¬èº«ï¼Œå¹¶ç”¨å¼•å·åŒ…è£¹ï¼‰
                try:
                    if getattr(self, 'secondary_category', None):
                        escaped = self.secondary_category.replace('\"', '\\\"')
                        if text.endswith("---"):
                            text = text[:-3].rstrip() + f"\nsecondary_category: \"{escaped}\"\n---"
                except Exception:
                    pass
                # åŸºäº worth_reading_judgment æå–ä¸­æ–‡"è®ºæ–‡é‡è¦ç¨‹åº¦"å’Œ"æ˜¯å¦ç²¾è¯»"ï¼Œè‹¥ç¼ºå¤±å›é€€é»˜è®¤
                try:
                    level = None
                    reading_recommendation = None
                    try:
                        judge = self.results.get("worth_reading_judgment", "")
                        if isinstance(judge, str) and judge:
                            if "å¼ºçƒˆæ¨è" in judge:
                                level = "å¼ºçƒˆæ¨è"
                                reading_recommendation = "å¼ºçƒˆæ¨èç²¾è¯»"
                            elif "ä¸æ¨è" in judge:
                                level = "ä¸æ¨è"
                                reading_recommendation = "ä¸æ¨èç²¾è¯»"
                            elif "è°¨æ…" in judge:
                                level = "è°¨æ…"
                                reading_recommendation = "è°¨æ…ç²¾è¯»"
                            elif "ä¸€èˆ¬" in judge:
                                level = "ä¸€èˆ¬"
                                reading_recommendation = "ä¸€èˆ¬"
                            elif "æ¨è" in judge:
                                level = "æ¨è"
                                reading_recommendation = "æ¨èç²¾è¯»"
                    except Exception:
                        pass
                    if not level:
                        level = "ä¸€èˆ¬"
                    if not reading_recommendation:
                        # å…œåº•ï¼šæ ¹æ®é‡è¦ç¨‹åº¦æ¨æ–­æ˜¯å¦ç²¾è¯»
                        if level in ["å¼ºçƒˆæ¨è", "æ¨è"]:
                            reading_recommendation = "æ¨èç²¾è¯»"
                        elif level == "ä¸æ¨è":
                            reading_recommendation = "ä¸æ¨èç²¾è¯»"
                        else:
                            reading_recommendation = "ä¸€èˆ¬"
                    
                    if text.endswith("---"):
                        text = text[:-3].rstrip() + f"\nè®ºæ–‡é‡è¦ç¨‹åº¦: \"{level}\"\næ˜¯å¦ç²¾è¯»: \"{reading_recommendation}\"\n---"
                except Exception:
                    pass
                
                # æ¸…ç†åˆ—è¡¨å­—æ®µä¸­çš„Noneå€¼
                list_fields = ["urls", "doi", "journal_or_conference", "year", "source_code"]
                text = self._clean_yaml_list(text, list_fields)
                
                return text
            return None
        except Exception as e:
            self.chatbot.append(["è­¦å‘Š", f"ç”Ÿæˆ YAML å¤´å¤±è´¥: {str(e)}"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return None

    def _load_paper(self, paper_path: str) -> Generator:
        from crazy_functions.doc_fns.text_content_loader import TextContentLoader
        yield from update_ui(chatbot=self.chatbot, history=self.history)
        self.paper_file_path = paper_path
        loader = TextContentLoader(self.chatbot, self.history)
        yield from loader.execute_single_file(paper_path)
        if len(self.history) >= 2 and self.history[-2]:
            self.paper_content = self.history[-2]
            # æ³¨å…¥ä¸€æ¬¡å…¨æ–‡åˆ°ä¸Šä¸‹æ–‡å†å²ï¼Œåç»­å¤šè½®ä»…å‘é€é—®é¢˜
            try:
                remembered = (
                    "è¯·è®°ä½ä»¥ä¸‹è®ºæ–‡å…¨æ–‡ï¼Œåç»­æ‰€æœ‰é—®é¢˜ä»…åŸºäºæ­¤å†…å®¹å›ç­”ï¼Œä¸è¦é‡å¤è¾“å‡ºåŸæ–‡ï¼š\n\n"
                    f"{self.paper_content}"
                )
                self.context_history = [remembered, "å·²æ¥æ”¶å¹¶è®°ä½è®ºæ–‡å†…å®¹"]
            except Exception:
                self.context_history = []
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return True
        self.chatbot.append(["é”™è¯¯", "æ— æ³•è¯»å–è®ºæ–‡å†…å®¹ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"])
        yield from update_ui(chatbot=self.chatbot, history=self.history)
        return False

    def _ask(self, q: DeepReadQuestion) -> Generator:
        try:
            # æ„å»ºé€’è¿›å¼åˆ†æçš„ä¸Šä¸‹æ–‡
            context_parts = [
                "è¯·åŸºäºå·²è®°ä½çš„è®ºæ–‡å…¨æ–‡è¿›è¡Œé€’è¿›å¼ç²¾è¯»åˆ†æï¼Œå¹¶ä¸¥æ ¼å›´ç»•é—®é¢˜ä½œç­”ã€‚\n"
                "æ³¨æ„ï¼šè¯·é¿å…æä¾›ä»»ä½•ä»£ç ã€ä¼ªä»£ç ã€å‘½ä»¤è¡Œæˆ–å…·ä½“å®ç°ç»†èŠ‚ï¼›"
                "è‹¥è¾“å‡ºæµç¨‹å›¾ï¼Œé¡»ä½¿ç”¨ ```mermaid ä»£ç å—ï¼Œå…¶ä½™å›ç­”ä¿æŒè‡ªç„¶è¯­è¨€ã€‚\n"
            ]
            
            # æ·»åŠ å‰é¢åˆ†æçš„ç»“æœä½œä¸ºä¸Šä¸‹æ–‡
            if self.results:
                context_parts.append("\nã€å‰é¢åˆ†æçš„å…³é”®å‘ç°ã€‘")
                for prev_q in self.questions:
                    if prev_q.id in self.results and prev_q.id != q.id:
                        # åªæ·»åŠ å‰é¢å·²åˆ†æçš„é—®é¢˜ç»“æœ
                        if any(prev_q.id == existing_id for existing_id in self.results.keys()):
                            context_parts.append(f"\n{prev_q.description}ï¼š{self.results[prev_q.id][:300]}...")
            
            context_parts.append(f"\n\nã€å½“å‰åˆ†æä»»åŠ¡ã€‘\n{q.question}")
            
            prompt = "".join(context_parts)
            
            resp = yield from request_gpt_model_in_new_thread_with_ui_alive(
                inputs=prompt,
                inputs_show_user=q.question,
                llm_kwargs=self.llm_kwargs,
                chatbot=self.chatbot,
                history=self.context_history or [],
                sys_prompt=(
                    "ä½ æ˜¯èµ„æ·±ç ”ç©¶å‘˜ï¼Œè¿›è¡Œé€’è¿›å¼æ·±åº¦åˆ†æã€‚æ¯ä¸ªé—®é¢˜éƒ½åŸºäºå‰é¢çš„åˆ†æç»“æœè¿›è¡Œæ·±å…¥ã€‚"
                    "è¾“å‡ºä»¥æ¦‚å¿µä¸æ–¹æ³•è®ºå±‚é¢ä¸ºä¸»ï¼Œä¸åŒ…å«ä»»ä½•ä»£ç æˆ–ä¼ªä»£ç ã€‚"
                    "å¦‚æ¶‰åŠMermaidæµç¨‹å›¾ï¼Œè¯·ä½¿ç”¨```mermaid åŒ…è£¹å¹¶ä¿æŒè¯­æ³•æ­£ç¡®ï¼Œå…¶ä½™ä¿æŒè‡ªç„¶è¯­è¨€ã€‚"
                    "æ³¨æ„ä¿æŒåˆ†æçš„è¿è´¯æ€§å’Œé€’è¿›æ€§ã€‚"
                ),
            )
            if resp:
                self.results[q.id] = resp
                # è®°å½•æœ¬è½®äº¤äº’çš„è¾“å…¥ä¸è¾“å‡ºç”¨äºtokenä¼°ç®—
                try:
                    self._token_inputs.append(prompt)
                    self._token_outputs.append(resp)
                except Exception:
                    pass
                return True
            return False
        except Exception as e:
            self.chatbot.append(["é”™è¯¯", f"ç²¾è¯»é—®é¢˜åˆ†æå¤±è´¥: {str(e)}"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return False

    def _generate_report(self) -> Generator:
        self.chatbot.append(["ç”ŸæˆæŠ¥å‘Š", "æ­£åœ¨æ•´åˆé€’è¿›å¼ç²¾è¯»ç»“æœï¼Œç”Ÿæˆæ·±åº¦æŠ€æœ¯æŠ¥å‘Š..."])
        yield from update_ui(chatbot=self.chatbot, history=self.history)

        prompt = (
            "è¯·å°†ä»¥ä¸‹é€’è¿›å¼ç²¾è¯»åˆ†ææ•´ç†ä¸ºå®Œæ•´çš„æŠ€æœ¯æŠ¥å‘Šã€‚"
            "æŠ¥å‘Šåº”ä½“ç°åˆ†æçš„é€’è¿›é€»è¾‘ï¼šä»é—®é¢˜åŸŸç†è§£â†’ç†è®ºæ„å»ºâ†’æŠ€æœ¯å®ç°â†’å®éªŒéªŒè¯â†’æ‰¹åˆ¤åˆ†æâ†’å·¥ç¨‹å¤ç°â†’æ¶æ„å¯è§†åŒ–â†’å½±å“è¯„ä¼°â†’è¦ç‚¹æ€»ç»“ã€‚"
            "å±‚æ¬¡æ¸…æ™°ï¼Œçªå‡ºæ ¸å¿ƒæ€æƒ³ä¸å®éªŒè®¾è®¡è¦ç‚¹ï¼Œä¸åŒ…å«ä»»ä½•ä»£ç /ä¼ªä»£ç /å‘½ä»¤è¡Œã€‚"
            "è‹¥åŒ…å«```mermaid ä»£ç å—ï¼Œè¯·åŸæ ·ä¿ç•™ã€‚\n\n"
            "ã€é€’è¿›å¼åˆ†æç»“æœã€‘"
        )
        
        # æŒ‰ç…§é€’è¿›é¡ºåºç»„ç»‡åˆ†æç»“æœ
        layer_order = [
            "problem_domain_and_motivation",
            "theoretical_framework_and_contributions", 
            "method_design_and_technical_details",
            "experimental_validation_and_effectiveness",
            "assumptions_limitations_and_threats",
            "reproduction_guide_and_engineering",
            "flowcharts_and_architecture",
            "impact_assessment_and_future_directions",
            "executive_summary_and_key_points"
        ]
        
        for layer_id in layer_order:
            for q in self.questions:
                if q.id == layer_id and q.id in self.results:
                    prompt += f"\n\n## {q.description}\n{self.results[q.id]}"
                    break

        resp = yield from request_gpt_model_in_new_thread_with_ui_alive(
            inputs=prompt,
            inputs_show_user="ç”Ÿæˆé€’è¿›å¼è®ºæ–‡ç²¾è¯»æŠ€æœ¯æŠ¥å‘Š",
            llm_kwargs=self.llm_kwargs,
            chatbot=self.chatbot,
            history=[],
            sys_prompt=(
                "ä»¥é€’è¿›å¼æ·±åº¦åˆ†æä¸ºä¸»çº¿ç»„ç»‡æŠ¥å‘Šï¼šä½“ç°ä»å®è§‚åˆ°å¾®è§‚ã€ä»ç†è®ºåˆ°å®è·µçš„å®Œæ•´åˆ†æé“¾æ¡ã€‚"
                "æ¯ä¸ªéƒ¨åˆ†éƒ½è¦ä¸å‰é¢çš„åˆ†æå½¢æˆé€»è¾‘å…³è”ï¼Œçªå‡ºé€’è¿›å…³ç³»ã€‚"
                "ä»¥å·¥ç¨‹å¤ç°ä¸ºç›®æ ‡ï¼ŒèƒŒæ™¯æç®€ï¼Œæ–¹æ³•ä¸å®ç°ç»†èŠ‚å……åˆ†ï¼Œæ¡ç†åˆ†æ˜ï¼ŒåŒ…å«å¿…è¦çš„æ¸…å•ä¸æ­¥éª¤ã€‚"
            ),
        )
        return resp or "æŠ¥å‘Šç”Ÿæˆå¤±è´¥"

    def _extract_secondary_category(self, report: str) -> str:
        """ä»æŠ¥å‘Šä¸­æå–â€œå½’å±ï¼šâ€åçš„äºŒçº§åˆ†ç±»æ–‡æœ¬ï¼Œåªä¿ç•™ç±»ä¼¼
        â€œ7. æœºå™¨å­¦ä¹ è¾…åŠ©è®¾è®¡ (ML-Aided RF Design) -> ç³»ç»Ÿçº§å»ºæ¨¡ä¸å¿«é€Ÿç»¼åˆâ€ã€‚
        """
        try:
            if not isinstance(report, str):
                return None
            m = re.search(r"^å½’å±ï¼š\s*([^\r\n]+)", report, flags=re.MULTILINE)
            if not m:
                return None
            category_line = m.group(1).strip()
            category_line = re.sub(r"[\s\u3000]+$", "", category_line)
            return category_line if category_line else None
        except Exception:
            return None

    def save_report(self, report: str) -> str:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        pdf_basename = "æœªçŸ¥è®ºæ–‡"
        if self.paper_file_path and os.path.exists(self.paper_file_path):
            pdf_basename = os.path.splitext(os.path.basename(self.paper_file_path))[0]
            pdf_basename = re.sub(r'[^\w\u4e00-\u9fff]', '_', pdf_basename)
            if len(pdf_basename) > 50:
                pdf_basename = pdf_basename[:50]

        parts: List[str] = []
        parts.append(f"è®ºæ–‡ç²¾è¯»æŠ€æœ¯æŠ¥å‘Š\n\n{report}")
        # ä¼˜å…ˆè¿½åŠ æ‰§è¡Œçº§æ‘˜è¦ä¸æµç¨‹å›¾
        if "executive_summary_and_key_points" in self.results:
            parts.append(f"\n\n## æ‰§è¡Œçº§æ‘˜è¦\n\n{self.results['executive_summary_and_key_points']}")
        if "flowcharts_and_architecture" in self.results:
            parts.append(f"\n\n## æ ¸å¿ƒæµç¨‹å›¾\n\n{self.results['flowcharts_and_architecture']}")
        # è¿½åŠ å…¶ä½™ç»´åº¦
        for q in self.questions:
            if q.id in self.results and q.id not in {"executive_summary_and_key_points", "flowcharts_and_architecture"}:
                parts.append(f"\n\n## {q.description}\n\n{self.results[q.id]}")

        # è¿½åŠ  Token ä¼°ç®—ç»“æœ
        try:
            stats = estimate_token_usage(self._token_inputs, self._token_outputs, self.llm_kwargs.get('llm_model', 'gpt-3.5-turbo'))
            if stats and stats.get('sum_total_tokens', 0) > 0:
                parts.append(
                    "\n\n## Token ä¼°ç®—\n\n"
                    f"- æ¨¡å‹: {stats.get('model')}\n\n"
                    f"- è¾“å…¥ tokens: {stats.get('sum_input_tokens', 0)}\n"
                    f"- è¾“å‡º tokens: {stats.get('sum_output_tokens', 0)}\n"
                    f"- æ€» tokens: {stats.get('sum_total_tokens', 0)}\n"
                )
        except Exception:
            pass

        content = "".join(parts)
        if hasattr(self, 'yaml_header') and self.yaml_header:
            content = f"{self.yaml_header}\n\n" + content
        result_file = write_history_to_file(
            history=[content],
            file_basename=f"{timestamp}_{pdf_basename}_ç²¾è¯»æŠ¥å‘Š.md",
        )
        if result_file and os.path.exists(result_file):
            promote_file_to_downloadzone(result_file, chatbot=self.chatbot)
            return result_file
        return None

    def analyze_paper(self, paper_path: str) -> Generator:
        ok = yield from self._load_paper(paper_path)
        if not ok:
            return None
        for q in self.questions:
            yield from self._ask(q)
        report = yield from self._generate_report()
        # ä»æŠ¥å‘Šä¸­æå–äºŒçº§åˆ†ç±»å½’å±
        self.secondary_category = self._extract_secondary_category(report)
        # ç”Ÿæˆ YAML å¤´
        self.yaml_header = yield from self._generate_yaml_header()
        saved = self.save_report(report)
        return saved


def _find_paper_files(path: str) -> List[str]:
    files: List[str] = []
    if os.path.isfile(path):
        ext = os.path.splitext(path)[1].lower()
        if ext in [".pdf", ".docx", ".doc", ".txt", ".md", ".tex"]:
            files.append(path)
        return files
    if os.path.isdir(path):
        exts = [".pdf", ".docx", ".doc", ".txt", ".md", ".tex"]
        for root, _dirs, fnames in os.walk(path):
            for fname in fnames:
                fpath = os.path.join(root, fname)
                if os.path.splitext(fname)[1].lower() in exts:
                    files.append(fpath)
    return files


def _download_paper_by_id(paper_info, chatbot, history) -> str:
    from crazy_functions.review_fns.data_sources.scihub_source import SciHub
    id_type, paper_id = paper_info

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    user_name = chatbot.get_user() if hasattr(chatbot, 'get_user') else "default"
    from toolbox import get_log_folder, get_user
    base_save_dir = get_log_folder(get_user(chatbot), plugin_name='paper_download')
    save_dir = os.path.join(base_save_dir, f"papers_{timestamp}")
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    save_path = Path(save_dir)

    chatbot.append(["ä¸‹è½½è®ºæ–‡", f"æ­£åœ¨ä¸‹è½½{'arXiv' if id_type == 'arxiv' else 'DOI'} {paper_id} çš„è®ºæ–‡..."])
    update_ui(chatbot=chatbot, history=history)

    pdf_path = None
    try:
        if id_type == 'arxiv':
            formatted_id = format_arxiv_id(paper_id)
            paper_result = get_arxiv_paper(formatted_id)
            if not paper_result:
                chatbot.append(["ä¸‹è½½å¤±è´¥", f"æœªæ‰¾åˆ°arXivè®ºæ–‡: {paper_id}"])
                update_ui(chatbot=chatbot, history=history)
                return None
            filename = f"arxiv_{paper_id.replace('/', '_')}.pdf"
            pdf_path = str(save_path / filename)
            paper_result.download_pdf(filename=pdf_path)
        else:
            sci_hub = SciHub(doi=paper_id, path=save_path)
            pdf_path = sci_hub.fetch()

        if pdf_path and os.path.exists(pdf_path):
            promote_file_to_downloadzone(pdf_path, chatbot=chatbot)
            chatbot.append(["ä¸‹è½½æˆåŠŸ", f"å·²æˆåŠŸä¸‹è½½è®ºæ–‡: {os.path.basename(pdf_path)}"])
            update_ui(chatbot=chatbot, history=history)
            return pdf_path
        chatbot.append(["ä¸‹è½½å¤±è´¥", f"è®ºæ–‡ä¸‹è½½å¤±è´¥: {paper_id}"])
        update_ui(chatbot=chatbot, history=history)
        return None
    except Exception as e:
        chatbot.append(["ä¸‹è½½é”™è¯¯", f"ä¸‹è½½è®ºæ–‡æ—¶å‡ºé”™: {str(e)}"])
        update_ui(chatbot=chatbot, history=history)
        return None


@CatchException
def æ‰¹é‡è®ºæ–‡ç²¾è¯»(txt: str, llm_kwargs: Dict, plugin_kwargs: Dict, chatbot: List,
            history: List, system_prompt: str, user_request: str):
    """ä¸»å‡½æ•° - æ‰¹é‡è®ºæ–‡ç²¾è¯»"""
    chatbot.append([
        "å‡½æ•°æ’ä»¶åŠŸèƒ½åŠä½¿ç”¨æ–¹å¼",
        (
            "æ‰¹é‡è®ºæ–‡ç²¾è¯»ï¼šå¯¹å¤šä¸ªè®ºæ–‡æ–‡ä»¶è¿›è¡Œæ·±å…¥é˜…è¯»ä¸æŠ€æœ¯å¤ç›˜ï¼Œè¾“å‡ºé¢å‘å®ç°ä¸å¤ç°çš„æ·±åº¦æŠ¥å‘Šã€‚\n\n"
            "ä½¿ç”¨æ–¹å¼ï¼š\n1) è¾“å…¥åŒ…å«å¤šä¸ªPDFçš„æ–‡ä»¶å¤¹è·¯å¾„ï¼›\n2) æˆ–è¾“å…¥å¤šä¸ªè®ºæ–‡IDï¼ˆDOIæˆ–arXivï¼‰ï¼Œç”¨é€—å·åˆ†éš”ï¼›\n3) ç‚¹å‡»å¼€å§‹ã€‚\n\n"
            "æ³¨æ„äº‹é¡¹ï¼š\n- è‹¥éœ€è¦è¾“å‡ºå…¬å¼ï¼Œè¯·ä½¿ç”¨ LaTeX æ•°å­¦æ ¼å¼ï¼šè¡Œå†…å…¬å¼ç”¨ $...$ï¼Œè¡Œé—´å…¬å¼ç”¨ $$...$$ã€‚"
        ),
    ])
    yield from update_ui(chatbot=chatbot, history=history)

    paper_files: List[str] = []

    if ',' in txt:
        paper_ids = [pid.strip() for pid in txt.split(',') if pid.strip()]
        chatbot.append(["æ£€æµ‹åˆ°å¤šä¸ªè®ºæ–‡ID", f"æ£€æµ‹åˆ° {len(paper_ids)} ä¸ªè®ºæ–‡IDï¼Œå‡†å¤‡æ‰¹é‡ä¸‹è½½..."])
        yield from update_ui(chatbot=chatbot, history=history)
        for i, pid in enumerate(paper_ids):
            paper_info = extract_paper_id(pid)
            if paper_info:
                chatbot.append([f"ä¸‹è½½è®ºæ–‡ {i+1}/{len(paper_ids)}", f"æ­£åœ¨ä¸‹è½½ {'arXiv' if paper_info[0] == 'arxiv' else 'DOI'} ID: {paper_info[1]}..."])
                yield from update_ui(chatbot=chatbot, history=history)
                p = _download_paper_by_id(paper_info, chatbot, history)
                if p:
                    paper_files.append(p)
                else:
                    chatbot.append(["ä¸‹è½½å¤±è´¥", f"æ— æ³•ä¸‹è½½è®ºæ–‡: {pid}"])
                    yield from update_ui(chatbot=chatbot, history=history)
            else:
                chatbot.append(["IDæ ¼å¼é”™è¯¯", f"æ— æ³•è¯†åˆ«è®ºæ–‡IDæ ¼å¼: {pid}"])
                yield from update_ui(chatbot=chatbot, history=history)
    else:
        paper_info = extract_paper_id(txt)
        if paper_info:
            chatbot.append(["æ£€æµ‹åˆ°è®ºæ–‡ID", f"æ£€æµ‹åˆ°{'arXiv' if paper_info[0] == 'arxiv' else 'DOI'} ID: {paper_info[1]}ï¼Œå‡†å¤‡ä¸‹è½½è®ºæ–‡..."])
            yield from update_ui(chatbot=chatbot, history=history)
            p = _download_paper_by_id(paper_info, chatbot, history)
            if p:
                paper_files.append(p)
            else:
                report_exception(chatbot, history, a="ä¸‹è½½è®ºæ–‡å¤±è´¥", b=f"æ— æ³•ä¸‹è½½{'arXiv' if paper_info[0] == 'arxiv' else 'DOI'}è®ºæ–‡: {paper_info[1]}")
                yield from update_ui(chatbot=chatbot, history=history)
                return
        else:
            if not os.path.exists(txt):
                report_exception(chatbot, history, a=f"æ‰¹é‡ç²¾è¯»è®ºæ–‡: {txt}", b=f"æ‰¾ä¸åˆ°æ–‡ä»¶æˆ–æ— æƒè®¿é—®: {txt}")
                yield from update_ui(chatbot=chatbot, history=history)
                return
            user_name = chatbot.get_user()
            validate_path_safety(txt, user_name)
            paper_files = _find_paper_files(txt)
            if not paper_files:
                report_exception(chatbot, history, a="æ‰¹é‡ç²¾è¯»è®ºæ–‡", b=f"åœ¨è·¯å¾„ {txt} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„è®ºæ–‡æ–‡ä»¶")
                yield from update_ui(chatbot=chatbot, history=history)
                return

    yield from update_ui(chatbot=chatbot, history=history)

    if not paper_files:
        chatbot.append(["é”™è¯¯", "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯åˆ†æçš„è®ºæ–‡æ–‡ä»¶"])
        yield from update_ui(chatbot=chatbot, history=history)
        return

    chatbot.append(["å¼€å§‹æ‰¹é‡ç²¾è¯»", f"æ‰¾åˆ° {len(paper_files)} ç¯‡è®ºæ–‡ï¼Œå¼€å§‹æ·±å…¥åˆ†æ..."])
    yield from update_ui(chatbot=chatbot, history=history)

    analyzer = BatchPaperDetailAnalyzer(llm_kwargs, plugin_kwargs, chatbot, history, system_prompt)

    successes: List[str] = []
    failures: List[str] = []

    for i, paper_file in enumerate(paper_files):
        try:
            chatbot.append([f"ç²¾è¯»è®ºæ–‡ {i+1}/{len(paper_files)}", f"æ­£åœ¨ç²¾è¯»: {os.path.basename(paper_file)}"])
            yield from update_ui(chatbot=chatbot, history=history)
            outfile = yield from analyzer.analyze_paper(paper_file)
            if outfile:
                successes.append(outfile)
                chatbot.append([f"å®Œæˆè®ºæ–‡ {i+1}/{len(paper_files)}", f"æˆåŠŸç”ŸæˆæŠ¥å‘Š: {os.path.basename(outfile)}"])
            else:
                failures.append(os.path.basename(paper_file))
                chatbot.append([f"å¤±è´¥è®ºæ–‡ {i+1}/{len(paper_files)}", f"åˆ†æå¤±è´¥: {os.path.basename(paper_file)}"])
            yield from update_ui(chatbot=chatbot, history=history)
        except Exception as e:
            failures.append(os.path.basename(paper_file))
            chatbot.append([f"é”™è¯¯è®ºæ–‡ {i+1}/{len(paper_files)}", f"åˆ†æå‡ºé”™: {os.path.basename(paper_file)} - {str(e)}"])
            yield from update_ui(chatbot=chatbot, history=history)

    summary = "æ‰¹é‡ç²¾è¯»å®Œæˆï¼\n\n"
    summary += "ğŸ“Š åˆ†æç»Ÿè®¡ï¼š\n"
    summary += f"- æ€»è®ºæ–‡æ•°ï¼š{len(paper_files)}\n"
    summary += f"- æˆåŠŸåˆ†æï¼š{len(successes)}\n"
    summary += f"- åˆ†æå¤±è´¥ï¼š{len(failures)}\n\n"
    if successes:
        summary += "âœ… æˆåŠŸç”ŸæˆæŠ¥å‘Šï¼š\n"
        for p in successes:
            summary += f"- {os.path.basename(p)}\n"
    if failures:
        summary += "\nâŒ åˆ†æå¤±è´¥çš„è®ºæ–‡ï¼š\n"
        for name in failures:
            summary += f"- {name}\n"

    chatbot.append(["æ‰¹é‡ç²¾è¯»å®Œæˆ", summary])
    yield from update_ui(chatbot=chatbot, history=history)


