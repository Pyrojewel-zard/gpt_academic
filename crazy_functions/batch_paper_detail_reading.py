import os
import re
import time
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass
from typing import Dict, List, Generator

from crazy_functions.crazy_utils import request_gpt_model_in_new_thread_with_ui_alive
from toolbox import update_ui, promote_file_to_downloadzone, write_history_to_file, CatchException, report_exception
from shared_utils.fastapi_server import validate_path_safety
from crazy_functions.paper_fns.paper_download import extract_paper_id, get_arxiv_paper, format_arxiv_id


@dataclass
class DeepReadQuestion:
    """è®ºæ–‡ç²¾è¯»é—®é¢˜é¡¹"""
    id: str
    question: str
    importance: int
    description: str


class BatchPaperDetailAnalyzer:
    """æ‰¹é‡è®ºæ–‡ç²¾è¯»åˆ†æå™¨"""

    def __init__(self, llm_kwargs: Dict, plugin_kwargs: Dict, chatbot: List, history: List, system_prompt: str):
        self.llm_kwargs = llm_kwargs
        self.plugin_kwargs = plugin_kwargs
        self.chatbot = chatbot
        self.history = history
        self.system_prompt = system_prompt
        self.paper_content = ""
        self.results: Dict[str, str] = {}
        self.paper_file_path: str = None

        # ç²¾è¯»ç»´åº¦ï¼ˆæ›´æ·±å…¥çš„æŠ€æœ¯ç»†èŠ‚ã€å¯å¤ç°æ€§ã€ç†è®ºä¾æ®ç­‰ï¼‰
        self.questions: List[DeepReadQuestion] = [
            DeepReadQuestion(
                id="problem_statement_and_contributions",
                description="é—®é¢˜å®šä¹‰ä¸è´¡çŒ®",
                importance=5,
                question=(
                    "è¯·ä¸¥è°¨æ¦‚è¿°è®ºæ–‡è¦è§£å†³çš„é—®é¢˜ã€å½¢å¼åŒ–å®šä¹‰ä¸è¾¹ç•Œæ¡ä»¶ï¼›"
                    "æ¢³ç†è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰ï¼Œå¹¶é€æ¡è¯´æ˜å…¶ä¸å·²æœ‰å·¥ä½œçš„æœ¬è´¨åŒºåˆ«ã€‚"
                ),
            ),
            DeepReadQuestion(
                id="method_derivation_and_theory",
                description="æ–¹æ³•æ¨å¯¼ä¸ç†è®ºä¿éšœ",
                importance=5,
                question=(
                    "è¯·å¯¹æ ¸å¿ƒæ–¹æ³•è¿›è¡Œå…¬å¼çº§ç²¾è¯»ï¼šç»™å‡ºå…³é”®ç¬¦å·å®šä¹‰ã€æŸå¤±å‡½æ•°/ç›®æ ‡å‡½æ•°ï¼Œ"
                    "æ¨å¯¼ä¸»å‘½é¢˜ï¼ˆæˆ–å®šç†/å¼•ç†ï¼‰çš„å…³é”®æ­¥éª¤ä¸å¿…è¦å‡è®¾ï¼›è‹¥ç»™å‡ºæ”¶æ•›æ€§/å¤æ‚åº¦/ä¸€è‡´æ€§ç»“è®ºï¼Œ"
                    "è¯·æ˜ç¡®å…¶é€‚ç”¨å‰æä¸å±€é™ã€‚å¿…è¦æ—¶ç»™å‡ºç®€åŒ–ç‰ˆç­‰ä»·è¡¨è¿°æ–¹ä¾¿å®ç°ã€‚"
                ),
            ),
            DeepReadQuestion(
                id="algorithm_pseudocode",
                description="ç®—æ³•ä¼ªä»£ç ï¼ˆå¯å®ç°ï¼‰",
                importance=5,
                question=(
                    "è¯·ç»™å‡ºæ ¸å¿ƒç®—æ³•çš„å¯å®ç°ä¼ªä»£ç ï¼ˆç±»ä¼¼Pythonä¼ªä»£ç é£æ ¼ï¼‰ï¼Œ"
                    "åŒ…æ‹¬è¾“å…¥ã€è¾“å‡ºã€å…³é”®è¶…å‚æ•°ã€å¾ªç¯/åˆ†æ”¯ç»“æ„ã€é‡è¦ä¸­é—´é‡ï¼Œ"
                    "å¹¶æ ‡æ³¨æ¯ä¸€æ­¥çš„æ—¶é—´/ç©ºé—´å¤æ‚åº¦çº§åˆ«ã€‚"
                ),
            ),
            DeepReadQuestion(
                id="assumptions_and_threats",
                description="å…³é”®å‡è®¾ä¸æœ‰æ•ˆæ€§å¨èƒ",
                importance=4,
                question=(
                    "åˆ—å‡ºè®ºæ–‡æ˜¾å¼æˆ–éšå«å‡è®¾ï¼ˆæ•°æ®åˆ†å¸ƒã€ç‹¬ç«‹æ€§ã€å¯è·å–æ€§ã€ç¡¬ä»¶æ¡ä»¶ç­‰ï¼‰ï¼Œ"
                    "åˆ†æè¿™äº›å‡è®¾ä¸ç°å®å·®è·å¯èƒ½å¸¦æ¥çš„å¤±æ•ˆæƒ…å½¢ï¼›"
                    "è¡¥å……ä½œè€…æœªå……åˆ†è®¨è®ºä½†å¯èƒ½é‡è¦çš„æœ‰æ•ˆæ€§å¨èƒã€‚"
                ),
            ),
            DeepReadQuestion(
                id="experiments_reproduction_plan",
                description="å®éªŒå¤ç°å®æ“æ–¹æ¡ˆ",
                importance=5,
                question=(
                    "è¯·è¾“å‡ºå¤ç°æ¸…å•ï¼šæ•°æ®é›†ç‰ˆæœ¬/è·å–æ–¹å¼/è®¸å¯ã€é¢„å¤„ç†æµç¨‹ã€æ ¸å¿ƒè¶…å‚ã€è®­ç»ƒç»†èŠ‚ï¼ˆæ‰¹å¤§å°ã€å­¦ä¹ ç‡ã€è°ƒåº¦å™¨ã€åœæ­¢å‡†åˆ™ã€éšæœºç§å­ï¼‰ã€"
                    "è¯„ä¼°æŒ‡æ ‡å®šä¹‰ä¸è®¡ç®—ã€å¯¹æ¯”æ–¹æ³•ä¸æ¶ˆèå®éªŒè®¾è®¡ã€é¢„æœŸèµ„æºéœ€æ±‚ï¼ˆGPU/CPU/å†…å­˜/æ—¶é•¿ï¼‰ã€‚"
                ),
            ),
            DeepReadQuestion(
                id="implementation_gotchas",
                description="å®ç°ç»†èŠ‚ä¸å‘ä½",
                importance=4,
                question=(
                    "è¯·åˆ—å‡ºå®ç°ä¸­å®¹æ˜“å¿½ç•¥ä½†å½±å“ç»“æœçš„ç»†èŠ‚ï¼šæ•°å€¼ç¨³å®šæ€§ï¼ˆå½’ä¸€åŒ–/è£å‰ª/åˆå§‹åŒ–ï¼‰ã€ç²¾åº¦å·®å¼‚ï¼ˆfp16/bf16ï¼‰ã€"
                    "éšæœºæ€§æ§åˆ¶ã€å¹¶è¡Œ/åˆ†å¸ƒå¼å·®å¼‚ã€æ¡†æ¶ç‰ˆæœ¬å…¼å®¹æ€§ã€å¸¸è§æŠ¥é”™ä¸æ’æŸ¥è¦ç‚¹ã€‚"
                ),
            ),
            DeepReadQuestion(
                id="dataset_and_license",
                description="æ•°æ®é›†ä¸è®¸å¯",
                importance=3,
                question=(
                    "è¯·ç½—åˆ—è®ºæ–‡ç”¨åˆ°çš„æ•°æ®/æ¨¡å‹èµ„æºåŠå…¶è®¸å¯è¯ï¼›è¯´æ˜æ˜¯å¦å­˜åœ¨é—­æºä¾èµ–æˆ–ä¸å¯è·å¾—èµ„æºï¼Œ"
                    "å¹¶æä¾›ç›¸åº”æ›¿ä»£æ–¹æ¡ˆå»ºè®®ã€‚"
                ),
            ),
            DeepReadQuestion(
                id="results_and_ablations",
                description="ç»“æœå¤æ ¸ä¸æ¶ˆèæ´å¯Ÿ",
                importance=4,
                question=(
                    "è¯·å¯¹ä¸»è¦ç»“æœè¿›è¡Œè¦ç‚¹å¤æ ¸ï¼šæ˜¯å¦æ»¡è¶³ç»Ÿè®¡æ˜¾è‘—æ€§ã€æ˜¯å¦æœ‰æ–¹å·®æŠ¥å‘Šï¼›"
                    "ä»æ¶ˆèå®éªŒä¸­æç‚¼æœ€å…³é”®çš„å½±å“å› ç´ ä¸äº¤äº’æ•ˆåº”ï¼ŒæŒ‡å‡ºå¯èƒ½çš„è¯¯å¯¼æ€§ç»“è®ºã€‚"
                ),
            ),
            DeepReadQuestion(
                id="limitations_future_and_impact",
                description="é™åˆ¶ã€æœªæ¥æ–¹å‘ä¸å½±å“",
                importance=3,
                question=(
                    "æ€»ç»“è®ºæ–‡çš„ä¸»è¦å±€é™ã€æ½œåœ¨é£é™©æˆ–ä¼¦ç†é—®é¢˜ï¼›æå‡ºå¯æ“ä½œçš„æ”¹è¿›æ–¹å‘ä¸åç»­ç ”ç©¶å‡è®¾ï¼›"
                    "è¯„ä¼°å…¶å¯¹å­¦æœ¯ä¸äº§ä¸šçš„ä¸­çŸ­æœŸå½±å“è·¯å¾„ã€‚"
                ),
            ),
            DeepReadQuestion(
                id="mermaid_flowcharts",
                description="æ ¸å¿ƒæµç¨‹å›¾ï¼ˆMermaidï¼‰",
                importance=4,
                question=(
                    "è¯·ç»™å‡ºä¸å®ç°å¼ºç›¸å…³çš„æµç¨‹å›¾ï¼ˆå¯å¤šä¸ªæ¨¡å—ï¼‰ï¼Œä½¿ç”¨```mermaid ä»£ç å—ï¼Œ"
                    "é‡‡ç”¨ flowchart TD æˆ– LRï¼ŒèŠ‚ç‚¹åç”¨å¼•å·ï¼Œåˆ†æ”¯ç”¨åˆ¤å®šèŠ‚ç‚¹ï¼Œçªå‡ºè¾“å…¥â†’å¤„ç†â†’è¾“å‡ºä¸å…³é”®åˆ†æ”¯ã€‚"
                ),
            ),
            DeepReadQuestion(
                id="exec_summary_md",
                description="æ‰§è¡Œçº§æ‘˜è¦ï¼ˆMarkdownï¼‰",
                importance=5,
                question=(
                    "ç»™å‡ºç”¨äºäº¤ä»˜/è½åœ°çš„æç®€ Markdown æ‘˜è¦ï¼š\n"
                    "- ä¸€å¥è¯æ€»è¿°æ–¹æ³•ä¸ä½œç”¨\n"
                    "- ä¸‰åˆ°äº”æ¡å®ç°è¦ç‚¹ï¼ˆè¾“å…¥/æ­¥éª¤/è¾“å‡ºï¼‰\n"
                    "- ä¸‰åˆ°äº”æ¡å¤ç°å®ç”¨è¦ç‚¹ï¼ˆæ•°æ®/è¶…å‚/èµ„æº/æ—¶é•¿ï¼‰"
                ),
            ),
        ]

        self.questions.sort(key=lambda q: q.importance, reverse=True)

    def _load_paper(self, paper_path: str) -> Generator:
        from crazy_functions.doc_fns.text_content_loader import TextContentLoader
        yield from update_ui(chatbot=self.chatbot, history=self.history)
        self.paper_file_path = paper_path
        loader = TextContentLoader(self.chatbot, self.history)
        yield from loader.execute_single_file(paper_path)
        if len(self.history) >= 2 and self.history[-2]:
            self.paper_content = self.history[-2]
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return True
        self.chatbot.append(["é”™è¯¯", "æ— æ³•è¯»å–è®ºæ–‡å†…å®¹ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"])
        yield from update_ui(chatbot=self.chatbot, history=self.history)
        return False

    def _ask(self, q: DeepReadQuestion) -> Generator:
        try:
            prompt = (
                "è¯·åŸºäºä»¥ä¸‹è®ºæ–‡å†…å®¹è¿›è¡Œç²¾è¯»åˆ†æï¼Œå¹¶ä¸¥æ ¼å›´ç»•é—®é¢˜ä½œç­”ã€‚\n\n"
                f"è®ºæ–‡å†…å®¹ï¼š\n{self.paper_content}\n\n"
                f"é—®é¢˜ï¼š{q.question}"
            )
            resp = yield from request_gpt_model_in_new_thread_with_ui_alive(
                inputs=prompt,
                inputs_show_user=q.question,
                llm_kwargs=self.llm_kwargs,
                chatbot=self.chatbot,
                history=[],
                sys_prompt=(
                    "ä½ æ˜¯èµ„æ·±ç ”ç©¶å‘˜ä¸ä»£ç å®ç°è€…ï¼Œå›ç­”è¦å¯æ“ä½œã€å¯å¤ç°ã€ç»“æ„æ¸…æ™°ã€‚"
                    "å¦‚æ¶‰åŠå…¬å¼ä¸ä¼ªä»£ç ï¼Œä½¿ç”¨æ¸…æ™°å¯å®ç°çš„å†™æ³•ï¼›å¦‚æ¶‰åŠMermaidï¼Œè¯·ç”¨```mermaid åŒ…è£¹ã€‚"
                ),
            )
            if resp:
                self.results[q.id] = resp
                return True
            return False
        except Exception as e:
            self.chatbot.append(["é”™è¯¯", f"ç²¾è¯»é—®é¢˜åˆ†æå¤±è´¥: {str(e)}"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return False

    def _generate_report(self) -> Generator:
        self.chatbot.append(["ç”ŸæˆæŠ¥å‘Š", "æ­£åœ¨æ•´åˆç²¾è¯»ç»“æœï¼Œç”Ÿæˆæ·±åº¦æŠ€æœ¯æŠ¥å‘Š..."])
        yield from update_ui(chatbot=self.chatbot, history=self.history)

        prompt = (
            "è¯·å°†ä»¥ä¸‹ç²¾è¯»åˆ†ææ•´ç†ä¸ºå®Œæ•´çš„æŠ€æœ¯æŠ¥å‘Šï¼Œå±‚æ¬¡æ¸…æ™°ã€å¯è½åœ°å®ç°ã€‚"
            "è‹¥åŒ…å«```mermaid ä»£ç å—ï¼Œè¯·åŸæ ·ä¿ç•™ã€‚"
        )
        for q in self.questions:
            if q.id in self.results:
                prompt += f"\n\n[{q.description}]\n{self.results[q.id]}"

        resp = yield from request_gpt_model_in_new_thread_with_ui_alive(
            inputs=prompt,
            inputs_show_user="ç”Ÿæˆè®ºæ–‡ç²¾è¯»æŠ€æœ¯æŠ¥å‘Š",
            llm_kwargs=self.llm_kwargs,
            chatbot=self.chatbot,
            history=[],
            sys_prompt=(
                "ä»¥å·¥ç¨‹å¤ç°ä¸ºç›®æ ‡ç»„ç»‡æŠ¥å‘Šï¼šèƒŒæ™¯æç®€ï¼Œæ–¹æ³•ä¸å®ç°ç»†èŠ‚å……åˆ†ï¼Œ"
                "æ¡ç†åˆ†æ˜ï¼ŒåŒ…å«å¿…è¦çš„æ¸…å•ä¸æ­¥éª¤ã€‚"
            ),
        )
        return resp or "æŠ¥å‘Šç”Ÿæˆå¤±è´¥"

    def save_report(self, report: str) -> str:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        pdf_basename = "æœªçŸ¥è®ºæ–‡"
        if self.paper_file_path and os.path.exists(self.paper_file_path):
            pdf_basename = os.path.splitext(os.path.basename(self.paper_file_path))[0]
            pdf_basename = re.sub(r'[^\w\u4e00-\u9fff]', '_', pdf_basename)
            if len(pdf_basename) > 50:
                pdf_basename = pdf_basename[:50]

        parts: List[str] = []
        parts.append(f"è®ºæ–‡ç²¾è¯»æŠ€æœ¯æŠ¥å‘Š\n\n{report}")
        # ä¼˜å…ˆè¿½åŠ æ‰§è¡Œçº§æ‘˜è¦ä¸æµç¨‹å›¾
        if "exec_summary_md" in self.results:
            parts.append(f"\n\n## æ‰§è¡Œçº§æ‘˜è¦\n\n{self.results['exec_summary_md']}")
        if "mermaid_flowcharts" in self.results:
            parts.append(f"\n\n## æ ¸å¿ƒæµç¨‹å›¾\n\n{self.results['mermaid_flowcharts']}")
        # è¿½åŠ å…¶ä½™ç»´åº¦
        for q in self.questions:
            if q.id in self.results and q.id not in {"exec_summary_md", "mermaid_flowcharts"}:
                parts.append(f"\n\n## {q.description}\n\n{self.results[q.id]}")

        content = "".join(parts)
        result_file = write_history_to_file(
            history=[content],
            file_basename=f"{timestamp}_{pdf_basename}_ç²¾è¯»æŠ¥å‘Š.md",
        )
        if result_file and os.path.exists(result_file):
            promote_file_to_downloadzone(result_file, chatbot=self.chatbot)
            return result_file
        return None

    def analyze_paper(self, paper_path: str) -> Generator:
        ok = yield from self._load_paper(paper_path)
        if not ok:
            return None
        for q in self.questions:
            yield from self._ask(q)
        report = yield from self._generate_report()
        saved = self.save_report(report)
        return saved


def _find_paper_files(path: str) -> List[str]:
    files: List[str] = []
    if os.path.isfile(path):
        ext = os.path.splitext(path)[1].lower()
        if ext in [".pdf", ".docx", ".doc", ".txt", ".md", ".tex"]:
            files.append(path)
        return files
    if os.path.isdir(path):
        exts = [".pdf", ".docx", ".doc", ".txt", ".md", ".tex"]
        for root, _dirs, fnames in os.walk(path):
            for fname in fnames:
                fpath = os.path.join(root, fname)
                if os.path.splitext(fname)[1].lower() in exts:
                    files.append(fpath)
    return files


def _download_paper_by_id(paper_info, chatbot, history) -> str:
    from crazy_functions.review_fns.data_sources.scihub_source import SciHub
    id_type, paper_id = paper_info

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    user_name = chatbot.get_user() if hasattr(chatbot, 'get_user') else "default"
    from toolbox import get_log_folder, get_user
    base_save_dir = get_log_folder(get_user(chatbot), plugin_name='paper_download')
    save_dir = os.path.join(base_save_dir, f"papers_{timestamp}")
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    save_path = Path(save_dir)

    chatbot.append(["ä¸‹è½½è®ºæ–‡", f"æ­£åœ¨ä¸‹è½½{'arXiv' if id_type == 'arxiv' else 'DOI'} {paper_id} çš„è®ºæ–‡..."])
    update_ui(chatbot=chatbot, history=history)

    pdf_path = None
    try:
        if id_type == 'arxiv':
            formatted_id = format_arxiv_id(paper_id)
            paper_result = get_arxiv_paper(formatted_id)
            if not paper_result:
                chatbot.append(["ä¸‹è½½å¤±è´¥", f"æœªæ‰¾åˆ°arXivè®ºæ–‡: {paper_id}"])
                update_ui(chatbot=chatbot, history=history)
                return None
            filename = f"arxiv_{paper_id.replace('/', '_')}.pdf"
            pdf_path = str(save_path / filename)
            paper_result.download_pdf(filename=pdf_path)
        else:
            sci_hub = SciHub(doi=paper_id, path=save_path)
            pdf_path = sci_hub.fetch()

        if pdf_path and os.path.exists(pdf_path):
            promote_file_to_downloadzone(pdf_path, chatbot=chatbot)
            chatbot.append(["ä¸‹è½½æˆåŠŸ", f"å·²æˆåŠŸä¸‹è½½è®ºæ–‡: {os.path.basename(pdf_path)}"])
            update_ui(chatbot=chatbot, history=history)
            return pdf_path
        chatbot.append(["ä¸‹è½½å¤±è´¥", f"è®ºæ–‡ä¸‹è½½å¤±è´¥: {paper_id}"])
        update_ui(chatbot=chatbot, history=history)
        return None
    except Exception as e:
        chatbot.append(["ä¸‹è½½é”™è¯¯", f"ä¸‹è½½è®ºæ–‡æ—¶å‡ºé”™: {str(e)}"])
        update_ui(chatbot=chatbot, history=history)
        return None


@CatchException
def æ‰¹é‡è®ºæ–‡ç²¾è¯»(txt: str, llm_kwargs: Dict, plugin_kwargs: Dict, chatbot: List,
            history: List, system_prompt: str, user_request: str):
    """ä¸»å‡½æ•° - æ‰¹é‡è®ºæ–‡ç²¾è¯»"""
    chatbot.append([
        "å‡½æ•°æ’ä»¶åŠŸèƒ½åŠä½¿ç”¨æ–¹å¼",
        (
            "æ‰¹é‡è®ºæ–‡ç²¾è¯»ï¼šå¯¹å¤šä¸ªè®ºæ–‡æ–‡ä»¶è¿›è¡Œæ·±å…¥é˜…è¯»ä¸æŠ€æœ¯å¤ç›˜ï¼Œè¾“å‡ºé¢å‘å®ç°ä¸å¤ç°çš„æ·±åº¦æŠ¥å‘Šã€‚\n\n"
            "ä½¿ç”¨æ–¹å¼ï¼š\n1) è¾“å…¥åŒ…å«å¤šä¸ªPDFçš„æ–‡ä»¶å¤¹è·¯å¾„ï¼›\n2) æˆ–è¾“å…¥å¤šä¸ªè®ºæ–‡IDï¼ˆDOIæˆ–arXivï¼‰ï¼Œç”¨é€—å·åˆ†éš”ï¼›\n3) ç‚¹å‡»å¼€å§‹ã€‚"
        ),
    ])
    yield from update_ui(chatbot=chatbot, history=history)

    paper_files: List[str] = []

    if ',' in txt:
        paper_ids = [pid.strip() for pid in txt.split(',') if pid.strip()]
        chatbot.append(["æ£€æµ‹åˆ°å¤šä¸ªè®ºæ–‡ID", f"æ£€æµ‹åˆ° {len(paper_ids)} ä¸ªè®ºæ–‡IDï¼Œå‡†å¤‡æ‰¹é‡ä¸‹è½½..."])
        yield from update_ui(chatbot=chatbot, history=history)
        for i, pid in enumerate(paper_ids):
            paper_info = extract_paper_id(pid)
            if paper_info:
                chatbot.append([f"ä¸‹è½½è®ºæ–‡ {i+1}/{len(paper_ids)}", f"æ­£åœ¨ä¸‹è½½ {'arXiv' if paper_info[0] == 'arxiv' else 'DOI'} ID: {paper_info[1]}..."])
                yield from update_ui(chatbot=chatbot, history=history)
                p = _download_paper_by_id(paper_info, chatbot, history)
                if p:
                    paper_files.append(p)
                else:
                    chatbot.append(["ä¸‹è½½å¤±è´¥", f"æ— æ³•ä¸‹è½½è®ºæ–‡: {pid}"])
                    yield from update_ui(chatbot=chatbot, history=history)
            else:
                chatbot.append(["IDæ ¼å¼é”™è¯¯", f"æ— æ³•è¯†åˆ«è®ºæ–‡IDæ ¼å¼: {pid}"])
                yield from update_ui(chatbot=chatbot, history=history)
    else:
        paper_info = extract_paper_id(txt)
        if paper_info:
            chatbot.append(["æ£€æµ‹åˆ°è®ºæ–‡ID", f"æ£€æµ‹åˆ°{'arXiv' if paper_info[0] == 'arxiv' else 'DOI'} ID: {paper_info[1]}ï¼Œå‡†å¤‡ä¸‹è½½è®ºæ–‡..."])
            yield from update_ui(chatbot=chatbot, history=history)
            p = _download_paper_by_id(paper_info, chatbot, history)
            if p:
                paper_files.append(p)
            else:
                report_exception(chatbot, history, a="ä¸‹è½½è®ºæ–‡å¤±è´¥", b=f"æ— æ³•ä¸‹è½½{'arXiv' if paper_info[0] == 'arxiv' else 'DOI'}è®ºæ–‡: {paper_info[1]}")
                yield from update_ui(chatbot=chatbot, history=history)
                return
        else:
            if not os.path.exists(txt):
                report_exception(chatbot, history, a=f"æ‰¹é‡ç²¾è¯»è®ºæ–‡: {txt}", b=f"æ‰¾ä¸åˆ°æ–‡ä»¶æˆ–æ— æƒè®¿é—®: {txt}")
                yield from update_ui(chatbot=chatbot, history=history)
                return
            user_name = chatbot.get_user()
            validate_path_safety(txt, user_name)
            paper_files = _find_paper_files(txt)
            if not paper_files:
                report_exception(chatbot, history, a="æ‰¹é‡ç²¾è¯»è®ºæ–‡", b=f"åœ¨è·¯å¾„ {txt} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„è®ºæ–‡æ–‡ä»¶")
                yield from update_ui(chatbot=chatbot, history=history)
                return

    yield from update_ui(chatbot=chatbot, history=history)

    if not paper_files:
        chatbot.append(["é”™è¯¯", "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯åˆ†æçš„è®ºæ–‡æ–‡ä»¶"])
        yield from update_ui(chatbot=chatbot, history=history)
        return

    chatbot.append(["å¼€å§‹æ‰¹é‡ç²¾è¯»", f"æ‰¾åˆ° {len(paper_files)} ç¯‡è®ºæ–‡ï¼Œå¼€å§‹æ·±å…¥åˆ†æ..."])
    yield from update_ui(chatbot=chatbot, history=history)

    analyzer = BatchPaperDetailAnalyzer(llm_kwargs, plugin_kwargs, chatbot, history, system_prompt)

    successes: List[str] = []
    failures: List[str] = []

    for i, paper_file in enumerate(paper_files):
        try:
            chatbot.append([f"ç²¾è¯»è®ºæ–‡ {i+1}/{len(paper_files)}", f"æ­£åœ¨ç²¾è¯»: {os.path.basename(paper_file)}"])
            yield from update_ui(chatbot=chatbot, history=history)
            outfile = yield from analyzer.analyze_paper(paper_file)
            if outfile:
                successes.append(outfile)
                chatbot.append([f"å®Œæˆè®ºæ–‡ {i+1}/{len(paper_files)}", f"æˆåŠŸç”ŸæˆæŠ¥å‘Š: {os.path.basename(outfile)}"])
            else:
                failures.append(os.path.basename(paper_file))
                chatbot.append([f"å¤±è´¥è®ºæ–‡ {i+1}/{len(paper_files)}", f"åˆ†æå¤±è´¥: {os.path.basename(paper_file)}"])
            yield from update_ui(chatbot=chatbot, history=history)
        except Exception as e:
            failures.append(os.path.basename(paper_file))
            chatbot.append([f"é”™è¯¯è®ºæ–‡ {i+1}/{len(paper_files)}", f"åˆ†æå‡ºé”™: {os.path.basename(paper_file)} - {str(e)}"])
            yield from update_ui(chatbot=chatbot, history=history)

    summary = "æ‰¹é‡ç²¾è¯»å®Œæˆï¼\n\n"
    summary += "ğŸ“Š åˆ†æç»Ÿè®¡ï¼š\n"
    summary += f"- æ€»è®ºæ–‡æ•°ï¼š{len(paper_files)}\n"
    summary += f"- æˆåŠŸåˆ†æï¼š{len(successes)}\n"
    summary += f"- åˆ†æå¤±è´¥ï¼š{len(failures)}\n\n"
    if successes:
        summary += "âœ… æˆåŠŸç”ŸæˆæŠ¥å‘Šï¼š\n"
        for p in successes:
            summary += f"- {os.path.basename(p)}\n"
    if failures:
        summary += "\nâŒ åˆ†æå¤±è´¥çš„è®ºæ–‡ï¼š\n"
        for name in failures:
            summary += f"- {name}\n"

    chatbot.append(["æ‰¹é‡ç²¾è¯»å®Œæˆ", summary])
    yield from update_ui(chatbot=chatbot, history=history)


