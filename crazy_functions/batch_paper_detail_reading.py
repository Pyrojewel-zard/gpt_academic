import os
import re
import time
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass
from typing import Dict, List, Generator

from crazy_functions.crazy_utils import request_gpt_model_in_new_thread_with_ui_alive
from toolbox import update_ui, promote_file_to_downloadzone, write_history_to_file, CatchException, report_exception
from shared_utils.fastapi_server import validate_path_safety
from crazy_functions.paper_fns.paper_download import extract_paper_id, get_arxiv_paper, format_arxiv_id


@dataclass
class DeepReadQuestion:
    """è®ºæ–‡ç²¾è¯»é—®é¢˜é¡¹"""
    id: str
    question: str
    importance: int
    description: str


class BatchPaperDetailAnalyzer:
    """æ‰¹é‡è®ºæ–‡ç²¾è¯»åˆ†æå™¨"""

    def __init__(self, llm_kwargs: Dict, plugin_kwargs: Dict, chatbot: List, history: List, system_prompt: str):
        self.llm_kwargs = llm_kwargs
        self.plugin_kwargs = plugin_kwargs
        self.chatbot = chatbot
        self.history = history
        self.system_prompt = system_prompt
        self.paper_content = ""
        self.results: Dict[str, str] = {}
        self.paper_file_path: str = None
        self.secondary_category: str = None

        # ç²¾è¯»ç»´åº¦ï¼ˆæ›´æ·±å…¥çš„æŠ€æœ¯ç»†èŠ‚ã€å¯å¤ç°æ€§ã€ç†è®ºä¾æ®ç­‰ï¼‰
        self.questions: List[DeepReadQuestion] = [
            DeepReadQuestion(
                id="problem_statement_and_contributions",
                description="é—®é¢˜å®šä¹‰ä¸è´¡çŒ®",
                importance=5,
                question=(
                    "è¯·ä¸¥è°¨æ¦‚è¿°è®ºæ–‡è¦è§£å†³çš„é—®é¢˜ã€å½¢å¼åŒ–å®šä¹‰ä¸è¾¹ç•Œæ¡ä»¶ï¼›"
                    "æ¢³ç†è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰ï¼Œå¹¶é€æ¡è¯´æ˜å…¶ä¸å·²æœ‰å·¥ä½œçš„æœ¬è´¨åŒºåˆ«ã€‚"
                ),
            ),
            DeepReadQuestion(
                id="method_derivation_and_theory",
                description="æ–¹æ³•æ¨å¯¼ä¸ç†è®ºä¿éšœ",
                importance=5,
                question=(
                    "è¯·å¯¹æ ¸å¿ƒæ–¹æ³•è¿›è¡Œå…¬å¼çº§ç²¾è¯»ï¼šç»™å‡ºå…³é”®ç¬¦å·å®šä¹‰ã€æŸå¤±å‡½æ•°/ç›®æ ‡å‡½æ•°ï¼Œ"
                    "æ¨å¯¼ä¸»å‘½é¢˜ï¼ˆæˆ–å®šç†/å¼•ç†ï¼‰çš„å…³é”®æ­¥éª¤ä¸å¿…è¦å‡è®¾ï¼›è‹¥ç»™å‡ºæ”¶æ•›æ€§/å¤æ‚åº¦/ä¸€è‡´æ€§ç»“è®ºï¼Œ"
                    "è¯·æ˜ç¡®å…¶é€‚ç”¨å‰æä¸å±€é™ã€‚å¿…è¦æ—¶ç»™å‡ºç®€åŒ–ç‰ˆç­‰ä»·è¡¨è¿°æ–¹ä¾¿å®ç°ã€‚"
                ),
            ),
            DeepReadQuestion(
                id="assumptions_and_threats",
                description="å…³é”®å‡è®¾ä¸æœ‰æ•ˆæ€§å¨èƒ",
                importance=4,
                question=(
                    "åˆ—å‡ºè®ºæ–‡æ˜¾å¼æˆ–éšå«å‡è®¾ï¼ˆæ•°æ®åˆ†å¸ƒã€ç‹¬ç«‹æ€§ã€å¯è·å–æ€§ã€ç¡¬ä»¶æ¡ä»¶ç­‰ï¼‰ï¼Œ"
                    "åˆ†æè¿™äº›å‡è®¾ä¸ç°å®å·®è·å¯èƒ½å¸¦æ¥çš„å¤±æ•ˆæƒ…å½¢ï¼›"
                    "è¡¥å……ä½œè€…æœªå……åˆ†è®¨è®ºä½†å¯èƒ½é‡è¦çš„æœ‰æ•ˆæ€§å¨èƒã€‚"
                ),
            ),
            DeepReadQuestion(
                id="experiments_reproduction_plan",
                description="å®éªŒè®¾è®¡ä¸å¯é‡å¤æ€§è¦ç‚¹ï¼ˆä¸å«ä»£ç ï¼‰",
                importance=5,
                question=(
                    "è¯·æ€»ç»“å®éªŒè®¾è®¡ä¸å¯é‡å¤æ€§å…³é”®è¦ç‚¹ï¼ˆä¸æ¶‰åŠä»»ä½•ä»£ç /å‘½ä»¤ï¼‰ï¼šæ•°æ®é›†ç‰ˆæœ¬ä¸è·å–æ–¹å¼ã€é¢„å¤„ç†æµç¨‹ã€"
                    "æ ¸å¿ƒè¶…å‚æ•°åç§°ï¼ˆæ— éœ€ç»™å‡ºå…·ä½“æ•°å€¼ï¼‰ã€è®­ç»ƒä¸è¯„æµ‹æµç¨‹æ¦‚è¿°ã€å¯¹æ¯”æ–¹æ³•ä¸æ¶ˆèå®éªŒè®¾è®¡ã€"
                    "èµ„æºéœ€æ±‚é‡çº§ï¼ˆå¦‚GPU/CPU/æ—¶é•¿ï¼‰ã€‚"
                ),
            ),
            DeepReadQuestion(
                id="dataset_and_license",
                description="æ•°æ®é›†ä¸è®¸å¯",
                importance=3,
                question=(
                    "è¯·ç½—åˆ—è®ºæ–‡ç”¨åˆ°çš„æ•°æ®/æ¨¡å‹èµ„æºåŠå…¶è®¸å¯è¯ï¼›è¯´æ˜æ˜¯å¦å­˜åœ¨é—­æºä¾èµ–æˆ–ä¸å¯è·å¾—èµ„æºï¼Œ"
                    "å¹¶æä¾›ç›¸åº”æ›¿ä»£æ–¹æ¡ˆå»ºè®®ã€‚"
                ),
            ),
            DeepReadQuestion(
                id="results_and_ablations",
                description="ç»“æœå¤æ ¸ä¸æ¶ˆèæ´å¯Ÿ",
                importance=4,
                question=(
                    "è¯·å¯¹ä¸»è¦ç»“æœè¿›è¡Œè¦ç‚¹å¤æ ¸ï¼šæ˜¯å¦æ»¡è¶³ç»Ÿè®¡æ˜¾è‘—æ€§ã€æ˜¯å¦æœ‰æ–¹å·®æŠ¥å‘Šï¼›"
                    "ä»æ¶ˆèå®éªŒä¸­æç‚¼æœ€å…³é”®çš„å½±å“å› ç´ ä¸äº¤äº’æ•ˆåº”ï¼ŒæŒ‡å‡ºå¯èƒ½çš„è¯¯å¯¼æ€§ç»“è®ºã€‚"
                ),
            ),
            DeepReadQuestion(
                id="limitations_future_and_impact",
                description="é™åˆ¶ã€æœªæ¥æ–¹å‘ä¸å½±å“",
                importance=3,
                question=(
                    "æ€»ç»“è®ºæ–‡çš„ä¸»è¦å±€é™ã€æ½œåœ¨é£é™©æˆ–ä¼¦ç†é—®é¢˜ï¼›æå‡ºå¯æ“ä½œçš„æ”¹è¿›æ–¹å‘ä¸åç»­ç ”ç©¶å‡è®¾ï¼›"
                    "è¯„ä¼°å…¶å¯¹å­¦æœ¯ä¸äº§ä¸šçš„ä¸­çŸ­æœŸå½±å“è·¯å¾„ã€‚"
                ),
            ),
            DeepReadQuestion(
                id="mermaid_flowcharts",
                description="æ ¸å¿ƒæµç¨‹å›¾ï¼ˆMermaidï¼‰",
                importance=4,
                question=(
                    "è¯·ç»™å‡ºä¸å®ç°å¼ºç›¸å…³çš„æµç¨‹å›¾ï¼ˆå¯å¤šä¸ªæ¨¡å—ï¼‰\n"
                    "è¦æ±‚ï¼š\n"
                    "1) æ¯ä¸ªæµç¨‹å›¾ä½¿ç”¨ Mermaid è¯­æ³•ï¼Œä»£ç å—éœ€ä»¥ ```mermaid å¼€å§‹ï¼Œä»¥ ``` ç»“æŸï¼›\n"
                    "2) æ¨èä½¿ç”¨ flowchart TD æˆ– LRï¼ŒèŠ‚ç‚¹éœ€æ¦‚æ‹¬å…³é”®æ­¥éª¤/å­æ¨¡å—ï¼ŒåŒ…å«ä¸»è¦æ•°æ®æµä¸å…³é”®åˆ†æ”¯/åˆ¤å®šï¼›\n"
                    "3) æ¯ä¸ªæµç¨‹å›¾å‰ä»¥ä¸€å¥è¯æ ‡æ˜æ¨¡å—/é˜¶æ®µåç§°ï¼Œä¾‹å¦‚ï¼šæ¨¡å—ï¼šè®­ç»ƒé˜¶æ®µï¼›\n"
                    "4) ä»…èšç„¦æ ¸å¿ƒé€»è¾‘ï¼Œé¿å…è¿‡åº¦ç»†èŠ‚ï¼›\n"
                    "5) è‹¥åªæœ‰å•ä¸€æ ¸å¿ƒæµç¨‹ï¼Œä»…è¾“å‡ºä¸€ä¸ªæµç¨‹å›¾ï¼›\n"
                    "6) æ ¼å¼çº¦æŸï¼š\n"
                    "   - èŠ‚ç‚¹åç”¨å¼•å·åŒ…è£¹ï¼Œå¦‚ [\"èŠ‚ç‚¹å\"] æˆ– (\"èŠ‚ç‚¹å\")ï¼›\n"
                    "   - ç®­å¤´æ ‡ç­¾é‡‡ç”¨ |\"æ ‡ç­¾å\"| å½¢å¼ï¼Œä¸” | ä¸ \" ä¹‹é—´ä¸è¦æœ‰ç©ºæ ¼ï¼›\n"
                    "   - æ ¹æ®é€»è¾‘é€‰æ‹© flowchart LRï¼ˆä»å·¦åˆ°å³ï¼‰æˆ– flowchart TDï¼ˆä»ä¸Šåˆ°ä¸‹ï¼‰ã€‚\n"
                    "7) ç¤ºä¾‹ï¼š\n"
                    "```mermaid\n"
                    "flowchart TD\n"
                    "    A[\"è¾“å…¥\"] --> B(\"å¤„ç†\")\n"
                    "    B --> C{\"æ˜¯å¦æ»¡è¶³æ¡ä»¶\"}\n"
                    "    C --> D[\"è¾“å‡º1\"]\n"
                    "    C --> |\"å¦\"| E[\"è¾“å‡º2\"]\n"
                    "```"
                ),
            ),
            DeepReadQuestion(
                id="exec_summary_md",
                description="ç²¾è¯»è¦ç‚¹æ‘˜è¦ï¼ˆMarkdownï¼‰",
                importance=5,
                question=(
                    "ç»™å‡ºæç®€ Markdown æ‘˜è¦ï¼ˆä¸åŒ…å«ä»»ä½•ä»£ç /å‘½ä»¤ï¼‰ï¼š\n"
                    "- ä¸€å¥è¯æ€»è¿°æ–¹æ³•ä¸ä½œç”¨\n"
                    "- ä¸‰åˆ°äº”æ¡æ–¹æ³•è¦ç‚¹ï¼ˆè¾“å…¥/æ­¥éª¤/è¾“å‡ºï¼‰\n"
                    "- ä¸‰åˆ°äº”æ¡å¤ç°è¦ç‚¹ï¼ˆæ•°æ®/è¶…å‚åç§°/èµ„æºé‡çº§/æ—¶é•¿ï¼‰"
                ),
            ),
        ]

        self.questions.sort(key=lambda q: q.importance, reverse=True)

    # ---------- å…³é”®è¯åº“å·¥å…·ï¼ˆä¸é€Ÿè¯»ç‰ˆä¸€è‡´ï¼‰ ----------
    def _get_keywords_db_path(self) -> str:
        return os.path.join(os.path.dirname(__file__), 'keywords.txt')

    def _load_keywords_db(self) -> List[str]:
        path = self._get_keywords_db_path()
        if os.path.exists(path):
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    return [line.strip() for line in f if line.strip()]
            except Exception:
                return []
        return []

    def _save_keywords_db(self, keywords: List[str]):
        path = self._get_keywords_db_path()
        try:
            with open(path, 'w', encoding='utf-8') as f:
                for kw in sorted(set(keywords), key=lambda x: x.lower()):
                    f.write(kw + '\n')
        except Exception:
            pass

    def _normalize_keyword(self, kw: str) -> str:
        kw = kw.strip()
        kw = re.sub(r'[\s\u3000]+', ' ', kw)
        kw = kw.strip().strip('.,;:')
        return kw.lower()

    def _find_similar_in_db(self, db: List[str], new_kw: str, threshold: float = 0.88) -> str:
        import difflib
        if not new_kw:
            return None
        candidates = difflib.get_close_matches(new_kw, [self._normalize_keyword(k) for k in db], n=1, cutoff=threshold)
        if candidates:
            norm = candidates[0]
            for k in db:
                if self._normalize_keyword(k) == norm:
                    return k
        return None

    def _merge_keywords_with_db(self, extracted_keywords: List[str]):
        db = self._load_keywords_db()
        canonical_list: List[str] = []
        for kw in extracted_keywords:
            clean = self._normalize_keyword(kw)
            if not clean:
                continue
            similar = self._find_similar_in_db(db, clean)
            if similar:
                if similar not in canonical_list:
                    canonical_list.append(similar)
            else:
                db.append(kw)
                if kw not in canonical_list:
                    canonical_list.append(kw)
        self._save_keywords_db(db)
        return canonical_list, db

    def _generate_yaml_header(self) -> Generator:
        """åŸºäºè®ºæ–‡å†…å®¹ä¸å·²å¾—åˆ†æï¼Œç”Ÿæˆ YAML Front Matter"""
        try:
            prompt = (
                "è¯·åŸºäºä»¥ä¸‹è®ºæ–‡å†…å®¹ä¸åˆ†æè¦ç‚¹ï¼Œæå–è®ºæ–‡æ ¸å¿ƒå…ƒä¿¡æ¯å¹¶è¾“å‡º YAML Front Matterï¼š\n\n"
                f"è®ºæ–‡å…¨æ–‡å†…å®¹ç‰‡æ®µï¼š\n{self.paper_content}\n\n"
                "è‹¥æœ‰å¯ç”¨çš„åˆ†æè¦ç‚¹ï¼š\n"
            )
            for q in self.questions:
                if q.id in self.results:
                    prompt += f"- {q.description}: {self.results[q.id][:400]}\n"

            prompt += (
                "\nä¸¥æ ¼è¾“å‡º YAMLï¼ˆä¸ä½¿ç”¨ä»£ç å—å›´æ ï¼‰ï¼Œå­—æ®µå¦‚ä¸‹ï¼š\n"
                "title: åŸæ–‡æ ‡é¢˜ï¼ˆå°½é‡è‹±æ–‡åŸé¢˜,æ ‡é¢˜éœ€è¦æœ‰å¼•å·åŒ…è£¹ï¼‰\n"
                "title_zh: ä¸­æ–‡æ ‡é¢˜ï¼ˆè‹¥å¯ï¼‰\n"
                "authors: [ä½œè€…è‹±æ–‡ååˆ—è¡¨]\n"
                "affiliation_zh: ç¬¬ä¸€ä½œè€…å•ä½ï¼ˆä¸­æ–‡ï¼‰\n"
                "keywords: [è‹±æ–‡å…³é”®è¯åˆ—è¡¨]\n"
                "urls: [è®ºæ–‡é“¾æ¥, Githubé“¾æ¥æˆ–None]\n"
                "doi: [DOIé“¾æ¥, None]\n"
                "journal_or_conference: [æœŸåˆŠæˆ–ä¼šè®®åç§°, None]\n"
                "year: [å¹´ä»½, None]\n"
                "source_code: [æºç é“¾æ¥, None]\n"
                "read_status: [å·²é˜…è¯», æœªé˜…è¯»]\n"
                "stars: [â­â­â­â­â­, â­â­â­â­, â­â­â­, â­â­, â­]\n"
                "ä»…è¾“å‡ºä»¥ --- å¼€å§‹ã€ä»¥ --- ç»“æŸçš„ YAML Front Matterï¼Œä¸è¦é™„åŠ å…¶ä»–æ–‡æœ¬ã€‚é»˜è®¤starsä¸ºâ­â­â­ï¼Œread_statusä¸ºæœªé˜…è¯»ã€‚"
            )

            yaml_str = yield from request_gpt_model_in_new_thread_with_ui_alive(
                inputs=prompt,
                inputs_show_user="ç”Ÿæˆè®ºæ–‡æ ¸å¿ƒä¿¡æ¯ YAML å¤´",
                llm_kwargs=self.llm_kwargs,
                chatbot=self.chatbot,
                history=[],
                sys_prompt=(
                    "ä½ æ˜¯è®ºæ–‡ä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚è¯·ä»…è¾“å‡º YAML Front Matterï¼Œ"
                    "é”®åå›ºå®šä¸”é¡ºåºä¸é™ï¼Œæ³¨æ„ authors/keywords/urls åº”ä¸ºåˆ—è¡¨ã€‚"
                )
            )

            if isinstance(yaml_str, str) and yaml_str.strip().startswith("---") and yaml_str.strip().endswith("---"):
                text = yaml_str.strip()
                m = re.search(r"^keywords:\s*\[(.*?)\]\s*$", text, flags=re.MULTILINE)
                if m:
                    inner = m.group(1).strip()
                    raw_list = [x.strip().strip('\"\'\'') for x in inner.split(',') if x.strip()]
                    merged, _ = self._merge_keywords_with_db(raw_list)
                    rebuilt = ', '.join([f'"{k}"' for k in merged])
                    text = re.sub(r"^keywords:\s*\[(.*?)\]\s*$", f"keywords: [{rebuilt}]", text, flags=re.MULTILINE)
                # å°†æœ¬æ¬¡ç²¾è¯»ä½¿ç”¨çš„åˆ†ç±»åŒ–æç¤ºï¼ˆpromptsï¼‰å½’æ¡£åˆ° YAML å¤´
                try:
                    prompts_lines = ["deep_read_prompts:"]
                    for q in self.questions:
                        desc = q.description.replace('"', '\\"')
                        prompts_lines.append(f"  - id: {q.id}")
                        prompts_lines.append(f"    description: \"{desc}\"")
                        prompts_lines.append(f"    importance: {q.importance}")
                    prompts_block = "\n".join(prompts_lines) + "\n"
                    if text.endswith("---"):
                        text = text[:-3].rstrip() + "\n" + prompts_block + "---"
                except Exception:
                    pass
                # æ³¨å…¥â€œå½’å±â€äºŒçº§åˆ†ç±»åˆ° YAML å¤´ï¼ˆä»…å†™å…¥åˆ†ç±»è·¯å¾„æœ¬èº«ï¼Œå¹¶ç”¨å¼•å·åŒ…è£¹ï¼‰
                try:
                    if getattr(self, 'secondary_category', None):
                        escaped = self.secondary_category.replace('"', '\\"')
                        if text.endswith("---"):
                            text = text[:-3].rstrip() + f"\nsecondary_category: \"{escaped}\"\n---"
                except Exception:
                    pass
                # åŸºäº stars æ¨æ–­â€œè®ºæ–‡é‡è¦ç¨‹åº¦â€å¹¶å†™å…¥ï¼ˆä¸­æ–‡ç­‰çº§ï¼Œå€¼ç”¨å¼•å·åŒ…è£¹ï¼‰
                try:
                    m_star = re.search(r"^stars:\s*\[(.*?)\]\s*$", text, flags=re.MULTILINE)
                    level = None
                    if m_star:
                        inner = m_star.group(1)
                        m_seq = re.search(r"(â­{1,5})", inner)
                        if m_seq:
                            count = len(m_seq.group(1))
                            if count >= 5:
                                level = "å¼ºçƒˆæ¨è"
                            elif count == 4:
                                level = "æ¨è"
                            elif count == 3:
                                level = "ä¸€èˆ¬"
                            elif count == 2:
                                level = "è°¨æ…"
                            elif count == 1:
                                level = "ä¸æ¨è"
                    if not level:
                        level = "ä¸€èˆ¬"
                    if text.endswith("---"):
                        text = text[:-3].rstrip() + f"\nè®ºæ–‡é‡è¦ç¨‹åº¦: \"{level}\"\n---"
                except Exception:
                    pass
                return text
            return None
        except Exception as e:
            self.chatbot.append(["è­¦å‘Š", f"ç”Ÿæˆ YAML å¤´å¤±è´¥: {str(e)}"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return None

    def _load_paper(self, paper_path: str) -> Generator:
        from crazy_functions.doc_fns.text_content_loader import TextContentLoader
        yield from update_ui(chatbot=self.chatbot, history=self.history)
        self.paper_file_path = paper_path
        loader = TextContentLoader(self.chatbot, self.history)
        yield from loader.execute_single_file(paper_path)
        if len(self.history) >= 2 and self.history[-2]:
            self.paper_content = self.history[-2]
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return True
        self.chatbot.append(["é”™è¯¯", "æ— æ³•è¯»å–è®ºæ–‡å†…å®¹ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"])
        yield from update_ui(chatbot=self.chatbot, history=self.history)
        return False

    def _ask(self, q: DeepReadQuestion) -> Generator:
        try:
            prompt = (
                "è¯·åŸºäºä»¥ä¸‹è®ºæ–‡å†…å®¹è¿›è¡Œç²¾è¯»åˆ†æï¼Œå¹¶ä¸¥æ ¼å›´ç»•é—®é¢˜ä½œç­”ã€‚\n"
                "æ³¨æ„ï¼šè¯·é¿å…æä¾›ä»»ä½•ä»£ç ã€ä¼ªä»£ç ã€å‘½ä»¤è¡Œæˆ–å…·ä½“å®ç°ç»†èŠ‚ï¼›"
                "è‹¥è¾“å‡ºæµç¨‹å›¾ï¼Œé¡»ä½¿ç”¨ ```mermaid ä»£ç å—ï¼Œå…¶ä½™å›ç­”ä¿æŒè‡ªç„¶è¯­è¨€ã€‚\n\n"
                
                f"è®ºæ–‡å†…å®¹ï¼š\n{self.paper_content}\n\n"
                f"é—®é¢˜ï¼š{q.question}"
            )
            resp = yield from request_gpt_model_in_new_thread_with_ui_alive(
                inputs=prompt,
                inputs_show_user=q.question,
                llm_kwargs=self.llm_kwargs,
                chatbot=self.chatbot,
                history=[],
                sys_prompt=(
                    "ä½ æ˜¯èµ„æ·±ç ”ç©¶å‘˜ï¼Œè¾“å‡ºä»¥æ¦‚å¿µä¸æ–¹æ³•è®ºå±‚é¢ä¸ºä¸»ï¼Œä¸åŒ…å«ä»»ä½•ä»£ç æˆ–ä¼ªä»£ç ã€‚"
                    "å¦‚æ¶‰åŠMermaidæµç¨‹å›¾ï¼Œè¯·ä½¿ç”¨```mermaid åŒ…è£¹å¹¶ä¿æŒè¯­æ³•æ­£ç¡®ï¼Œå…¶ä½™ä¿æŒè‡ªç„¶è¯­è¨€ã€‚"
                ),
            )
            if resp:
                self.results[q.id] = resp
                return True
            return False
        except Exception as e:
            self.chatbot.append(["é”™è¯¯", f"ç²¾è¯»é—®é¢˜åˆ†æå¤±è´¥: {str(e)}"])
            yield from update_ui(chatbot=self.chatbot, history=self.history)
            return False

    def _generate_report(self) -> Generator:
        self.chatbot.append(["ç”ŸæˆæŠ¥å‘Š", "æ­£åœ¨æ•´åˆç²¾è¯»ç»“æœï¼Œç”Ÿæˆæ·±åº¦æŠ€æœ¯æŠ¥å‘Š..."])
        yield from update_ui(chatbot=self.chatbot, history=self.history)

        prompt = (
            "è¯·å°†ä»¥ä¸‹ç²¾è¯»åˆ†ææ•´ç†ä¸ºå®Œæ•´çš„æŠ€æœ¯æŠ¥å‘Šï¼Œå±‚æ¬¡æ¸…æ™°ï¼Œçªå‡ºæ ¸å¿ƒæ€æƒ³ä¸å®éªŒè®¾è®¡è¦ç‚¹ï¼Œ"
            "ä¸åŒ…å«ä»»ä½•ä»£ç /ä¼ªä»£ç /å‘½ä»¤è¡Œã€‚è‹¥åŒ…å«```mermaid ä»£ç å—ï¼Œè¯·åŸæ ·ä¿ç•™ã€‚"
        )
        for q in self.questions:
            if q.id in self.results:
                prompt += f"\n\n[{q.description}]\n{self.results[q.id]}"

        resp = yield from request_gpt_model_in_new_thread_with_ui_alive(
            inputs=prompt,
            inputs_show_user="ç”Ÿæˆè®ºæ–‡ç²¾è¯»æŠ€æœ¯æŠ¥å‘Š",
            llm_kwargs=self.llm_kwargs,
            chatbot=self.chatbot,
            history=[],
            sys_prompt=(
                "ä»¥å·¥ç¨‹å¤ç°ä¸ºç›®æ ‡ç»„ç»‡æŠ¥å‘Šï¼šèƒŒæ™¯æç®€ï¼Œæ–¹æ³•ä¸å®ç°ç»†èŠ‚å……åˆ†ï¼Œ"
                "æ¡ç†åˆ†æ˜ï¼ŒåŒ…å«å¿…è¦çš„æ¸…å•ä¸æ­¥éª¤ã€‚"
            ),
        )
        return resp or "æŠ¥å‘Šç”Ÿæˆå¤±è´¥"

    def _extract_secondary_category(self, report: str) -> str:
        """ä»æŠ¥å‘Šä¸­æå–â€œå½’å±ï¼šâ€åçš„äºŒçº§åˆ†ç±»æ–‡æœ¬ï¼Œåªä¿ç•™ç±»ä¼¼
        â€œ7. æœºå™¨å­¦ä¹ è¾…åŠ©è®¾è®¡ (ML-Aided RF Design) -> ç³»ç»Ÿçº§å»ºæ¨¡ä¸å¿«é€Ÿç»¼åˆâ€ã€‚
        """
        try:
            if not isinstance(report, str):
                return None
            m = re.search(r"^å½’å±ï¼š\s*([^\r\n]+)", report, flags=re.MULTILINE)
            if not m:
                return None
            category_line = m.group(1).strip()
            category_line = re.sub(r"[\s\u3000]+$", "", category_line)
            return category_line if category_line else None
        except Exception:
            return None

    def save_report(self, report: str) -> str:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        pdf_basename = "æœªçŸ¥è®ºæ–‡"
        if self.paper_file_path and os.path.exists(self.paper_file_path):
            pdf_basename = os.path.splitext(os.path.basename(self.paper_file_path))[0]
            pdf_basename = re.sub(r'[^\w\u4e00-\u9fff]', '_', pdf_basename)
            if len(pdf_basename) > 50:
                pdf_basename = pdf_basename[:50]

        parts: List[str] = []
        parts.append(f"è®ºæ–‡ç²¾è¯»æŠ€æœ¯æŠ¥å‘Š\n\n{report}")
        # ä¼˜å…ˆè¿½åŠ æ‰§è¡Œçº§æ‘˜è¦ä¸æµç¨‹å›¾
        if "exec_summary_md" in self.results:
            parts.append(f"\n\n## æ‰§è¡Œçº§æ‘˜è¦\n\n{self.results['exec_summary_md']}")
        if "mermaid_flowcharts" in self.results:
            parts.append(f"\n\n## æ ¸å¿ƒæµç¨‹å›¾\n\n{self.results['mermaid_flowcharts']}")
        # è¿½åŠ å…¶ä½™ç»´åº¦
        for q in self.questions:
            if q.id in self.results and q.id not in {"exec_summary_md", "mermaid_flowcharts"}:
                parts.append(f"\n\n## {q.description}\n\n{self.results[q.id]}")

        content = "".join(parts)
        if hasattr(self, 'yaml_header') and self.yaml_header:
            content = f"{self.yaml_header}\n\n" + content
        result_file = write_history_to_file(
            history=[content],
            file_basename=f"{timestamp}_{pdf_basename}_ç²¾è¯»æŠ¥å‘Š.md",
        )
        if result_file and os.path.exists(result_file):
            promote_file_to_downloadzone(result_file, chatbot=self.chatbot)
            return result_file
        return None

    def analyze_paper(self, paper_path: str) -> Generator:
        ok = yield from self._load_paper(paper_path)
        if not ok:
            return None
        for q in self.questions:
            yield from self._ask(q)
        report = yield from self._generate_report()
        # ä»æŠ¥å‘Šä¸­æå–äºŒçº§åˆ†ç±»å½’å±
        self.secondary_category = self._extract_secondary_category(report)
        # ç”Ÿæˆ YAML å¤´
        self.yaml_header = yield from self._generate_yaml_header()
        saved = self.save_report(report)
        return saved


def _find_paper_files(path: str) -> List[str]:
    files: List[str] = []
    if os.path.isfile(path):
        ext = os.path.splitext(path)[1].lower()
        if ext in [".pdf", ".docx", ".doc", ".txt", ".md", ".tex"]:
            files.append(path)
        return files
    if os.path.isdir(path):
        exts = [".pdf", ".docx", ".doc", ".txt", ".md", ".tex"]
        for root, _dirs, fnames in os.walk(path):
            for fname in fnames:
                fpath = os.path.join(root, fname)
                if os.path.splitext(fname)[1].lower() in exts:
                    files.append(fpath)
    return files


def _download_paper_by_id(paper_info, chatbot, history) -> str:
    from crazy_functions.review_fns.data_sources.scihub_source import SciHub
    id_type, paper_id = paper_info

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    user_name = chatbot.get_user() if hasattr(chatbot, 'get_user') else "default"
    from toolbox import get_log_folder, get_user
    base_save_dir = get_log_folder(get_user(chatbot), plugin_name='paper_download')
    save_dir = os.path.join(base_save_dir, f"papers_{timestamp}")
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    save_path = Path(save_dir)

    chatbot.append(["ä¸‹è½½è®ºæ–‡", f"æ­£åœ¨ä¸‹è½½{'arXiv' if id_type == 'arxiv' else 'DOI'} {paper_id} çš„è®ºæ–‡..."])
    update_ui(chatbot=chatbot, history=history)

    pdf_path = None
    try:
        if id_type == 'arxiv':
            formatted_id = format_arxiv_id(paper_id)
            paper_result = get_arxiv_paper(formatted_id)
            if not paper_result:
                chatbot.append(["ä¸‹è½½å¤±è´¥", f"æœªæ‰¾åˆ°arXivè®ºæ–‡: {paper_id}"])
                update_ui(chatbot=chatbot, history=history)
                return None
            filename = f"arxiv_{paper_id.replace('/', '_')}.pdf"
            pdf_path = str(save_path / filename)
            paper_result.download_pdf(filename=pdf_path)
        else:
            sci_hub = SciHub(doi=paper_id, path=save_path)
            pdf_path = sci_hub.fetch()

        if pdf_path and os.path.exists(pdf_path):
            promote_file_to_downloadzone(pdf_path, chatbot=chatbot)
            chatbot.append(["ä¸‹è½½æˆåŠŸ", f"å·²æˆåŠŸä¸‹è½½è®ºæ–‡: {os.path.basename(pdf_path)}"])
            update_ui(chatbot=chatbot, history=history)
            return pdf_path
        chatbot.append(["ä¸‹è½½å¤±è´¥", f"è®ºæ–‡ä¸‹è½½å¤±è´¥: {paper_id}"])
        update_ui(chatbot=chatbot, history=history)
        return None
    except Exception as e:
        chatbot.append(["ä¸‹è½½é”™è¯¯", f"ä¸‹è½½è®ºæ–‡æ—¶å‡ºé”™: {str(e)}"])
        update_ui(chatbot=chatbot, history=history)
        return None


@CatchException
def æ‰¹é‡è®ºæ–‡ç²¾è¯»(txt: str, llm_kwargs: Dict, plugin_kwargs: Dict, chatbot: List,
            history: List, system_prompt: str, user_request: str):
    """ä¸»å‡½æ•° - æ‰¹é‡è®ºæ–‡ç²¾è¯»"""
    chatbot.append([
        "å‡½æ•°æ’ä»¶åŠŸèƒ½åŠä½¿ç”¨æ–¹å¼",
        (
            "æ‰¹é‡è®ºæ–‡ç²¾è¯»ï¼šå¯¹å¤šä¸ªè®ºæ–‡æ–‡ä»¶è¿›è¡Œæ·±å…¥é˜…è¯»ä¸æŠ€æœ¯å¤ç›˜ï¼Œè¾“å‡ºé¢å‘å®ç°ä¸å¤ç°çš„æ·±åº¦æŠ¥å‘Šã€‚\n\n"
            "ä½¿ç”¨æ–¹å¼ï¼š\n1) è¾“å…¥åŒ…å«å¤šä¸ªPDFçš„æ–‡ä»¶å¤¹è·¯å¾„ï¼›\n2) æˆ–è¾“å…¥å¤šä¸ªè®ºæ–‡IDï¼ˆDOIæˆ–arXivï¼‰ï¼Œç”¨é€—å·åˆ†éš”ï¼›\n3) ç‚¹å‡»å¼€å§‹ã€‚\n\n"
            "æ³¨æ„äº‹é¡¹ï¼š\n- è‹¥éœ€è¦è¾“å‡ºå…¬å¼ï¼Œè¯·ä½¿ç”¨ LaTeX æ•°å­¦æ ¼å¼ï¼šè¡Œå†…å…¬å¼ç”¨ $...$ï¼Œè¡Œé—´å…¬å¼ç”¨ $$...$$ã€‚"
        ),
    ])
    yield from update_ui(chatbot=chatbot, history=history)

    paper_files: List[str] = []

    if ',' in txt:
        paper_ids = [pid.strip() for pid in txt.split(',') if pid.strip()]
        chatbot.append(["æ£€æµ‹åˆ°å¤šä¸ªè®ºæ–‡ID", f"æ£€æµ‹åˆ° {len(paper_ids)} ä¸ªè®ºæ–‡IDï¼Œå‡†å¤‡æ‰¹é‡ä¸‹è½½..."])
        yield from update_ui(chatbot=chatbot, history=history)
        for i, pid in enumerate(paper_ids):
            paper_info = extract_paper_id(pid)
            if paper_info:
                chatbot.append([f"ä¸‹è½½è®ºæ–‡ {i+1}/{len(paper_ids)}", f"æ­£åœ¨ä¸‹è½½ {'arXiv' if paper_info[0] == 'arxiv' else 'DOI'} ID: {paper_info[1]}..."])
                yield from update_ui(chatbot=chatbot, history=history)
                p = _download_paper_by_id(paper_info, chatbot, history)
                if p:
                    paper_files.append(p)
                else:
                    chatbot.append(["ä¸‹è½½å¤±è´¥", f"æ— æ³•ä¸‹è½½è®ºæ–‡: {pid}"])
                    yield from update_ui(chatbot=chatbot, history=history)
            else:
                chatbot.append(["IDæ ¼å¼é”™è¯¯", f"æ— æ³•è¯†åˆ«è®ºæ–‡IDæ ¼å¼: {pid}"])
                yield from update_ui(chatbot=chatbot, history=history)
    else:
        paper_info = extract_paper_id(txt)
        if paper_info:
            chatbot.append(["æ£€æµ‹åˆ°è®ºæ–‡ID", f"æ£€æµ‹åˆ°{'arXiv' if paper_info[0] == 'arxiv' else 'DOI'} ID: {paper_info[1]}ï¼Œå‡†å¤‡ä¸‹è½½è®ºæ–‡..."])
            yield from update_ui(chatbot=chatbot, history=history)
            p = _download_paper_by_id(paper_info, chatbot, history)
            if p:
                paper_files.append(p)
            else:
                report_exception(chatbot, history, a="ä¸‹è½½è®ºæ–‡å¤±è´¥", b=f"æ— æ³•ä¸‹è½½{'arXiv' if paper_info[0] == 'arxiv' else 'DOI'}è®ºæ–‡: {paper_info[1]}")
                yield from update_ui(chatbot=chatbot, history=history)
                return
        else:
            if not os.path.exists(txt):
                report_exception(chatbot, history, a=f"æ‰¹é‡ç²¾è¯»è®ºæ–‡: {txt}", b=f"æ‰¾ä¸åˆ°æ–‡ä»¶æˆ–æ— æƒè®¿é—®: {txt}")
                yield from update_ui(chatbot=chatbot, history=history)
                return
            user_name = chatbot.get_user()
            validate_path_safety(txt, user_name)
            paper_files = _find_paper_files(txt)
            if not paper_files:
                report_exception(chatbot, history, a="æ‰¹é‡ç²¾è¯»è®ºæ–‡", b=f"åœ¨è·¯å¾„ {txt} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„è®ºæ–‡æ–‡ä»¶")
                yield from update_ui(chatbot=chatbot, history=history)
                return

    yield from update_ui(chatbot=chatbot, history=history)

    if not paper_files:
        chatbot.append(["é”™è¯¯", "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯åˆ†æçš„è®ºæ–‡æ–‡ä»¶"])
        yield from update_ui(chatbot=chatbot, history=history)
        return

    chatbot.append(["å¼€å§‹æ‰¹é‡ç²¾è¯»", f"æ‰¾åˆ° {len(paper_files)} ç¯‡è®ºæ–‡ï¼Œå¼€å§‹æ·±å…¥åˆ†æ..."])
    yield from update_ui(chatbot=chatbot, history=history)

    analyzer = BatchPaperDetailAnalyzer(llm_kwargs, plugin_kwargs, chatbot, history, system_prompt)

    successes: List[str] = []
    failures: List[str] = []

    for i, paper_file in enumerate(paper_files):
        try:
            chatbot.append([f"ç²¾è¯»è®ºæ–‡ {i+1}/{len(paper_files)}", f"æ­£åœ¨ç²¾è¯»: {os.path.basename(paper_file)}"])
            yield from update_ui(chatbot=chatbot, history=history)
            outfile = yield from analyzer.analyze_paper(paper_file)
            if outfile:
                successes.append(outfile)
                chatbot.append([f"å®Œæˆè®ºæ–‡ {i+1}/{len(paper_files)}", f"æˆåŠŸç”ŸæˆæŠ¥å‘Š: {os.path.basename(outfile)}"])
            else:
                failures.append(os.path.basename(paper_file))
                chatbot.append([f"å¤±è´¥è®ºæ–‡ {i+1}/{len(paper_files)}", f"åˆ†æå¤±è´¥: {os.path.basename(paper_file)}"])
            yield from update_ui(chatbot=chatbot, history=history)
        except Exception as e:
            failures.append(os.path.basename(paper_file))
            chatbot.append([f"é”™è¯¯è®ºæ–‡ {i+1}/{len(paper_files)}", f"åˆ†æå‡ºé”™: {os.path.basename(paper_file)} - {str(e)}"])
            yield from update_ui(chatbot=chatbot, history=history)

    summary = "æ‰¹é‡ç²¾è¯»å®Œæˆï¼\n\n"
    summary += "ğŸ“Š åˆ†æç»Ÿè®¡ï¼š\n"
    summary += f"- æ€»è®ºæ–‡æ•°ï¼š{len(paper_files)}\n"
    summary += f"- æˆåŠŸåˆ†æï¼š{len(successes)}\n"
    summary += f"- åˆ†æå¤±è´¥ï¼š{len(failures)}\n\n"
    if successes:
        summary += "âœ… æˆåŠŸç”ŸæˆæŠ¥å‘Šï¼š\n"
        for p in successes:
            summary += f"- {os.path.basename(p)}\n"
    if failures:
        summary += "\nâŒ åˆ†æå¤±è´¥çš„è®ºæ–‡ï¼š\n"
        for name in failures:
            summary += f"- {name}\n"

    chatbot.append(["æ‰¹é‡ç²¾è¯»å®Œæˆ", summary])
    yield from update_ui(chatbot=chatbot, history=history)


